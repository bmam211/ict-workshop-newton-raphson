{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: stable-baselines3 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: wandb in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: torch>=1.13 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from stable-baselines3) (2.5.1)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from stable-baselines3) (3.10.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (6.1.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (2.10.5)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (2.20.0)\n",
      "Requirement already satisfied: setproctitle in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from pandas->stable-baselines3) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from pandas->stable-baselines3) (2024.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages (from jinja2->torch>=1.13->stable-baselines3) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install gymnasium stable-baselines3 wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandapower as pp\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "import wandb\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTwoBus:\n",
    " \n",
    "    def __init__(self, V_ext, P, Q, G, B, V_init, theta_init, V_bus1, theta_bus1):\n",
    "        '''This class creates a simple 2-bus network.'''\n",
    "        \n",
    "        self.V_ext = V_ext\n",
    "        self.P = P\n",
    "        self.Q = Q\n",
    "        self.G = G\n",
    "        self.B = B\n",
    "        self.V_init = V_init\n",
    "        self.theta_init = theta_init\n",
    "        self.V_bus1 = V_bus1\n",
    "        self.theta_bus1 = theta_bus1\n",
    "        self.net = pp.create_empty_network()\n",
    "        self.create_two_bus_grid()\n",
    " \n",
    " \n",
    "    def create_two_bus_grid(self):\n",
    "   \n",
    "        # Create two buses with initialized voltage and angle\n",
    "        bus1 = pp.create_bus(self.net, vn_kv=1.0, name=\"Bus 1\")\n",
    "        bus2 = pp.create_bus(self.net, vn_kv=1.0, name=\"Bus 2\")\n",
    "   \n",
    "        # Initialize voltage and angle for buses\n",
    "        self.net.bus.loc[bus1, 'vm_pu'] = self.V_bus1\n",
    "        self.net.bus.loc[bus1, 'va_degree'] = self.theta_bus1\n",
    "        self.net.bus.loc[bus2, 'vm_pu'] = self.V_init\n",
    "        self.net.bus.loc[bus2, 'va_degree'] = self.theta_init\n",
    "   \n",
    "        # create a line between the two buses\n",
    "        pp.create_line_from_parameters(\n",
    "            self.net,\n",
    "            from_bus=0,\n",
    "            to_bus=1,\n",
    "            length_km=1.0,\n",
    "            r_ohm_per_km=1/self.G,\n",
    "            x_ohm_per_km=1/self.B,\n",
    "            c_nf_per_km=0.0,\n",
    "            g_us_per_km=0.0,\n",
    "            max_i_ka=100.0,\n",
    "        )\n",
    " \n",
    "        # Create a transformer between the two buses\n",
    "        # pp.create_transformer(self.net, bus1, bus2, std_type=\"0.25 MVA 20/0.4 kV\")\n",
    "   \n",
    "        # Create a load at bus 2 with specified P and Q\n",
    "        pp.create_load(self.net, bus2, p_mw=self.P, q_mvar=self.Q, name=\"Load\")\n",
    "   \n",
    "        # Create an external grid connection at bus 1 with specified G and B\n",
    "        pp.create_ext_grid(self.net, bus1, vm_pu=self.V_ext, name=\"Grid Connection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnv(gym.Env):\n",
    "    def __init__(self,V_ext = 1.0, G = 100, B = 10, k_limit = 3, max_iteration=50, termination_counter=10):\n",
    "\n",
    "\n",
    "        self.observation_space = spaces.Box(low = np.array([0.5,-90, 0]), high = np.array([2, 90, max_iteration+1]), dtype=np.float32) #[V_init, theta_init, number_iterations]\n",
    "        \n",
    "        self.action_space = spaces.Box(low=np.array([-0.5, -50]), high=np.array([0.5, 50]), dtype=np.float32)\n",
    "\n",
    "        self.k_limit = k_limit\n",
    "        self.termination_counter = termination_counter\n",
    "        self.max_iteration = max_iteration\n",
    "\n",
    "\n",
    "        self.G = G\n",
    "        self.B = B\n",
    "        self.V_ext = V_ext\n",
    "\n",
    "        #initialize network\n",
    "        self.state, info = self.reset()\n",
    "\n",
    "    def create_feasible_Ybusnet(self):\n",
    "\n",
    "        YbusNet = SimpleTwoBus(self.V_ext,self.P,self.Q,self.G,self.B,self.V_bus1,self.theta_bus1, 0.98, 0.5) #just to create a sparse Ybus matrix\n",
    "        net = YbusNet.net\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = gym.utils.seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "\n",
    "        self.counter = 0\n",
    "        self.done = False\n",
    "        self.terminated = False\n",
    "        self.state = np.zeros(3)\n",
    "\n",
    "        self.P = 0.9 #np.random.uniform(low= 0, high=10)\n",
    "        self.Q = 0.6 #np.random.uniform(low = 0, high =10)\n",
    "\n",
    "        self.V_bus1 = 1.0#np.random.uniform(low = 0.85, high = 1.15, size=1)\n",
    "        self.theta_bus1 = 0#np.random.uniform(low = -45, high = 45, size=1)\n",
    "        self.V = np.random.uniform(low = 0.5, high = 2, size=1) # initial guess\n",
    "        self.theta = np.random.uniform(low = -90, high = 90, size=1) # initial guess\n",
    "\n",
    "\n",
    "        Net = SimpleTwoBus(self.V_ext,self.P,self.Q,self.G,self.B,self.V,self.theta, self.V_bus1, self.theta_bus1)\n",
    "        self.net = Net.net\n",
    "\n",
    "        self.Ybus = self.calculate_Ybus()\n",
    "\n",
    "        iterations = self.perform_NR_step()\n",
    "\n",
    "        \n",
    "        self.update_state(iterations)\n",
    "\n",
    "\n",
    "        return self.state, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_Ybus(self):\n",
    "\n",
    "\n",
    "        Ybusnet = self.create_feasible_Ybusnet()\n",
    "        pp.runpp(Ybusnet, max_iteration = self.max_iteration, tolerance_mva=1e-5)\n",
    "        Ybus = Ybusnet._ppc[\"internal\"][\"Ybus\"]\n",
    "\n",
    "\n",
    "\n",
    "        return Ybus\n",
    "\n",
    "\n",
    "    def calculate_complex_V(self, V, theta):\n",
    "        complex_V = V*np.exp(1j*theta) #rectangular form\n",
    "\n",
    "        return complex_V\n",
    "    \n",
    "    def update_V(self, action):\n",
    "\n",
    "\n",
    "        new_V = self.V - action[0]\n",
    "        new_theta = self.theta - action[1]\n",
    "\n",
    "\n",
    "        # maybe try different way of scaling the actions back when they exceed the limits?\n",
    "        #defines the action constraints -> this might not be the correct way to do this!\n",
    "        if new_V[0] > 2:\n",
    "            new_V[0] = 2 \n",
    "        if new_V[0] < 0.5:\n",
    "            new_V[0] = 0.5\n",
    "        if new_theta[0] > 90:\n",
    "            new_theta[0] = 90\n",
    "        if new_theta[0] < -90:\n",
    "            new_theta[0] = -90\n",
    "        \n",
    "\n",
    "\n",
    "        complete_V = np.zeros(2)\n",
    "        complete_theta = np.zeros(2)\n",
    "\n",
    "        complete_V[0] = self.V_bus1\n",
    "        complete_V[1] = new_V[0]\n",
    "        complete_theta[0] = self.theta_bus1\n",
    "        complete_theta[1] = new_theta[0]\n",
    "\n",
    "\n",
    "        self.complex_V = self.calculate_complex_V(complete_V, complete_theta)\n",
    "\n",
    "\n",
    "\n",
    "        self.V = new_V\n",
    "\n",
    "        self.theta = new_theta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_residual(self, action):\n",
    "\n",
    "        # net = self.net.deepcopy()  # Keep the network unchanged\n",
    "\n",
    "        self.update_V(action) #rectangular form\n",
    "        # print(f\"{self.complex_V=}\")\n",
    "\n",
    "        # print(f\"{self.Ybus[0,1]=}\")\n",
    "\n",
    "        term2 = self.Ybus@self.complex_V\n",
    "        term2_complex_conj = np.conj(term2)\n",
    "\n",
    "        term1 = self.complex_V@term2_complex_conj\n",
    "\n",
    "        F = self.P + 1j*self.Q - term1\n",
    "\n",
    "        delta_P = np.real(F)\n",
    "        delta_Q = np.imag(F)\n",
    "\n",
    "        residual = np.array([delta_P, delta_Q])\n",
    "\n",
    "\n",
    "\n",
    "        return residual\n",
    "\n",
    "\n",
    "    def perform_NR_step(self):\n",
    "\n",
    "        net = self.net.deepcopy()  # Keep the network unchanged\n",
    "\n",
    "        try:\n",
    "            pp.runpp(net, max_iteration = self.max_iteration, tolerance_mva = 1e-5, init_vm_pu=self.V,init_va_degree=self.theta)\n",
    "            # print(f\"{net.res_bus[['va_degree']].values=}\")\n",
    "            # print(f\"{net.res_bus[['vm_pu']].values=}\")\n",
    "        \n",
    "\n",
    "            iterations = net._ppc[\"iterations\"]\n",
    "        except:\n",
    "            iterations = 50\n",
    "            # print(f\"{net.res_bus[['va_degree']].values=}\")\n",
    "            # print(f\"{net.res_bus[['vm_pu']].values=}\")\n",
    "\n",
    "        return iterations\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def calculate_reward(self, residual):\n",
    "\n",
    "\n",
    "        reward = np.linalg.norm(residual)\n",
    "\n",
    "        return -reward\n",
    "    \n",
    "\n",
    "    def update_state(self, iterations):\n",
    "        \n",
    "        self.state[0] = self.V[0]\n",
    "        self.state[1] = self.theta[0]\n",
    "        self.state[2] = iterations\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "\n",
    "        self.counter += 1\n",
    "        # action = [delta_V, delta_theta]\n",
    "\n",
    "        # perform action\n",
    "        residual = self.calculate_residual(action)\n",
    "        # print(f\"{residual=}\")\n",
    "\n",
    "        # calcualate reward\n",
    "        reward = self.calculate_reward(residual)\n",
    "\n",
    "        iterations = self.perform_NR_step()\n",
    "\n",
    "        #update state:\n",
    "        self.update_state(iterations)\n",
    "\n",
    "        if iterations <= self.k_limit:\n",
    "            self.done = True\n",
    "            return self.state, reward, self.done, self.terminated, {}\n",
    "\n",
    "        \n",
    "\n",
    "        if self.counter == self.termination_counter:\n",
    "            self.terminated = True\n",
    "            return self.state, reward, self.done, self.terminated, {}\n",
    "\n",
    "        return self.state, reward, self.done, self.terminated, {}\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    def render(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [  1.21384304 -56.31770724  14.        ]\n",
      "\n",
      "Action Taken: [ 0.03 15.  ]\n",
      "Next State: [  1.18384304 -71.31770724   8.        ]\n",
      "Reward: -37.13398868869652\n",
      "Done: False\n"
     ]
    }
   ],
   "source": [
    "# Test run\n",
    "\n",
    "\n",
    "env = GridEnv()\n",
    "\n",
    "k_list = []\n",
    "for i in range(int(10000)):\n",
    "\n",
    "    state, info = env.reset()\n",
    "    k = state[-1]\n",
    "    k_list.append(k)\n",
    "\n",
    "\n",
    "#view the distribution of hard and easy cases\n",
    "plt.figure()\n",
    "plt.hist(k_list)\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.show()\n",
    "\n",
    "state,info = env.reset()\n",
    "\n",
    "print(\"Initial State:\", state)\n",
    "# env.render()\n",
    "\n",
    "# Define a sample action within the specified ranges\n",
    "action = np.array([0.03, 15.0], dtype=np.float32)\n",
    "\n",
    "# Take a step in the environment using the sample action\n",
    "next_state, reward, done, terminated, info = env.step(action)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nAction Taken:\", action)\n",
    "print(\"Next State:\", next_state)\n",
    "# env.render()\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandbCallback(BaseCallback):\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(WandbCallback, self).__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Check if the episode is done\n",
    "        if self.locals[\"dones\"][0]:\n",
    "            # Log the episode return (sum of rewards)\n",
    "            episode_reward = self.locals[\"infos\"][0].get(\"episode\", {}).get(\"r\", 0)\n",
    "            episode_length = self.locals[\"infos\"][0].get(\"episode\", {}).get(\"l\", 0)\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_lengths.append(episode_length)\n",
    "            wandb.log({\"episode_reward\": np.mean(self.episode_rewards[:-100]), \"episode_length\": np.mean(self.episode_lengths[:-100])})\n",
    "            # print(f\"{episode_reward=}\")\n",
    "\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlindsayspoor\u001b[0m (\u001b[33mlindsayspoor-rlg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/lindsayspoor/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/Projects/test_ictwi_2/wandb/run-20250123_173028-eu6881zy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lindsayspoor-rlg/grid-env-training/runs/eu6881zy' target=\"_blank\">stilted-smoke-34</a></strong> to <a href='https://wandb.ai/lindsayspoor-rlg/grid-env-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lindsayspoor-rlg/grid-env-training' target=\"_blank\">https://wandb.ai/lindsayspoor-rlg/grid-env-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lindsayspoor-rlg/grid-env-training/runs/eu6881zy' target=\"_blank\">https://wandb.ai/lindsayspoor-rlg/grid-env-training/runs/eu6881zy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: -238.08\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 2000\n",
      "Best mean reward: -238.08 - Last mean reward per episode: -259.68\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9.48     |\n",
      "|    ep_rew_mean     | -253     |\n",
      "| time/              |          |\n",
      "|    fps             | 57       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: -238.08 - Last mean reward per episode: -227.59\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 4000\n",
      "Best mean reward: -227.59 - Last mean reward per episode: -228.37\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.98        |\n",
      "|    ep_rew_mean          | -220        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015399545 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.82       |\n",
      "|    explained_variance   | 0.000187    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 5.89e+03    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 2.05e+04    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -227.59 - Last mean reward per episode: -180.81\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 6000\n",
      "Best mean reward: -180.81 - Last mean reward per episode: -205.94\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.36        |\n",
      "|    ep_rew_mean          | -204        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017858103 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.78       |\n",
      "|    explained_variance   | 0.000279    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 6.28e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.96        |\n",
      "|    value_loss           | 1.82e+04    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: -180.81 - Last mean reward per episode: -178.38\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 8000\n",
      "Best mean reward: -178.38 - Last mean reward per episode: -161.23\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.4         |\n",
      "|    ep_rew_mean          | -171        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016351363 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.72       |\n",
      "|    explained_variance   | 3.24e-05    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.97e+03    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    std                  | 0.933       |\n",
      "|    value_loss           | 1.42e+04    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: -161.23 - Last mean reward per episode: -170.80\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -161.23 - Last mean reward per episode: -151.55\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.17        |\n",
      "|    ep_rew_mean          | -149        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011585164 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.65       |\n",
      "|    explained_variance   | 2.29e-05    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 4.82e+03    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    std                  | 0.898       |\n",
      "|    value_loss           | 9.27e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: -151.55 - Last mean reward per episode: -139.30\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 12000\n",
      "Best mean reward: -139.30 - Last mean reward per episode: -144.79\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.12        |\n",
      "|    ep_rew_mean          | -142        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012056526 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.59       |\n",
      "|    explained_variance   | 3.16e-06    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 3.2e+03     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.879       |\n",
      "|    value_loss           | 7.51e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 13000\n",
      "Best mean reward: -139.30 - Last mean reward per episode: -134.37\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 14000\n",
      "Best mean reward: -134.37 - Last mean reward per episode: -137.52\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.14        |\n",
      "|    ep_rew_mean          | -138        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 235         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007828173 |\n",
      "|    clip_fraction        | 0.0909      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.56       |\n",
      "|    explained_variance   | 1.51e-05    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.71e+03    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00781    |\n",
      "|    std                  | 0.869       |\n",
      "|    value_loss           | 5.47e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -134.37 - Last mean reward per episode: -132.40\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 16000\n",
      "Best mean reward: -132.40 - Last mean reward per episode: -134.35\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.95       |\n",
      "|    ep_rew_mean          | -132       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 61         |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 268        |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00886996 |\n",
      "|    clip_fraction        | 0.0898     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.51      |\n",
      "|    explained_variance   | 4.65e-06   |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.58e+03   |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.00787   |\n",
      "|    std                  | 0.842      |\n",
      "|    value_loss           | 5.13e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: -132.40 - Last mean reward per episode: -132.25\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 18000\n",
      "Best mean reward: -132.25 - Last mean reward per episode: -139.06\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.09        |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 301         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012707822 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.45       |\n",
      "|    explained_variance   | 4.95e-06    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.79e+03    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00974    |\n",
      "|    std                  | 0.818       |\n",
      "|    value_loss           | 4.58e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 19000\n",
      "Best mean reward: -132.25 - Last mean reward per episode: -126.46\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -126.46 - Last mean reward per episode: -121.23\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.6         |\n",
      "|    ep_rew_mean          | -136        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 332         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009584716 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.41       |\n",
      "|    explained_variance   | 5.9e-06     |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.67e+03    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00635    |\n",
      "|    std                  | 0.814       |\n",
      "|    value_loss           | 4.73e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 21000\n",
      "Best mean reward: -121.23 - Last mean reward per episode: -122.84\n",
      "Num timesteps: 22000\n",
      "Best mean reward: -121.23 - Last mean reward per episode: -127.09\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.28        |\n",
      "|    ep_rew_mean          | -132        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 367         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012741739 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.37       |\n",
      "|    explained_variance   | 5.13e-06    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.7e+03     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 0.789       |\n",
      "|    value_loss           | 4.23e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 23000\n",
      "Best mean reward: -121.23 - Last mean reward per episode: -132.53\n",
      "Num timesteps: 24000\n",
      "Best mean reward: -121.23 - Last mean reward per episode: -125.17\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.39        |\n",
      "|    ep_rew_mean          | -121        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 402         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010376601 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.33       |\n",
      "|    explained_variance   | 2.5e-06     |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.83e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    std                  | 0.78        |\n",
      "|    value_loss           | 3.96e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -121.23 - Last mean reward per episode: -120.30\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 26000\n",
      "Best mean reward: -120.30 - Last mean reward per episode: -132.90\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.01         |\n",
      "|    ep_rew_mean          | -119         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 61           |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 433          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066579343 |\n",
      "|    clip_fraction        | 0.0671       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.31        |\n",
      "|    explained_variance   | 6.02e-06     |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.73e+03     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    std                  | 0.776        |\n",
      "|    value_loss           | 3.85e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 27000\n",
      "Best mean reward: -120.30 - Last mean reward per episode: -111.92\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 28000\n",
      "Best mean reward: -111.92 - Last mean reward per episode: -121.39\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.21        |\n",
      "|    ep_rew_mean          | -129        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008009666 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.28       |\n",
      "|    explained_variance   | 4.35e-06    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.41e+03    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00657    |\n",
      "|    std                  | 0.766       |\n",
      "|    value_loss           | 3.96e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 29000\n",
      "Best mean reward: -111.92 - Last mean reward per episode: -136.36\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -111.92 - Last mean reward per episode: -126.13\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.34        |\n",
      "|    ep_rew_mean          | -128        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 496         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010319844 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.25       |\n",
      "|    explained_variance   | 3.7e-06     |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.47e+03    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00491    |\n",
      "|    std                  | 0.753       |\n",
      "|    value_loss           | 4.04e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 31000\n",
      "Best mean reward: -111.92 - Last mean reward per episode: -126.42\n",
      "Num timesteps: 32000\n",
      "Best mean reward: -111.92 - Last mean reward per episode: -119.50\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.46        |\n",
      "|    ep_rew_mean          | -130        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 526         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010254941 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.21       |\n",
      "|    explained_variance   | 4.59e-06    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.52e+03    |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00735    |\n",
      "|    std                  | 0.739       |\n",
      "|    value_loss           | 4.18e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 33000\n",
      "Best mean reward: -111.92 - Last mean reward per episode: -121.06\n",
      "Num timesteps: 34000\n",
      "Best mean reward: -111.92 - Last mean reward per episode: -126.31\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.81        |\n",
      "|    ep_rew_mean          | -107        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 557         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016747294 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | 4.29e-06    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.76e+03    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00722    |\n",
      "|    std                  | 0.73        |\n",
      "|    value_loss           | 3.69e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 35000\n",
      "Best mean reward: -111.92 - Last mean reward per episode: -109.92\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 36000\n",
      "Best mean reward: -109.92 - Last mean reward per episode: -115.03\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.98        |\n",
      "|    ep_rew_mean          | -115        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 588         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010330738 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.16       |\n",
      "|    explained_variance   | 4.23e-06    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.56e+03    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0044     |\n",
      "|    std                  | 0.731       |\n",
      "|    value_loss           | 3.86e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 37000\n",
      "Best mean reward: -109.92 - Last mean reward per episode: -115.87\n",
      "Num timesteps: 38000\n",
      "Best mean reward: -109.92 - Last mean reward per episode: -110.54\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.1         |\n",
      "|    ep_rew_mean          | -118        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 621         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010800526 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.14       |\n",
      "|    explained_variance   | 9.72e-06    |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.97e+03    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.000367   |\n",
      "|    std                  | 0.721       |\n",
      "|    value_loss           | 3.75e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 39000\n",
      "Best mean reward: -109.92 - Last mean reward per episode: -116.71\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -109.92 - Last mean reward per episode: -112.82\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.53        |\n",
      "|    ep_rew_mean          | -110        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 654         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007304211 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.13       |\n",
      "|    explained_variance   | 0.0924      |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.19e+03    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.000751   |\n",
      "|    std                  | 0.717       |\n",
      "|    value_loss           | 3.74e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 41000\n",
      "Best mean reward: -109.92 - Last mean reward per episode: -109.67\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 42000\n",
      "Best mean reward: -109.67 - Last mean reward per episode: -122.23\n",
      "Num timesteps: 43000\n",
      "Best mean reward: -109.67 - Last mean reward per episode: -107.40\n",
      "Saving new best model to log/best_model\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.84         |\n",
      "|    ep_rew_mean          | -107         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 62           |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 685          |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061134826 |\n",
      "|    clip_fraction        | 0.0854       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.11        |\n",
      "|    explained_variance   | 0.144        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.39e+03     |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    std                  | 0.716        |\n",
      "|    value_loss           | 3.87e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 44000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -112.68\n",
      "Num timesteps: 45000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -118.55\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.98       |\n",
      "|    ep_rew_mean          | -120       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 62         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 718        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00828128 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.11      |\n",
      "|    explained_variance   | 0.325      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 1.87e+03   |\n",
      "|    n_updates            | 210        |\n",
      "|    policy_gradient_loss | -0.00654   |\n",
      "|    std                  | 0.716      |\n",
      "|    value_loss           | 3.3e+03    |\n",
      "----------------------------------------\n",
      "Num timesteps: 46000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -115.33\n",
      "Num timesteps: 47000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -116.06\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.18        |\n",
      "|    ep_rew_mean          | -116        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 750         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008667966 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.11       |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.67e+03    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00694    |\n",
      "|    std                  | 0.71        |\n",
      "|    value_loss           | 3.44e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 48000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -122.27\n",
      "Num timesteps: 49000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -111.53\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.87        |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 783         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006340584 |\n",
      "|    clip_fraction        | 0.0784      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.09       |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.35e+03    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    std                  | 0.702       |\n",
      "|    value_loss           | 2.93e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -116.88\n",
      "Num timesteps: 51000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -117.89\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.92        |\n",
      "|    ep_rew_mean          | -117        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 62          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 824         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008020822 |\n",
      "|    clip_fraction        | 0.0942      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.08       |\n",
      "|    explained_variance   | 0.609       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.48e+03    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00447    |\n",
      "|    std                  | 0.701       |\n",
      "|    value_loss           | 2.91e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 52000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -114.06\n",
      "Num timesteps: 53000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -115.60\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.97         |\n",
      "|    ep_rew_mean          | -119         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 61           |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 860          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032482576 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.08        |\n",
      "|    explained_variance   | 0.599        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 1.31e+03     |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    std                  | 0.701        |\n",
      "|    value_loss           | 2.9e+03      |\n",
      "------------------------------------------\n",
      "Num timesteps: 54000\n",
      "Best mean reward: -107.40 - Last mean reward per episode: -106.56\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 55000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -116.25\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.64        |\n",
      "|    ep_rew_mean          | -117        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 894         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008381541 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.07       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.35e+03    |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00515    |\n",
      "|    std                  | 0.695       |\n",
      "|    value_loss           | 2.84e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 56000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -116.01\n",
      "Num timesteps: 57000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -109.79\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.78        |\n",
      "|    ep_rew_mean          | -111        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 930         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009509807 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.07       |\n",
      "|    explained_variance   | 0.651       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.26e+03    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    std                  | 0.698       |\n",
      "|    value_loss           | 2.73e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 58000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -115.47\n",
      "Num timesteps: 59000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -115.11\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.62        |\n",
      "|    ep_rew_mean          | -122        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 967         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008785427 |\n",
      "|    clip_fraction        | 0.0751      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.06       |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.2e+03     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00568    |\n",
      "|    std                  | 0.691       |\n",
      "|    value_loss           | 2.71e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -123.07\n",
      "Num timesteps: 61000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -108.29\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.32        |\n",
      "|    ep_rew_mean          | -100        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 1005        |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008209617 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.04       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.34e+03    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    std                  | 0.679       |\n",
      "|    value_loss           | 2.77e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 62000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -107.04\n",
      "Num timesteps: 63000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -111.45\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.77        |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 1036        |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007604585 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.03       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.41e+03    |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00566    |\n",
      "|    std                  | 0.685       |\n",
      "|    value_loss           | 2.5e+03     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 64000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -112.07\n",
      "Num timesteps: 65000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -112.58\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.37        |\n",
      "|    ep_rew_mean          | -115        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 61          |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 1071        |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008871572 |\n",
      "|    clip_fraction        | 0.0957      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.02       |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 898         |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    std                  | 0.676       |\n",
      "|    value_loss           | 2.12e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 66000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -111.75\n",
      "Num timesteps: 67000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -109.84\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.36        |\n",
      "|    ep_rew_mean          | -113        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 1108        |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010100229 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.99       |\n",
      "|    explained_variance   | 0.802       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 759         |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00974    |\n",
      "|    std                  | 0.661       |\n",
      "|    value_loss           | 2.31e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 68000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -114.67\n",
      "Num timesteps: 69000\n",
      "Best mean reward: -106.56 - Last mean reward per episode: -101.13\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.79        |\n",
      "|    ep_rew_mean          | -110        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 1146        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010667012 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.97       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1e+03       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00461    |\n",
      "|    std                  | 0.658       |\n",
      "|    value_loss           | 2.52e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 70000\n",
      "Best mean reward: -101.13 - Last mean reward per episode: -112.77\n",
      "Num timesteps: 71000\n",
      "Best mean reward: -101.13 - Last mean reward per episode: -111.89\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.06       |\n",
      "|    ep_rew_mean          | -102       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 60         |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 1185       |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00755503 |\n",
      "|    clip_fraction        | 0.0965     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.96      |\n",
      "|    explained_variance   | 0.804      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 740        |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.00342   |\n",
      "|    std                  | 0.656      |\n",
      "|    value_loss           | 2.16e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 72000\n",
      "Best mean reward: -101.13 - Last mean reward per episode: -98.27\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 73000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -109.72\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.01        |\n",
      "|    ep_rew_mean          | -119        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 1218        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008192463 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.64e+03    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00347    |\n",
      "|    std                  | 0.653       |\n",
      "|    value_loss           | 2.3e+03     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 74000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -118.72\n",
      "Num timesteps: 75000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -118.65\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.51        |\n",
      "|    ep_rew_mean          | -113        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 1251        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011477492 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.95       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.05e+03    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00601    |\n",
      "|    std                  | 0.652       |\n",
      "|    value_loss           | 2.2e+03     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 76000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -110.94\n",
      "Num timesteps: 77000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -107.90\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.02        |\n",
      "|    ep_rew_mean          | -104        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 1287        |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015061598 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.07e+03    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00604    |\n",
      "|    std                  | 0.64        |\n",
      "|    value_loss           | 2.53e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 78000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -108.60\n",
      "Num timesteps: 79000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -109.39\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.76        |\n",
      "|    ep_rew_mean          | -100        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 1323        |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008728461 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | 0.85        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 873         |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.008      |\n",
      "|    std                  | 0.637       |\n",
      "|    value_loss           | 2.2e+03     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 80000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -99.52\n",
      "Num timesteps: 81000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -104.36\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.59        |\n",
      "|    ep_rew_mean          | -115        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 1356        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011270752 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.852       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.18e+03    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00465    |\n",
      "|    std                  | 0.628       |\n",
      "|    value_loss           | 2.29e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 82000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -116.74\n",
      "Num timesteps: 83000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -110.08\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.96        |\n",
      "|    ep_rew_mean          | -117        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 1388        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008059051 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 936         |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00426    |\n",
      "|    std                  | 0.62        |\n",
      "|    value_loss           | 1.88e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 84000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -116.72\n",
      "Num timesteps: 85000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -106.19\n",
      "Num timesteps: 86000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -103.20\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.41        |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 1422        |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010490082 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 897         |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    std                  | 0.618       |\n",
      "|    value_loss           | 1.92e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 87000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -103.91\n",
      "Num timesteps: 88000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -101.59\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.23        |\n",
      "|    ep_rew_mean          | -103        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 1457        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012804283 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.82       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 937         |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00576    |\n",
      "|    std                  | 0.608       |\n",
      "|    value_loss           | 1.98e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 89000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -102.37\n",
      "Num timesteps: 90000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -102.55\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.12        |\n",
      "|    ep_rew_mean          | -101        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 1492        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012852753 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.01e+03    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00631    |\n",
      "|    std                  | 0.602       |\n",
      "|    value_loss           | 1.87e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 91000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -105.52\n",
      "Num timesteps: 92000\n",
      "Best mean reward: -98.27 - Last mean reward per episode: -96.56\n",
      "Saving new best model to log/best_model\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.65       |\n",
      "|    ep_rew_mean          | -99.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 60         |\n",
      "|    iterations           | 45         |\n",
      "|    time_elapsed         | 1527       |\n",
      "|    total_timesteps      | 92160      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01526938 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.78      |\n",
      "|    explained_variance   | 0.891      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 710        |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.00182   |\n",
      "|    std                  | 0.599      |\n",
      "|    value_loss           | 1.77e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 93000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -104.89\n",
      "Num timesteps: 94000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -104.82\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.91        |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 1562        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013793426 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 838         |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00342    |\n",
      "|    std                  | 0.59        |\n",
      "|    value_loss           | 1.68e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 95000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -110.64\n",
      "Num timesteps: 96000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -111.82\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.27        |\n",
      "|    ep_rew_mean          | -106        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 1596        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008519329 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.74       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 716         |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.000873   |\n",
      "|    std                  | 0.588       |\n",
      "|    value_loss           | 1.5e+03     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 97000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -101.50\n",
      "Num timesteps: 98000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -115.20\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.85        |\n",
      "|    ep_rew_mean          | -120        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 1631        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012814755 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.73       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.09e+03    |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    std                  | 0.587       |\n",
      "|    value_loss           | 1.73e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 99000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -113.91\n",
      "Num timesteps: 100000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -104.78\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 8.19         |\n",
      "|    ep_rew_mean          | -104         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 60           |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 1667         |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081533175 |\n",
      "|    clip_fraction        | 0.131        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 0.904        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 831          |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00606     |\n",
      "|    std                  | 0.576        |\n",
      "|    value_loss           | 1.68e+03     |\n",
      "------------------------------------------\n",
      "Num timesteps: 101000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -109.73\n",
      "Num timesteps: 102000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -107.63\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.17       |\n",
      "|    ep_rew_mean          | -107       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 60         |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 1701       |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01663984 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.68      |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 709        |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | 0.00201    |\n",
      "|    std                  | 0.571      |\n",
      "|    value_loss           | 1.63e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 103000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -105.70\n",
      "Num timesteps: 104000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -103.65\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.45        |\n",
      "|    ep_rew_mean          | -103        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 1736        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011594245 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 0.912       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 875         |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0047     |\n",
      "|    std                  | 0.562       |\n",
      "|    value_loss           | 1.75e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 105000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -98.08\n",
      "Num timesteps: 106000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -106.95\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.38        |\n",
      "|    ep_rew_mean          | -96.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 1772        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014933909 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.62       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 763         |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    std                  | 0.555       |\n",
      "|    value_loss           | 1.4e+03     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 107000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -102.15\n",
      "Num timesteps: 108000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -103.02\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.22        |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 60          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 1809        |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009680518 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 762         |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    std                  | 0.555       |\n",
      "|    value_loss           | 1.71e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 109000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -108.64\n",
      "Num timesteps: 110000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -108.68\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.87        |\n",
      "|    ep_rew_mean          | -99.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 1845        |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020338923 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 924         |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00217    |\n",
      "|    std                  | 0.554       |\n",
      "|    value_loss           | 1.57e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 111000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -97.16\n",
      "Num timesteps: 112000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -103.98\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.57        |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 1880        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009157595 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 536         |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00305    |\n",
      "|    std                  | 0.55        |\n",
      "|    value_loss           | 1.52e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 113000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -108.04\n",
      "Num timesteps: 114000\n",
      "Best mean reward: -96.56 - Last mean reward per episode: -95.57\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.77        |\n",
      "|    ep_rew_mean          | -97.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 1916        |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016380824 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 432         |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    std                  | 0.551       |\n",
      "|    value_loss           | 1.29e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 115000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -106.04\n",
      "Num timesteps: 116000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -96.80\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.53        |\n",
      "|    ep_rew_mean          | -96.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 1954        |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010053521 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 617         |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00731    |\n",
      "|    std                  | 0.542       |\n",
      "|    value_loss           | 1.38e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 117000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -99.85\n",
      "Num timesteps: 118000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -107.28\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.84        |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 1993        |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014388027 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 1.89e+03    |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00761    |\n",
      "|    std                  | 0.54        |\n",
      "|    value_loss           | 1.44e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 119000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -100.54\n",
      "Num timesteps: 120000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -106.08\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.27        |\n",
      "|    ep_rew_mean          | -97.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 2031        |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010204773 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 805         |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    std                  | 0.535       |\n",
      "|    value_loss           | 1.38e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 121000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -96.11\n",
      "Num timesteps: 122000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -97.55\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 8.21       |\n",
      "|    ep_rew_mean          | -105       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 59         |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 2067       |\n",
      "|    total_timesteps      | 122880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01342344 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.54      |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 373        |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.00202   |\n",
      "|    std                  | 0.536      |\n",
      "|    value_loss           | 1.08e+03   |\n",
      "----------------------------------------\n",
      "Num timesteps: 123000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -103.45\n",
      "Num timesteps: 124000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -103.64\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.75        |\n",
      "|    ep_rew_mean          | -98         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 2105        |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010238561 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 483         |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00944    |\n",
      "|    std                  | 0.531       |\n",
      "|    value_loss           | 1.09e+03    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 125000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -99.09\n",
      "Num timesteps: 126000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -99.18\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.02        |\n",
      "|    ep_rew_mean          | -107        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 2140        |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010855505 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 341         |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    std                  | 0.527       |\n",
      "|    value_loss           | 751         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 127000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -107.89\n",
      "Num timesteps: 128000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -109.40\n",
      "Num timesteps: 129000\n",
      "Best mean reward: -95.57 - Last mean reward per episode: -93.19\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.11        |\n",
      "|    ep_rew_mean          | -92.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 2173        |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012912825 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 388         |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00831    |\n",
      "|    std                  | 0.525       |\n",
      "|    value_loss           | 948         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 130000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -106.74\n",
      "Num timesteps: 131000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -102.34\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.91        |\n",
      "|    ep_rew_mean          | -98.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 2207        |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014011957 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 431         |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.523       |\n",
      "|    value_loss           | 965         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 132000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -98.17\n",
      "Num timesteps: 133000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -101.78\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 8.02        |\n",
      "|    ep_rew_mean          | -101        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 2242        |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013569816 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 390         |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00988    |\n",
      "|    std                  | 0.522       |\n",
      "|    value_loss           | 901         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 134000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -98.96\n",
      "Num timesteps: 135000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -99.64\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.66        |\n",
      "|    ep_rew_mean          | -97.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 2276        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009515902 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 472         |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00626    |\n",
      "|    std                  | 0.518       |\n",
      "|    value_loss           | 810         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 136000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -99.78\n",
      "Num timesteps: 137000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -93.64\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.26        |\n",
      "|    ep_rew_mean          | -93.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 2311        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018163761 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 381         |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00837    |\n",
      "|    std                  | 0.515       |\n",
      "|    value_loss           | 852         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 138000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -104.66\n",
      "Num timesteps: 139000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -98.69\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.57        |\n",
      "|    ep_rew_mean          | -97.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 2344        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016517768 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 354         |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00653    |\n",
      "|    std                  | 0.514       |\n",
      "|    value_loss           | 740         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 140000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -104.51\n",
      "Num timesteps: 141000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -104.58\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.98       |\n",
      "|    ep_rew_mean          | -106       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 59         |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 2375       |\n",
      "|    total_timesteps      | 141312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01834224 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.47      |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 355        |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.00734   |\n",
      "|    std                  | 0.512      |\n",
      "|    value_loss           | 615        |\n",
      "----------------------------------------\n",
      "Num timesteps: 142000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -98.43\n",
      "Num timesteps: 143000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -106.20\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.72        |\n",
      "|    ep_rew_mean          | -97.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 2410        |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023798957 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 440         |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00546    |\n",
      "|    std                  | 0.509       |\n",
      "|    value_loss           | 570         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 144000\n",
      "Best mean reward: -93.19 - Last mean reward per episode: -91.57\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 145000\n",
      "Best mean reward: -91.57 - Last mean reward per episode: -94.31\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.36       |\n",
      "|    ep_rew_mean          | -97.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 59         |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 2445       |\n",
      "|    total_timesteps      | 145408     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01190187 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.46      |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 261        |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.00863   |\n",
      "|    std                  | 0.51       |\n",
      "|    value_loss           | 455        |\n",
      "----------------------------------------\n",
      "Num timesteps: 146000\n",
      "Best mean reward: -91.57 - Last mean reward per episode: -104.73\n",
      "Num timesteps: 147000\n",
      "Best mean reward: -91.57 - Last mean reward per episode: -103.48\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.5         |\n",
      "|    ep_rew_mean          | -101        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 2481        |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018027024 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 222         |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.00462    |\n",
      "|    std                  | 0.505       |\n",
      "|    value_loss           | 505         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 148000\n",
      "Best mean reward: -91.57 - Last mean reward per episode: -101.01\n",
      "Num timesteps: 149000\n",
      "Best mean reward: -91.57 - Last mean reward per episode: -98.75\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.79       |\n",
      "|    ep_rew_mean          | -103       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 59         |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 2516       |\n",
      "|    total_timesteps      | 149504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01731871 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 234        |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.00182   |\n",
      "|    std                  | 0.505      |\n",
      "|    value_loss           | 558        |\n",
      "----------------------------------------\n",
      "Num timesteps: 150000\n",
      "Best mean reward: -91.57 - Last mean reward per episode: -96.64\n",
      "Num timesteps: 151000\n",
      "Best mean reward: -91.57 - Last mean reward per episode: -85.76\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.16        |\n",
      "|    ep_rew_mean          | -92.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 2554        |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021529887 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 156         |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00849    |\n",
      "|    std                  | 0.501       |\n",
      "|    value_loss           | 479         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 152000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -96.97\n",
      "Num timesteps: 153000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -97.56\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.86        |\n",
      "|    ep_rew_mean          | -102        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 2591        |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021003556 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 165         |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.000982   |\n",
      "|    std                  | 0.496       |\n",
      "|    value_loss           | 426         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 154000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -95.66\n",
      "Num timesteps: 155000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -102.88\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.98        |\n",
      "|    ep_rew_mean          | -105        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 2626        |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017369632 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 148         |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    std                  | 0.493       |\n",
      "|    value_loss           | 373         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 156000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -96.70\n",
      "Num timesteps: 157000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -98.28\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.81       |\n",
      "|    ep_rew_mean          | -105       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 59         |\n",
      "|    iterations           | 77         |\n",
      "|    time_elapsed         | 2662       |\n",
      "|    total_timesteps      | 157696     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02136489 |\n",
      "|    clip_fraction        | 0.207      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 220        |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.00791   |\n",
      "|    std                  | 0.488      |\n",
      "|    value_loss           | 332        |\n",
      "----------------------------------------\n",
      "Num timesteps: 158000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -102.73\n",
      "Num timesteps: 159000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -95.73\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.44        |\n",
      "|    ep_rew_mean          | -95.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 2700        |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014758442 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 261         |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    std                  | 0.482       |\n",
      "|    value_loss           | 386         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 160000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -96.71\n",
      "Num timesteps: 161000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -88.75\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.42        |\n",
      "|    ep_rew_mean          | -95.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 59          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 2738        |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027869321 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 115         |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00924    |\n",
      "|    std                  | 0.482       |\n",
      "|    value_loss           | 303         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 162000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -92.38\n",
      "Num timesteps: 163000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -95.93\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.24        |\n",
      "|    ep_rew_mean          | -94.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 2777        |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021865372 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 113         |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00448    |\n",
      "|    std                  | 0.48        |\n",
      "|    value_loss           | 267         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 164000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -93.88\n",
      "Num timesteps: 165000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -95.31\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.23        |\n",
      "|    ep_rew_mean          | -96.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 2813        |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028005486 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 75.7        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    std                  | 0.476       |\n",
      "|    value_loss           | 207         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 166000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -94.73\n",
      "Num timesteps: 167000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -91.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.65        |\n",
      "|    ep_rew_mean          | -100        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 2849        |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022012107 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 111         |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00894    |\n",
      "|    std                  | 0.478       |\n",
      "|    value_loss           | 228         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 168000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -98.96\n",
      "Num timesteps: 169000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -96.34\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.41        |\n",
      "|    ep_rew_mean          | -92.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 2883        |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012183461 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 100         |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.000426   |\n",
      "|    std                  | 0.474       |\n",
      "|    value_loss           | 216         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 170000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -93.53\n",
      "Num timesteps: 171000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -95.75\n",
      "Num timesteps: 172000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -90.44\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 7.02         |\n",
      "|    ep_rew_mean          | -90.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 58           |\n",
      "|    iterations           | 84           |\n",
      "|    time_elapsed         | 2920         |\n",
      "|    total_timesteps      | 172032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0131520685 |\n",
      "|    clip_fraction        | 0.219        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0.951        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 84.6         |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.000159    |\n",
      "|    std                  | 0.471        |\n",
      "|    value_loss           | 246          |\n",
      "------------------------------------------\n",
      "Num timesteps: 173000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -98.87\n",
      "Num timesteps: 174000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -98.54\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.24        |\n",
      "|    ep_rew_mean          | -96.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 2955        |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027977992 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 88.4        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00637    |\n",
      "|    std                  | 0.463       |\n",
      "|    value_loss           | 232         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 175000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -94.02\n",
      "Num timesteps: 176000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -91.07\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.13        |\n",
      "|    ep_rew_mean          | -93         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 2990        |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034905355 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 131         |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00137    |\n",
      "|    std                  | 0.46        |\n",
      "|    value_loss           | 191         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 177000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -100.88\n",
      "Num timesteps: 178000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -88.78\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.94        |\n",
      "|    ep_rew_mean          | -92         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 3024        |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017272323 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 122         |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00581    |\n",
      "|    std                  | 0.451       |\n",
      "|    value_loss           | 214         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 179000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -94.51\n",
      "Num timesteps: 180000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -88.26\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.96        |\n",
      "|    ep_rew_mean          | -90.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 3059        |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015864678 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 74.4        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00506    |\n",
      "|    std                  | 0.445       |\n",
      "|    value_loss           | 165         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 181000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -99.88\n",
      "Num timesteps: 182000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -93.38\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.76       |\n",
      "|    ep_rew_mean          | -88.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 58         |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 3096       |\n",
      "|    total_timesteps      | 182272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02648723 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.18      |\n",
      "|    explained_variance   | 0.932      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 110        |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.00127   |\n",
      "|    std                  | 0.441      |\n",
      "|    value_loss           | 232        |\n",
      "----------------------------------------\n",
      "Num timesteps: 183000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -97.30\n",
      "Num timesteps: 184000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -95.68\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.99        |\n",
      "|    ep_rew_mean          | -92.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 3134        |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031781554 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 114         |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00362    |\n",
      "|    std                  | 0.436       |\n",
      "|    value_loss           | 195         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 185000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -96.88\n",
      "Num timesteps: 186000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -100.14\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.41        |\n",
      "|    ep_rew_mean          | -97.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 3170        |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023125276 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 99.8        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00432    |\n",
      "|    std                  | 0.434       |\n",
      "|    value_loss           | 186         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 187000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -94.03\n",
      "Num timesteps: 188000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -97.09\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.06        |\n",
      "|    ep_rew_mean          | -94.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 3205        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014506521 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 94.9        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00247    |\n",
      "|    std                  | 0.427       |\n",
      "|    value_loss           | 200         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 189000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -92.47\n",
      "Num timesteps: 190000\n",
      "Best mean reward: -85.76 - Last mean reward per episode: -85.38\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.44        |\n",
      "|    ep_rew_mean          | -99         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 3242        |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022730947 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 86.6        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    std                  | 0.425       |\n",
      "|    value_loss           | 211         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 191000\n",
      "Best mean reward: -85.38 - Last mean reward per episode: -94.33\n",
      "Num timesteps: 192000\n",
      "Best mean reward: -85.38 - Last mean reward per episode: -89.98\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.18       |\n",
      "|    ep_rew_mean          | -96.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 58         |\n",
      "|    iterations           | 94         |\n",
      "|    time_elapsed         | 3279       |\n",
      "|    total_timesteps      | 192512     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01663401 |\n",
      "|    clip_fraction        | 0.241      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 111        |\n",
      "|    n_updates            | 930        |\n",
      "|    policy_gradient_loss | -0.00255   |\n",
      "|    std                  | 0.423      |\n",
      "|    value_loss           | 176        |\n",
      "----------------------------------------\n",
      "Num timesteps: 193000\n",
      "Best mean reward: -85.38 - Last mean reward per episode: -97.73\n",
      "Num timesteps: 194000\n",
      "Best mean reward: -85.38 - Last mean reward per episode: -83.32\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.3         |\n",
      "|    ep_rew_mean          | -93.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 3314        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015537577 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00257    |\n",
      "|    std                  | 0.416       |\n",
      "|    value_loss           | 186         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 195000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -92.53\n",
      "Num timesteps: 196000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -92.92\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.1         |\n",
      "|    ep_rew_mean          | -92.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 3351        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011311961 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 81.5        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    std                  | 0.417       |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 197000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -91.89\n",
      "Num timesteps: 198000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -94.95\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.12       |\n",
      "|    ep_rew_mean          | -94.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 58         |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 3388       |\n",
      "|    total_timesteps      | 198656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01517478 |\n",
      "|    clip_fraction        | 0.217      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.07      |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 85.5       |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | 0.00185    |\n",
      "|    std                  | 0.418      |\n",
      "|    value_loss           | 196        |\n",
      "----------------------------------------\n",
      "Num timesteps: 199000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -91.73\n",
      "Num timesteps: 200000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -86.61\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.67        |\n",
      "|    ep_rew_mean          | -88.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 3424        |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014375418 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 101         |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | 0.00364     |\n",
      "|    std                  | 0.418       |\n",
      "|    value_loss           | 162         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 201000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -88.73\n",
      "Num timesteps: 202000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -91.53\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.96        |\n",
      "|    ep_rew_mean          | -89.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 99          |\n",
      "|    time_elapsed         | 3461        |\n",
      "|    total_timesteps      | 202752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015985686 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.933       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 90.8        |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | 0.000134    |\n",
      "|    std                  | 0.413       |\n",
      "|    value_loss           | 183         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 203000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -88.08\n",
      "Num timesteps: 204000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -94.78\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.71       |\n",
      "|    ep_rew_mean          | -85.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 58         |\n",
      "|    iterations           | 100        |\n",
      "|    time_elapsed         | 3498       |\n",
      "|    total_timesteps      | 204800     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02063781 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.02      |\n",
      "|    explained_variance   | 0.906      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 93.9       |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.00259   |\n",
      "|    std                  | 0.403      |\n",
      "|    value_loss           | 236        |\n",
      "----------------------------------------\n",
      "Num timesteps: 205000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -87.84\n",
      "Num timesteps: 206000\n",
      "Best mean reward: -83.32 - Last mean reward per episode: -81.35\n",
      "Saving new best model to log/best_model\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.43       |\n",
      "|    ep_rew_mean          | -97.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 58         |\n",
      "|    iterations           | 101        |\n",
      "|    time_elapsed         | 3532       |\n",
      "|    total_timesteps      | 206848     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01742767 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1         |\n",
      "|    explained_variance   | 0.913      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 89.6       |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | 0.00269    |\n",
      "|    std                  | 0.403      |\n",
      "|    value_loss           | 223        |\n",
      "----------------------------------------\n",
      "Num timesteps: 207000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -91.64\n",
      "Num timesteps: 208000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -92.07\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.4         |\n",
      "|    ep_rew_mean          | -96.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 102         |\n",
      "|    time_elapsed         | 3569        |\n",
      "|    total_timesteps      | 208896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012899389 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.992      |\n",
      "|    explained_variance   | 0.901       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 142         |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.00205    |\n",
      "|    std                  | 0.399       |\n",
      "|    value_loss           | 233         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 209000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -93.41\n",
      "Num timesteps: 210000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -92.97\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.13       |\n",
      "|    ep_rew_mean          | -91.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 58         |\n",
      "|    iterations           | 103        |\n",
      "|    time_elapsed         | 3607       |\n",
      "|    total_timesteps      | 210944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01023048 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.975     |\n",
      "|    explained_variance   | 0.919      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 121        |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.00105   |\n",
      "|    std                  | 0.397      |\n",
      "|    value_loss           | 207        |\n",
      "----------------------------------------\n",
      "Num timesteps: 211000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -92.19\n",
      "Num timesteps: 212000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -91.10\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.86        |\n",
      "|    ep_rew_mean          | -90.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 104         |\n",
      "|    time_elapsed         | 3641        |\n",
      "|    total_timesteps      | 212992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020541001 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | 0.892       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 142         |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | 0.00526     |\n",
      "|    std                  | 0.392       |\n",
      "|    value_loss           | 260         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 213000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -91.16\n",
      "Num timesteps: 214000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -88.27\n",
      "Num timesteps: 215000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -89.25\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.85        |\n",
      "|    ep_rew_mean          | -90         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 105         |\n",
      "|    time_elapsed         | 3680        |\n",
      "|    total_timesteps      | 215040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019059565 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.951      |\n",
      "|    explained_variance   | 0.886       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 91.6        |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.00201    |\n",
      "|    std                  | 0.394       |\n",
      "|    value_loss           | 262         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 216000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -85.65\n",
      "Num timesteps: 217000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -93.08\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.09        |\n",
      "|    ep_rew_mean          | -92.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 106         |\n",
      "|    time_elapsed         | 3715        |\n",
      "|    total_timesteps      | 217088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016435668 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.949      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 170         |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    std                  | 0.392       |\n",
      "|    value_loss           | 250         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 218000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -84.64\n",
      "Num timesteps: 219000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -91.36\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.98        |\n",
      "|    ep_rew_mean          | -90.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 107         |\n",
      "|    time_elapsed         | 3754        |\n",
      "|    total_timesteps      | 219136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019032113 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.942      |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.00136    |\n",
      "|    std                  | 0.39        |\n",
      "|    value_loss           | 286         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 220000\n",
      "Best mean reward: -81.35 - Last mean reward per episode: -80.33\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 221000\n",
      "Best mean reward: -80.33 - Last mean reward per episode: -78.12\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.48        |\n",
      "|    ep_rew_mean          | -87.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 108         |\n",
      "|    time_elapsed         | 3790        |\n",
      "|    total_timesteps      | 221184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016673733 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.926      |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 121         |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | 0.00135     |\n",
      "|    std                  | 0.386       |\n",
      "|    value_loss           | 270         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 222000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -85.34\n",
      "Num timesteps: 223000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -88.27\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.52        |\n",
      "|    ep_rew_mean          | -84.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 109         |\n",
      "|    time_elapsed         | 3827        |\n",
      "|    total_timesteps      | 223232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017541595 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.91       |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 162         |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | 0.0036      |\n",
      "|    std                  | 0.383       |\n",
      "|    value_loss           | 311         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 224000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -85.94\n",
      "Num timesteps: 225000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -91.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.75        |\n",
      "|    ep_rew_mean          | -90.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 110         |\n",
      "|    time_elapsed         | 3867        |\n",
      "|    total_timesteps      | 225280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013643261 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.89       |\n",
      "|    explained_variance   | 0.875       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 127         |\n",
      "|    n_updates            | 1090        |\n",
      "|    policy_gradient_loss | 0.00607     |\n",
      "|    std                  | 0.38        |\n",
      "|    value_loss           | 277         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 226000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -93.02\n",
      "Num timesteps: 227000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -91.02\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.93        |\n",
      "|    ep_rew_mean          | -89.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 111         |\n",
      "|    time_elapsed         | 3904        |\n",
      "|    total_timesteps      | 227328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019543527 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.883      |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 165         |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.00249    |\n",
      "|    std                  | 0.381       |\n",
      "|    value_loss           | 299         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 228000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -86.60\n",
      "Num timesteps: 229000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -89.14\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.62        |\n",
      "|    ep_rew_mean          | -85.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 112         |\n",
      "|    time_elapsed         | 3940        |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018580867 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.886      |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 169         |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | 0.00188     |\n",
      "|    std                  | 0.38        |\n",
      "|    value_loss           | 352         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 230000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -92.55\n",
      "Num timesteps: 231000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -91.49\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.96       |\n",
      "|    ep_rew_mean          | -90.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 58         |\n",
      "|    iterations           | 113        |\n",
      "|    time_elapsed         | 3978       |\n",
      "|    total_timesteps      | 231424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03621778 |\n",
      "|    clip_fraction        | 0.262      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.877     |\n",
      "|    explained_variance   | 0.886      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 126        |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | 0.00399    |\n",
      "|    std                  | 0.377      |\n",
      "|    value_loss           | 258        |\n",
      "----------------------------------------\n",
      "Num timesteps: 232000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -94.15\n",
      "Num timesteps: 233000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -82.93\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.67        |\n",
      "|    ep_rew_mean          | -92.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 114         |\n",
      "|    time_elapsed         | 4016        |\n",
      "|    total_timesteps      | 233472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017220452 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.858      |\n",
      "|    explained_variance   | 0.863       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 150         |\n",
      "|    n_updates            | 1130        |\n",
      "|    policy_gradient_loss | 0.00307     |\n",
      "|    std                  | 0.373       |\n",
      "|    value_loss           | 301         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 234000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -81.76\n",
      "Num timesteps: 235000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -83.80\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.58        |\n",
      "|    ep_rew_mean          | -86         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 115         |\n",
      "|    time_elapsed         | 4052        |\n",
      "|    total_timesteps      | 235520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017994137 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.84       |\n",
      "|    explained_variance   | 0.857       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 203         |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.000291   |\n",
      "|    std                  | 0.37        |\n",
      "|    value_loss           | 327         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 236000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -89.17\n",
      "Num timesteps: 237000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -83.42\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.6        |\n",
      "|    ep_rew_mean          | -87.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 58         |\n",
      "|    iterations           | 116        |\n",
      "|    time_elapsed         | 4089       |\n",
      "|    total_timesteps      | 237568     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01224358 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.812     |\n",
      "|    explained_variance   | 0.878      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 155        |\n",
      "|    n_updates            | 1150       |\n",
      "|    policy_gradient_loss | 0.00173    |\n",
      "|    std                  | 0.364      |\n",
      "|    value_loss           | 296        |\n",
      "----------------------------------------\n",
      "Num timesteps: 238000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -86.77\n",
      "Num timesteps: 239000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -87.60\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.79        |\n",
      "|    ep_rew_mean          | -86.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 117         |\n",
      "|    time_elapsed         | 4126        |\n",
      "|    total_timesteps      | 239616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019545782 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.799      |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 203         |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    std                  | 0.364       |\n",
      "|    value_loss           | 298         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 240000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -92.86\n",
      "Num timesteps: 241000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -93.98\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.67        |\n",
      "|    ep_rew_mean          | -84.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 58          |\n",
      "|    iterations           | 118         |\n",
      "|    time_elapsed         | 4166        |\n",
      "|    total_timesteps      | 241664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013311135 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.788      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | 0.00334     |\n",
      "|    std                  | 0.362       |\n",
      "|    value_loss           | 324         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 242000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -84.85\n",
      "Num timesteps: 243000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -83.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.82        |\n",
      "|    ep_rew_mean          | -89.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 4203        |\n",
      "|    total_timesteps      | 243712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014590234 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.783      |\n",
      "|    explained_variance   | 0.835       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 209         |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.00101    |\n",
      "|    std                  | 0.363       |\n",
      "|    value_loss           | 338         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 244000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -90.68\n",
      "Num timesteps: 245000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -83.58\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.22        |\n",
      "|    ep_rew_mean          | -81.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 120         |\n",
      "|    time_elapsed         | 4239        |\n",
      "|    total_timesteps      | 245760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014552772 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.777      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 124         |\n",
      "|    n_updates            | 1190        |\n",
      "|    policy_gradient_loss | -0.000346   |\n",
      "|    std                  | 0.362       |\n",
      "|    value_loss           | 293         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 246000\n",
      "Best mean reward: -78.12 - Last mean reward per episode: -76.35\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 247000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -82.99\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.21        |\n",
      "|    ep_rew_mean          | -95.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 121         |\n",
      "|    time_elapsed         | 4274        |\n",
      "|    total_timesteps      | 247808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014111925 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.757      |\n",
      "|    explained_variance   | 0.851       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.00235    |\n",
      "|    std                  | 0.356       |\n",
      "|    value_loss           | 317         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 248000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -98.18\n",
      "Num timesteps: 249000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -82.55\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.92        |\n",
      "|    ep_rew_mean          | -94.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 4311        |\n",
      "|    total_timesteps      | 249856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019699024 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.735      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 178         |\n",
      "|    n_updates            | 1210        |\n",
      "|    policy_gradient_loss | 0.00533     |\n",
      "|    std                  | 0.354       |\n",
      "|    value_loss           | 350         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 250000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -98.56\n",
      "Num timesteps: 251000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -92.84\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.88        |\n",
      "|    ep_rew_mean          | -85.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 123         |\n",
      "|    time_elapsed         | 4346        |\n",
      "|    total_timesteps      | 251904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008761479 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.722      |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 127         |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | 0.00449     |\n",
      "|    std                  | 0.35        |\n",
      "|    value_loss           | 361         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 252000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -87.89\n",
      "Num timesteps: 253000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -91.39\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.97        |\n",
      "|    ep_rew_mean          | -90.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 124         |\n",
      "|    time_elapsed         | 4382        |\n",
      "|    total_timesteps      | 253952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013208011 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.691      |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 155         |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | 0.00154     |\n",
      "|    std                  | 0.344       |\n",
      "|    value_loss           | 340         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 254000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -92.27\n",
      "Num timesteps: 255000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -83.27\n",
      "Num timesteps: 256000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -94.80\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.21        |\n",
      "|    ep_rew_mean          | -94.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 125         |\n",
      "|    time_elapsed         | 4419        |\n",
      "|    total_timesteps      | 256000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012653759 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | 0.844       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 168         |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | 0.00249     |\n",
      "|    std                  | 0.346       |\n",
      "|    value_loss           | 326         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 257000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -86.20\n",
      "Num timesteps: 258000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -78.44\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.48        |\n",
      "|    ep_rew_mean          | -78.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 4456        |\n",
      "|    total_timesteps      | 258048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013058927 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.677      |\n",
      "|    explained_variance   | 0.856       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 132         |\n",
      "|    n_updates            | 1250        |\n",
      "|    policy_gradient_loss | 0.000199    |\n",
      "|    std                  | 0.343       |\n",
      "|    value_loss           | 323         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 259000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -84.56\n",
      "Num timesteps: 260000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -84.29\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.56        |\n",
      "|    ep_rew_mean          | -83.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 127         |\n",
      "|    time_elapsed         | 4493        |\n",
      "|    total_timesteps      | 260096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015810192 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 171         |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | 0.000464    |\n",
      "|    std                  | 0.341       |\n",
      "|    value_loss           | 325         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 261000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -82.41\n",
      "Num timesteps: 262000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -82.78\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.71        |\n",
      "|    ep_rew_mean          | -86.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 128         |\n",
      "|    time_elapsed         | 4530        |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010375379 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.64       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 143         |\n",
      "|    n_updates            | 1270        |\n",
      "|    policy_gradient_loss | 0.0034      |\n",
      "|    std                  | 0.334       |\n",
      "|    value_loss           | 341         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 263000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -86.29\n",
      "Num timesteps: 264000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -83.70\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.94        |\n",
      "|    ep_rew_mean          | -88.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 129         |\n",
      "|    time_elapsed         | 4565        |\n",
      "|    total_timesteps      | 264192      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012199529 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 173         |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | 0.00276     |\n",
      "|    std                  | 0.331       |\n",
      "|    value_loss           | 315         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 265000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -83.56\n",
      "Num timesteps: 266000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -85.69\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.37        |\n",
      "|    ep_rew_mean          | -81.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 130         |\n",
      "|    time_elapsed         | 4606        |\n",
      "|    total_timesteps      | 266240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011321297 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 150         |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | 0.00235     |\n",
      "|    std                  | 0.326       |\n",
      "|    value_loss           | 381         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 267000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -85.84\n",
      "Num timesteps: 268000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -87.63\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.62       |\n",
      "|    ep_rew_mean          | -86.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 131        |\n",
      "|    time_elapsed         | 4643       |\n",
      "|    total_timesteps      | 268288     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04903941 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.565     |\n",
      "|    explained_variance   | 0.809      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 169        |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | 0.00274    |\n",
      "|    std                  | 0.324      |\n",
      "|    value_loss           | 358        |\n",
      "----------------------------------------\n",
      "Num timesteps: 269000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -88.11\n",
      "Num timesteps: 270000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -86.39\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.61        |\n",
      "|    ep_rew_mean          | -84.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 132         |\n",
      "|    time_elapsed         | 4681        |\n",
      "|    total_timesteps      | 270336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026434181 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | 0.846       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 160         |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | 0.00496     |\n",
      "|    std                  | 0.32        |\n",
      "|    value_loss           | 325         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 271000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -85.10\n",
      "Num timesteps: 272000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -88.83\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.56        |\n",
      "|    ep_rew_mean          | -85.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 4717        |\n",
      "|    total_timesteps      | 272384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011932611 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.829       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 217         |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | 0.00176     |\n",
      "|    std                  | 0.32        |\n",
      "|    value_loss           | 347         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 273000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -80.20\n",
      "Num timesteps: 274000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -80.06\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.49        |\n",
      "|    ep_rew_mean          | -83.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 134         |\n",
      "|    time_elapsed         | 4755        |\n",
      "|    total_timesteps      | 274432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014462797 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.833       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 135         |\n",
      "|    n_updates            | 1330        |\n",
      "|    policy_gradient_loss | 0.00201     |\n",
      "|    std                  | 0.317       |\n",
      "|    value_loss           | 350         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 275000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -82.58\n",
      "Num timesteps: 276000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -86.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.65        |\n",
      "|    ep_rew_mean          | -84.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 135         |\n",
      "|    time_elapsed         | 4792        |\n",
      "|    total_timesteps      | 276480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014672775 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.504      |\n",
      "|    explained_variance   | 0.843       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 149         |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.000194   |\n",
      "|    std                  | 0.313       |\n",
      "|    value_loss           | 330         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 277000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -81.98\n",
      "Num timesteps: 278000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -82.22\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.9        |\n",
      "|    ep_rew_mean          | -85.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 136        |\n",
      "|    time_elapsed         | 4829       |\n",
      "|    total_timesteps      | 278528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02604909 |\n",
      "|    clip_fraction        | 0.2        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.485     |\n",
      "|    explained_variance   | 0.849      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 169        |\n",
      "|    n_updates            | 1350       |\n",
      "|    policy_gradient_loss | 0.00415    |\n",
      "|    std                  | 0.309      |\n",
      "|    value_loss           | 296        |\n",
      "----------------------------------------\n",
      "Num timesteps: 279000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -83.21\n",
      "Num timesteps: 280000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -89.63\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.49        |\n",
      "|    ep_rew_mean          | -85.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 137         |\n",
      "|    time_elapsed         | 4868        |\n",
      "|    total_timesteps      | 280576      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015849825 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | 0.819       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 207         |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | 0.000996    |\n",
      "|    std                  | 0.309       |\n",
      "|    value_loss           | 358         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 281000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -90.76\n",
      "Num timesteps: 282000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -86.53\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.46        |\n",
      "|    ep_rew_mean          | -87         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 4906        |\n",
      "|    total_timesteps      | 282624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012276847 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 161         |\n",
      "|    n_updates            | 1370        |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    std                  | 0.306       |\n",
      "|    value_loss           | 424         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 283000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -85.92\n",
      "Num timesteps: 284000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -94.16\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.34        |\n",
      "|    ep_rew_mean          | -82.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 139         |\n",
      "|    time_elapsed         | 4945        |\n",
      "|    total_timesteps      | 284672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011427619 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.452      |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 179         |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | 0.00491     |\n",
      "|    std                  | 0.306       |\n",
      "|    value_loss           | 367         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 285000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -85.40\n",
      "Num timesteps: 286000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -83.65\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.31        |\n",
      "|    ep_rew_mean          | -83.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 140         |\n",
      "|    time_elapsed         | 4981        |\n",
      "|    total_timesteps      | 286720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016609214 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.452      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 222         |\n",
      "|    n_updates            | 1390        |\n",
      "|    policy_gradient_loss | 0.00257     |\n",
      "|    std                  | 0.306       |\n",
      "|    value_loss           | 336         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 287000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -83.73\n",
      "Num timesteps: 288000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -89.27\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.11       |\n",
      "|    ep_rew_mean          | -88        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 141        |\n",
      "|    time_elapsed         | 5018       |\n",
      "|    total_timesteps      | 288768     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01488529 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.45      |\n",
      "|    explained_variance   | 0.815      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 205        |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | 0.00266    |\n",
      "|    std                  | 0.305      |\n",
      "|    value_loss           | 369        |\n",
      "----------------------------------------\n",
      "Num timesteps: 289000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -87.18\n",
      "Num timesteps: 290000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -85.34\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.43        |\n",
      "|    ep_rew_mean          | -83.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 142         |\n",
      "|    time_elapsed         | 5057        |\n",
      "|    total_timesteps      | 290816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026194852 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.44       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 176         |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.000471   |\n",
      "|    std                  | 0.304       |\n",
      "|    value_loss           | 343         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 291000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -81.57\n",
      "Num timesteps: 292000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -83.51\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.77       |\n",
      "|    ep_rew_mean          | -89.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 143        |\n",
      "|    time_elapsed         | 5093       |\n",
      "|    total_timesteps      | 292864     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01566358 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.438     |\n",
      "|    explained_variance   | 0.85       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 146        |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | 0.00312    |\n",
      "|    std                  | 0.304      |\n",
      "|    value_loss           | 301        |\n",
      "----------------------------------------\n",
      "Num timesteps: 293000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -83.97\n",
      "Num timesteps: 294000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -89.08\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.82       |\n",
      "|    ep_rew_mean          | -90.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 144        |\n",
      "|    time_elapsed         | 5130       |\n",
      "|    total_timesteps      | 294912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01299753 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.424     |\n",
      "|    explained_variance   | 0.82       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 181        |\n",
      "|    n_updates            | 1430       |\n",
      "|    policy_gradient_loss | 0.00123    |\n",
      "|    std                  | 0.3        |\n",
      "|    value_loss           | 369        |\n",
      "----------------------------------------\n",
      "Num timesteps: 295000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -91.64\n",
      "Num timesteps: 296000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -78.75\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.27        |\n",
      "|    ep_rew_mean          | -79.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 145         |\n",
      "|    time_elapsed         | 5167        |\n",
      "|    total_timesteps      | 296960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016104378 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 153         |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | 0.00311     |\n",
      "|    std                  | 0.294       |\n",
      "|    value_loss           | 337         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 297000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -83.28\n",
      "Num timesteps: 298000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -82.90\n",
      "Num timesteps: 299000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -84.60\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.91        |\n",
      "|    ep_rew_mean          | -84.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 5203        |\n",
      "|    total_timesteps      | 299008      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019311111 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.379      |\n",
      "|    explained_variance   | 0.808       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 255         |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.00179    |\n",
      "|    std                  | 0.295       |\n",
      "|    value_loss           | 370         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 300000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -79.12\n",
      "Num timesteps: 301000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -84.66\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.69        |\n",
      "|    ep_rew_mean          | -87.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 147         |\n",
      "|    time_elapsed         | 5240        |\n",
      "|    total_timesteps      | 301056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017228907 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.364      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 173         |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | 0.00439     |\n",
      "|    std                  | 0.29        |\n",
      "|    value_loss           | 348         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 302000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -85.82\n",
      "Num timesteps: 303000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -86.39\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.66        |\n",
      "|    ep_rew_mean          | -87.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 148         |\n",
      "|    time_elapsed         | 5275        |\n",
      "|    total_timesteps      | 303104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015163483 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 159         |\n",
      "|    n_updates            | 1470        |\n",
      "|    policy_gradient_loss | 0.00555     |\n",
      "|    std                  | 0.288       |\n",
      "|    value_loss           | 327         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 304000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -85.63\n",
      "Num timesteps: 305000\n",
      "Best mean reward: -76.35 - Last mean reward per episode: -70.02\n",
      "Saving new best model to log/best_model\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.54       |\n",
      "|    ep_rew_mean          | -68.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 149        |\n",
      "|    time_elapsed         | 5313       |\n",
      "|    total_timesteps      | 305152     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01415156 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.336     |\n",
      "|    explained_variance   | 0.831      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 191        |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | 0.00589    |\n",
      "|    std                  | 0.29       |\n",
      "|    value_loss           | 342        |\n",
      "----------------------------------------\n",
      "Num timesteps: 306000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -89.95\n",
      "Num timesteps: 307000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -86.73\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.26        |\n",
      "|    ep_rew_mean          | -91.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 5350        |\n",
      "|    total_timesteps      | 307200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015007767 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 222         |\n",
      "|    n_updates            | 1490        |\n",
      "|    policy_gradient_loss | 0.00226     |\n",
      "|    std                  | 0.288       |\n",
      "|    value_loss           | 385         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 308000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.00\n",
      "Num timesteps: 309000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.83\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.16        |\n",
      "|    ep_rew_mean          | -91.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 151         |\n",
      "|    time_elapsed         | 5389        |\n",
      "|    total_timesteps      | 309248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018951118 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 219         |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    std                  | 0.283       |\n",
      "|    value_loss           | 386         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 310000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -78.97\n",
      "Num timesteps: 311000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.20\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.26        |\n",
      "|    ep_rew_mean          | -80.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 152         |\n",
      "|    time_elapsed         | 5425        |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018250816 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.293      |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 173         |\n",
      "|    n_updates            | 1510        |\n",
      "|    policy_gradient_loss | 0.00285     |\n",
      "|    std                  | 0.282       |\n",
      "|    value_loss           | 333         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 312000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.02\n",
      "Num timesteps: 313000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.48\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.85        |\n",
      "|    ep_rew_mean          | -89.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 5462        |\n",
      "|    total_timesteps      | 313344      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020551477 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.284      |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 154         |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | 0.00253     |\n",
      "|    std                  | 0.281       |\n",
      "|    value_loss           | 335         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 314000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -71.24\n",
      "Num timesteps: 315000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -74.02\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.24        |\n",
      "|    ep_rew_mean          | -79.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 154         |\n",
      "|    time_elapsed         | 5500        |\n",
      "|    total_timesteps      | 315392      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009392988 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.26       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 205         |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.000645   |\n",
      "|    std                  | 0.276       |\n",
      "|    value_loss           | 377         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 316000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.73\n",
      "Num timesteps: 317000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.35        |\n",
      "|    ep_rew_mean          | -76.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 155         |\n",
      "|    time_elapsed         | 5538        |\n",
      "|    total_timesteps      | 317440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019205686 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.22       |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 161         |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | 0.00125     |\n",
      "|    std                  | 0.271       |\n",
      "|    value_loss           | 341         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 318000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.84\n",
      "Num timesteps: 319000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -89.94\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.52        |\n",
      "|    ep_rew_mean          | -86.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 156         |\n",
      "|    time_elapsed         | 5575        |\n",
      "|    total_timesteps      | 319488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022921933 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.202      |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 232         |\n",
      "|    n_updates            | 1550        |\n",
      "|    policy_gradient_loss | 0.00906     |\n",
      "|    std                  | 0.27        |\n",
      "|    value_loss           | 334         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 320000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -95.26\n",
      "Num timesteps: 321000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -79.95\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.81        |\n",
      "|    ep_rew_mean          | -93.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 157         |\n",
      "|    time_elapsed         | 5613        |\n",
      "|    total_timesteps      | 321536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022524022 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.189      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 213         |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | 0.00342     |\n",
      "|    std                  | 0.267       |\n",
      "|    value_loss           | 361         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 322000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.72\n",
      "Num timesteps: 323000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -86.06\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.03       |\n",
      "|    ep_rew_mean          | -88.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 158        |\n",
      "|    time_elapsed         | 5653       |\n",
      "|    total_timesteps      | 323584     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03104765 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.186     |\n",
      "|    explained_variance   | 0.822      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 219        |\n",
      "|    n_updates            | 1570       |\n",
      "|    policy_gradient_loss | 0.00546    |\n",
      "|    std                  | 0.27       |\n",
      "|    value_loss           | 356        |\n",
      "----------------------------------------\n",
      "Num timesteps: 324000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.46\n",
      "Num timesteps: 325000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -87.15\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.27        |\n",
      "|    ep_rew_mean          | -92.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 159         |\n",
      "|    time_elapsed         | 5689        |\n",
      "|    total_timesteps      | 325632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016882349 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.18       |\n",
      "|    explained_variance   | 0.838       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 143         |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | 0.00727     |\n",
      "|    std                  | 0.265       |\n",
      "|    value_loss           | 310         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 326000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -90.19\n",
      "Num timesteps: 327000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -78.95\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.58        |\n",
      "|    ep_rew_mean          | -82.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 160         |\n",
      "|    time_elapsed         | 5726        |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049478225 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.181      |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 158         |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | 0.00674     |\n",
      "|    std                  | 0.269       |\n",
      "|    value_loss           | 308         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 328000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.21\n",
      "Num timesteps: 329000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -89.77\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.92        |\n",
      "|    ep_rew_mean          | -75.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 161         |\n",
      "|    time_elapsed         | 5765        |\n",
      "|    total_timesteps      | 329728      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023717877 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | 0.0143      |\n",
      "|    std                  | 0.27        |\n",
      "|    value_loss           | 286         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 330000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.67\n",
      "Num timesteps: 331000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -73.07\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.57        |\n",
      "|    ep_rew_mean          | -88.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 162         |\n",
      "|    time_elapsed         | 5804        |\n",
      "|    total_timesteps      | 331776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025995035 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.197      |\n",
      "|    explained_variance   | 0.842       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 194         |\n",
      "|    n_updates            | 1610        |\n",
      "|    policy_gradient_loss | 0.0056      |\n",
      "|    std                  | 0.269       |\n",
      "|    value_loss           | 306         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 332000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.66\n",
      "Num timesteps: 333000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.03\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.83        |\n",
      "|    ep_rew_mean          | -85         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 163         |\n",
      "|    time_elapsed         | 5842        |\n",
      "|    total_timesteps      | 333824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015301455 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.171      |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 133         |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | 0.00776     |\n",
      "|    std                  | 0.264       |\n",
      "|    value_loss           | 401         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 334000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -88.74\n",
      "Num timesteps: 335000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -79.40\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.32        |\n",
      "|    ep_rew_mean          | -80         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 164         |\n",
      "|    time_elapsed         | 5879        |\n",
      "|    total_timesteps      | 335872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014271379 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.153      |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 218         |\n",
      "|    n_updates            | 1630        |\n",
      "|    policy_gradient_loss | 0.00331     |\n",
      "|    std                  | 0.263       |\n",
      "|    value_loss           | 395         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 336000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -78.20\n",
      "Num timesteps: 337000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -93.01\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.39       |\n",
      "|    ep_rew_mean          | -81.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 165        |\n",
      "|    time_elapsed         | 5916       |\n",
      "|    total_timesteps      | 337920     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02828142 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.144     |\n",
      "|    explained_variance   | 0.812      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 190        |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | 0.00335    |\n",
      "|    std                  | 0.262      |\n",
      "|    value_loss           | 365        |\n",
      "----------------------------------------\n",
      "Num timesteps: 338000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.02\n",
      "Num timesteps: 339000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -90.31\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.52        |\n",
      "|    ep_rew_mean          | -82.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 5953        |\n",
      "|    total_timesteps      | 339968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045722164 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.116      |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 192         |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | 0.00842     |\n",
      "|    std                  | 0.256       |\n",
      "|    value_loss           | 389         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 340000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.55\n",
      "Num timesteps: 341000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -79.63\n",
      "Num timesteps: 342000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -86.76\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.08        |\n",
      "|    ep_rew_mean          | -87.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 167         |\n",
      "|    time_elapsed         | 5992        |\n",
      "|    total_timesteps      | 342016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022622919 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.102      |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 187         |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | 0.000638    |\n",
      "|    std                  | 0.257       |\n",
      "|    value_loss           | 416         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 343000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.52\n",
      "Num timesteps: 344000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.88\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 6.36         |\n",
      "|    ep_rew_mean          | -84          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 57           |\n",
      "|    iterations           | 168          |\n",
      "|    time_elapsed         | 6027         |\n",
      "|    total_timesteps      | 344064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0144733805 |\n",
      "|    clip_fraction        | 0.196        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0917      |\n",
      "|    explained_variance   | 0.791        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 204          |\n",
      "|    n_updates            | 1670         |\n",
      "|    policy_gradient_loss | 0.00199      |\n",
      "|    std                  | 0.254        |\n",
      "|    value_loss           | 426          |\n",
      "------------------------------------------\n",
      "Num timesteps: 345000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.97\n",
      "Num timesteps: 346000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -79.72\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.12       |\n",
      "|    ep_rew_mean          | -81.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 169        |\n",
      "|    time_elapsed         | 6064       |\n",
      "|    total_timesteps      | 346112     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04474341 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.07      |\n",
      "|    explained_variance   | 0.782      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 240        |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | 0.00483    |\n",
      "|    std                  | 0.251      |\n",
      "|    value_loss           | 391        |\n",
      "----------------------------------------\n",
      "Num timesteps: 347000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -86.01\n",
      "Num timesteps: 348000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -77.75\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.3         |\n",
      "|    ep_rew_mean          | -83.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 170         |\n",
      "|    time_elapsed         | 6104        |\n",
      "|    total_timesteps      | 348160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014466784 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0636     |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 252         |\n",
      "|    n_updates            | 1690        |\n",
      "|    policy_gradient_loss | 0.00931     |\n",
      "|    std                  | 0.252       |\n",
      "|    value_loss           | 429         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 349000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -88.78\n",
      "Num timesteps: 350000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.24\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.83        |\n",
      "|    ep_rew_mean          | -84.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 171         |\n",
      "|    time_elapsed         | 6141        |\n",
      "|    total_timesteps      | 350208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016724547 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0377     |\n",
      "|    explained_variance   | 0.775       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | 0.00219     |\n",
      "|    std                  | 0.246       |\n",
      "|    value_loss           | 431         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 351000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -73.37\n",
      "Num timesteps: 352000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.46\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.4         |\n",
      "|    ep_rew_mean          | -76.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 6179        |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019265622 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.016      |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 220         |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | 0.00418     |\n",
      "|    std                  | 0.246       |\n",
      "|    value_loss           | 435         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 353000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.17\n",
      "Num timesteps: 354000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -78.95\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.92        |\n",
      "|    ep_rew_mean          | -71.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 173         |\n",
      "|    time_elapsed         | 6216        |\n",
      "|    total_timesteps      | 354304      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017198175 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0184     |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 170         |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | 0.00377     |\n",
      "|    std                  | 0.246       |\n",
      "|    value_loss           | 380         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 355000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -89.90\n",
      "Num timesteps: 356000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.93        |\n",
      "|    ep_rew_mean          | -84.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 174         |\n",
      "|    time_elapsed         | 6254        |\n",
      "|    total_timesteps      | 356352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024944512 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0188     |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 185         |\n",
      "|    n_updates            | 1730        |\n",
      "|    policy_gradient_loss | 0.00439     |\n",
      "|    std                  | 0.245       |\n",
      "|    value_loss           | 370         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 357000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.51\n",
      "Num timesteps: 358000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.75\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.6         |\n",
      "|    ep_rew_mean          | -81.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 6291        |\n",
      "|    total_timesteps      | 358400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018971652 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.00176     |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 231         |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | 0.0073      |\n",
      "|    std                  | 0.242       |\n",
      "|    value_loss           | 390         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 359000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.55\n",
      "Num timesteps: 360000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -87.21\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.87        |\n",
      "|    ep_rew_mean          | -86.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 176         |\n",
      "|    time_elapsed         | 6327        |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017348373 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0101      |\n",
      "|    explained_variance   | 0.826       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 129         |\n",
      "|    n_updates            | 1750        |\n",
      "|    policy_gradient_loss | 0.00712     |\n",
      "|    std                  | 0.243       |\n",
      "|    value_loss           | 331         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 361000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -74.71\n",
      "Num timesteps: 362000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.49\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.23        |\n",
      "|    ep_rew_mean          | -77.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 177         |\n",
      "|    time_elapsed         | 6366        |\n",
      "|    total_timesteps      | 362496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021847535 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0206      |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 187         |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | 0.00649     |\n",
      "|    std                  | 0.24        |\n",
      "|    value_loss           | 412         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 363000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -88.15\n",
      "Num timesteps: 364000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.18\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.55        |\n",
      "|    ep_rew_mean          | -81.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 178         |\n",
      "|    time_elapsed         | 6403        |\n",
      "|    total_timesteps      | 364544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030306173 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0239      |\n",
      "|    explained_variance   | 0.83        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 172         |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | 0.00345     |\n",
      "|    std                  | 0.241       |\n",
      "|    value_loss           | 311         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 365000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -91.30\n",
      "Num timesteps: 366000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -89.22\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 6.27         |\n",
      "|    ep_rew_mean          | -78.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 56           |\n",
      "|    iterations           | 179          |\n",
      "|    time_elapsed         | 6443         |\n",
      "|    total_timesteps      | 366592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0151850805 |\n",
      "|    clip_fraction        | 0.243        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.0222       |\n",
      "|    explained_variance   | 0.812        |\n",
      "|    learning_rate        | 0.0005       |\n",
      "|    loss                 | 220          |\n",
      "|    n_updates            | 1780         |\n",
      "|    policy_gradient_loss | 0.00277      |\n",
      "|    std                  | 0.241        |\n",
      "|    value_loss           | 345          |\n",
      "------------------------------------------\n",
      "Num timesteps: 367000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.64\n",
      "Num timesteps: 368000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.80\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.72        |\n",
      "|    ep_rew_mean          | -87.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 180         |\n",
      "|    time_elapsed         | 6478        |\n",
      "|    total_timesteps      | 368640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035002545 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0359      |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 206         |\n",
      "|    n_updates            | 1790        |\n",
      "|    policy_gradient_loss | 0.00454     |\n",
      "|    std                  | 0.238       |\n",
      "|    value_loss           | 403         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 369000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.93\n",
      "Num timesteps: 370000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -78.17\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.69        |\n",
      "|    ep_rew_mean          | -83.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 181         |\n",
      "|    time_elapsed         | 6516        |\n",
      "|    total_timesteps      | 370688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018547107 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0506      |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 238         |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    std                  | 0.237       |\n",
      "|    value_loss           | 428         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 371000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -87.39\n",
      "Num timesteps: 372000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.59\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.94        |\n",
      "|    ep_rew_mean          | -85.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 182         |\n",
      "|    time_elapsed         | 6552        |\n",
      "|    total_timesteps      | 372736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017317044 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.0651      |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 121         |\n",
      "|    n_updates            | 1810        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.236       |\n",
      "|    value_loss           | 305         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 373000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -86.47\n",
      "Num timesteps: 374000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -78.26\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.24        |\n",
      "|    ep_rew_mean          | -79.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 183         |\n",
      "|    time_elapsed         | 6590        |\n",
      "|    total_timesteps      | 374784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017452346 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.063       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 185         |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | 0.00276     |\n",
      "|    std                  | 0.237       |\n",
      "|    value_loss           | 391         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 375000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.11\n",
      "Num timesteps: 376000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.73\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.44       |\n",
      "|    ep_rew_mean          | -87.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 184        |\n",
      "|    time_elapsed         | 6629       |\n",
      "|    total_timesteps      | 376832     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02703197 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.0741     |\n",
      "|    explained_variance   | 0.787      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 195        |\n",
      "|    n_updates            | 1830       |\n",
      "|    policy_gradient_loss | 0.0044     |\n",
      "|    std                  | 0.234      |\n",
      "|    value_loss           | 397        |\n",
      "----------------------------------------\n",
      "Num timesteps: 377000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.07\n",
      "Num timesteps: 378000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.99\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.13        |\n",
      "|    ep_rew_mean          | -78.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 185         |\n",
      "|    time_elapsed         | 6666        |\n",
      "|    total_timesteps      | 378880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018145792 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.11        |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 168         |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | 0.00469     |\n",
      "|    std                  | 0.229       |\n",
      "|    value_loss           | 379         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 379000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -77.38\n",
      "Num timesteps: 380000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -86.48\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.55        |\n",
      "|    ep_rew_mean          | -82.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 186         |\n",
      "|    time_elapsed         | 6705        |\n",
      "|    total_timesteps      | 380928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025056299 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.143       |\n",
      "|    explained_variance   | 0.828       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 162         |\n",
      "|    n_updates            | 1850        |\n",
      "|    policy_gradient_loss | 0.00843     |\n",
      "|    std                  | 0.226       |\n",
      "|    value_loss           | 347         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 381000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.50\n",
      "Num timesteps: 382000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.58\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.78       |\n",
      "|    ep_rew_mean          | -89.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 187        |\n",
      "|    time_elapsed         | 6743       |\n",
      "|    total_timesteps      | 382976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03644547 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.144      |\n",
      "|    explained_variance   | 0.797      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 186        |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | 0.00668    |\n",
      "|    std                  | 0.227      |\n",
      "|    value_loss           | 377        |\n",
      "----------------------------------------\n",
      "Num timesteps: 383000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -88.93\n",
      "Num timesteps: 384000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.88\n",
      "Num timesteps: 385000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.92\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.63        |\n",
      "|    ep_rew_mean          | -83.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 188         |\n",
      "|    time_elapsed         | 6780        |\n",
      "|    total_timesteps      | 385024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024846472 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.126       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 245         |\n",
      "|    n_updates            | 1870        |\n",
      "|    policy_gradient_loss | 0.00392     |\n",
      "|    std                  | 0.23        |\n",
      "|    value_loss           | 383         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 386000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -75.79\n",
      "Num timesteps: 387000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -79.88\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.44        |\n",
      "|    ep_rew_mean          | -82.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 189         |\n",
      "|    time_elapsed         | 6817        |\n",
      "|    total_timesteps      | 387072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025797673 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.13        |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | 0.0057      |\n",
      "|    std                  | 0.228       |\n",
      "|    value_loss           | 324         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 388000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.87\n",
      "Num timesteps: 389000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.78\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.86        |\n",
      "|    ep_rew_mean          | -87.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 190         |\n",
      "|    time_elapsed         | 6854        |\n",
      "|    total_timesteps      | 389120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042183273 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.133       |\n",
      "|    explained_variance   | 0.824       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 111         |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | 0.00837     |\n",
      "|    std                  | 0.228       |\n",
      "|    value_loss           | 348         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 390000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.81\n",
      "Num timesteps: 391000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -88.06\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.68        |\n",
      "|    ep_rew_mean          | -85.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 191         |\n",
      "|    time_elapsed         | 6894        |\n",
      "|    total_timesteps      | 391168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019477963 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.136       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 302         |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | 0.00514     |\n",
      "|    std                  | 0.228       |\n",
      "|    value_loss           | 392         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 392000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -87.39\n",
      "Num timesteps: 393000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.04\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.72       |\n",
      "|    ep_rew_mean          | -82.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 192        |\n",
      "|    time_elapsed         | 6932       |\n",
      "|    total_timesteps      | 393216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02498185 |\n",
      "|    clip_fraction        | 0.24       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.145      |\n",
      "|    explained_variance   | 0.807      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 218        |\n",
      "|    n_updates            | 1910       |\n",
      "|    policy_gradient_loss | 0.00137    |\n",
      "|    std                  | 0.227      |\n",
      "|    value_loss           | 409        |\n",
      "----------------------------------------\n",
      "Num timesteps: 394000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.65\n",
      "Num timesteps: 395000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -79.77\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.3         |\n",
      "|    ep_rew_mean          | -74.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 193         |\n",
      "|    time_elapsed         | 6970        |\n",
      "|    total_timesteps      | 395264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014970593 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.148       |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 152         |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | 0.00162     |\n",
      "|    std                  | 0.227       |\n",
      "|    value_loss           | 411         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 396000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.58\n",
      "Num timesteps: 397000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.52\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.69        |\n",
      "|    ep_rew_mean          | -84.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 194         |\n",
      "|    time_elapsed         | 7009        |\n",
      "|    total_timesteps      | 397312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028769085 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.158       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 138         |\n",
      "|    n_updates            | 1930        |\n",
      "|    policy_gradient_loss | 0.00688     |\n",
      "|    std                  | 0.227       |\n",
      "|    value_loss           | 365         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 398000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.20\n",
      "Num timesteps: 399000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.57\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.58        |\n",
      "|    ep_rew_mean          | -79.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 195         |\n",
      "|    time_elapsed         | 7048        |\n",
      "|    total_timesteps      | 399360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019889228 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.155       |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 202         |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | 0.00641     |\n",
      "|    std                  | 0.227       |\n",
      "|    value_loss           | 414         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 400000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.72\n",
      "Num timesteps: 401000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.99\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.71        |\n",
      "|    ep_rew_mean          | -83.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 196         |\n",
      "|    time_elapsed         | 7085        |\n",
      "|    total_timesteps      | 401408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049465347 |\n",
      "|    clip_fraction        | 0.3         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.154       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 97.5        |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    std                  | 0.228       |\n",
      "|    value_loss           | 396         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 402000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -86.32\n",
      "Num timesteps: 403000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -74.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.2         |\n",
      "|    ep_rew_mean          | -77.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 197         |\n",
      "|    time_elapsed         | 7122        |\n",
      "|    total_timesteps      | 403456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027188512 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.131       |\n",
      "|    explained_variance   | 0.811       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 201         |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | 0.00803     |\n",
      "|    std                  | 0.23        |\n",
      "|    value_loss           | 346         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 404000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -88.61\n",
      "Num timesteps: 405000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.86\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.72        |\n",
      "|    ep_rew_mean          | -80.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 198         |\n",
      "|    time_elapsed         | 7159        |\n",
      "|    total_timesteps      | 405504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017475568 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.135       |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 173         |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | 0.00294     |\n",
      "|    std                  | 0.228       |\n",
      "|    value_loss           | 323         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 406000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -76.40\n",
      "Num timesteps: 407000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -91.88\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.18        |\n",
      "|    ep_rew_mean          | -82         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 199         |\n",
      "|    time_elapsed         | 7199        |\n",
      "|    total_timesteps      | 407552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024012249 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.153       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 161         |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | 0.0103      |\n",
      "|    std                  | 0.227       |\n",
      "|    value_loss           | 392         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 408000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -90.78\n",
      "Num timesteps: 409000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.61\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.11        |\n",
      "|    ep_rew_mean          | -81.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 200         |\n",
      "|    time_elapsed         | 7236        |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022425154 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.155       |\n",
      "|    explained_variance   | 0.783       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 192         |\n",
      "|    n_updates            | 1990        |\n",
      "|    policy_gradient_loss | 0.00207     |\n",
      "|    std                  | 0.227       |\n",
      "|    value_loss           | 406         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 410000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -89.20\n",
      "Num timesteps: 411000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -87.29\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.72        |\n",
      "|    ep_rew_mean          | -85.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 201         |\n",
      "|    time_elapsed         | 7274        |\n",
      "|    total_timesteps      | 411648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013475962 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.162       |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 176         |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | 0.00358     |\n",
      "|    std                  | 0.225       |\n",
      "|    value_loss           | 369         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 412000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.55\n",
      "Num timesteps: 413000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.06\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.46       |\n",
      "|    ep_rew_mean          | -78.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 202        |\n",
      "|    time_elapsed         | 7313       |\n",
      "|    total_timesteps      | 413696     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01976273 |\n",
      "|    clip_fraction        | 0.195      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.151      |\n",
      "|    explained_variance   | 0.779      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 186        |\n",
      "|    n_updates            | 2010       |\n",
      "|    policy_gradient_loss | 0.00508    |\n",
      "|    std                  | 0.229      |\n",
      "|    value_loss           | 422        |\n",
      "----------------------------------------\n",
      "Num timesteps: 414000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -79.31\n",
      "Num timesteps: 415000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -91.65\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.42        |\n",
      "|    ep_rew_mean          | -85.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 203         |\n",
      "|    time_elapsed         | 7351        |\n",
      "|    total_timesteps      | 415744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020684395 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.137       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 209         |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | 0.00827     |\n",
      "|    std                  | 0.23        |\n",
      "|    value_loss           | 355         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 416000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.27\n",
      "Num timesteps: 417000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.88\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.62        |\n",
      "|    ep_rew_mean          | -80.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 204         |\n",
      "|    time_elapsed         | 7390        |\n",
      "|    total_timesteps      | 417792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024102138 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.153       |\n",
      "|    explained_variance   | 0.807       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 152         |\n",
      "|    n_updates            | 2030        |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    std                  | 0.226       |\n",
      "|    value_loss           | 337         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 418000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -79.23\n",
      "Num timesteps: 419000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -89.37\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.99        |\n",
      "|    ep_rew_mean          | -81.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 205         |\n",
      "|    time_elapsed         | 7428        |\n",
      "|    total_timesteps      | 419840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026046406 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.166       |\n",
      "|    explained_variance   | 0.776       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 274         |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | 0.00171     |\n",
      "|    std                  | 0.226       |\n",
      "|    value_loss           | 430         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 420000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.57\n",
      "Num timesteps: 421000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.39\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.53        |\n",
      "|    ep_rew_mean          | -85.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 206         |\n",
      "|    time_elapsed         | 7465        |\n",
      "|    total_timesteps      | 421888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018708078 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.187       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 191         |\n",
      "|    n_updates            | 2050        |\n",
      "|    policy_gradient_loss | 0.00521     |\n",
      "|    std                  | 0.221       |\n",
      "|    value_loss           | 352         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 422000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.44\n",
      "Num timesteps: 423000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -92.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.59        |\n",
      "|    ep_rew_mean          | -83.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 207         |\n",
      "|    time_elapsed         | 7501        |\n",
      "|    total_timesteps      | 423936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015779955 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.202       |\n",
      "|    explained_variance   | 0.801       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 215         |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | 0.00487     |\n",
      "|    std                  | 0.221       |\n",
      "|    value_loss           | 404         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 424000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.94\n",
      "Num timesteps: 425000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.95\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.73        |\n",
      "|    ep_rew_mean          | -88         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 208         |\n",
      "|    time_elapsed         | 7539        |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022825394 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.208       |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 225         |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    std                  | 0.22        |\n",
      "|    value_loss           | 343         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 426000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -86.23\n",
      "Num timesteps: 427000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.01\n",
      "Num timesteps: 428000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -89.43\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.12        |\n",
      "|    ep_rew_mean          | -91         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 209         |\n",
      "|    time_elapsed         | 7578        |\n",
      "|    total_timesteps      | 428032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025843855 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.224       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | 0.0144      |\n",
      "|    std                  | 0.218       |\n",
      "|    value_loss           | 396         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 429000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.23\n",
      "Num timesteps: 430000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -75.16\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.42        |\n",
      "|    ep_rew_mean          | -78.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 210         |\n",
      "|    time_elapsed         | 7618        |\n",
      "|    total_timesteps      | 430080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026864829 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.249       |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 261         |\n",
      "|    n_updates            | 2090        |\n",
      "|    policy_gradient_loss | 0.00437     |\n",
      "|    std                  | 0.216       |\n",
      "|    value_loss           | 345         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 431000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -82.65\n",
      "Num timesteps: 432000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -87.27\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.54        |\n",
      "|    ep_rew_mean          | -86         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 211         |\n",
      "|    time_elapsed         | 7663        |\n",
      "|    total_timesteps      | 432128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024450228 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.262       |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 149         |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | 0.00633     |\n",
      "|    std                  | 0.215       |\n",
      "|    value_loss           | 346         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 433000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -77.69\n",
      "Num timesteps: 434000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.56\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.43        |\n",
      "|    ep_rew_mean          | -77.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 212         |\n",
      "|    time_elapsed         | 7701        |\n",
      "|    total_timesteps      | 434176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024565957 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.272       |\n",
      "|    explained_variance   | 0.809       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 225         |\n",
      "|    n_updates            | 2110        |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    std                  | 0.214       |\n",
      "|    value_loss           | 357         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 435000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -73.24\n",
      "Num timesteps: 436000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -76.14\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.4        |\n",
      "|    ep_rew_mean          | -79.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 213        |\n",
      "|    time_elapsed         | 7738       |\n",
      "|    total_timesteps      | 436224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03218323 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.266      |\n",
      "|    explained_variance   | 0.776      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 145        |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | 0.00846    |\n",
      "|    std                  | 0.216      |\n",
      "|    value_loss           | 388        |\n",
      "----------------------------------------\n",
      "Num timesteps: 437000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -79.22\n",
      "Num timesteps: 438000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -80.23\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.21        |\n",
      "|    ep_rew_mean          | -71.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 214         |\n",
      "|    time_elapsed         | 7779        |\n",
      "|    total_timesteps      | 438272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023026844 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.263       |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 295         |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | 0.0121      |\n",
      "|    std                  | 0.215       |\n",
      "|    value_loss           | 344         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 439000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -75.25\n",
      "Num timesteps: 440000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.18\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1         |\n",
      "|    ep_rew_mean          | -76.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 215         |\n",
      "|    time_elapsed         | 7818        |\n",
      "|    total_timesteps      | 440320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025901482 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.265       |\n",
      "|    explained_variance   | 0.84        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 221         |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | 0.00916     |\n",
      "|    std                  | 0.215       |\n",
      "|    value_loss           | 296         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 441000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -86.71\n",
      "Num timesteps: 442000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.69\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.13        |\n",
      "|    ep_rew_mean          | -78.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 216         |\n",
      "|    time_elapsed         | 7854        |\n",
      "|    total_timesteps      | 442368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030349664 |\n",
      "|    clip_fraction        | 0.301       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.272       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 140         |\n",
      "|    n_updates            | 2150        |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    std                  | 0.214       |\n",
      "|    value_loss           | 334         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 443000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.96\n",
      "Num timesteps: 444000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -70.97\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.19        |\n",
      "|    ep_rew_mean          | -83.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 217         |\n",
      "|    time_elapsed         | 7889        |\n",
      "|    total_timesteps      | 444416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015917562 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.282       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 204         |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | 0.00543     |\n",
      "|    std                  | 0.213       |\n",
      "|    value_loss           | 402         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 445000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -77.61\n",
      "Num timesteps: 446000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -78.93\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.6         |\n",
      "|    ep_rew_mean          | -82.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 218         |\n",
      "|    time_elapsed         | 7923        |\n",
      "|    total_timesteps      | 446464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022300264 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.295       |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 278         |\n",
      "|    n_updates            | 2170        |\n",
      "|    policy_gradient_loss | 0.00309     |\n",
      "|    std                  | 0.212       |\n",
      "|    value_loss           | 385         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 447000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.94\n",
      "Num timesteps: 448000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.20\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.62        |\n",
      "|    ep_rew_mean          | -86.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 219         |\n",
      "|    time_elapsed         | 7956        |\n",
      "|    total_timesteps      | 448512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025898168 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.302       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 200         |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | 0.0054      |\n",
      "|    std                  | 0.211       |\n",
      "|    value_loss           | 332         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 449000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -93.07\n",
      "Num timesteps: 450000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.32\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.31       |\n",
      "|    ep_rew_mean          | -80.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 220        |\n",
      "|    time_elapsed         | 7992       |\n",
      "|    total_timesteps      | 450560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02094525 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.314      |\n",
      "|    explained_variance   | 0.803      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 156        |\n",
      "|    n_updates            | 2190       |\n",
      "|    policy_gradient_loss | 0.00719    |\n",
      "|    std                  | 0.21       |\n",
      "|    value_loss           | 360        |\n",
      "----------------------------------------\n",
      "Num timesteps: 451000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.13\n",
      "Num timesteps: 452000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.53\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 6.81     |\n",
      "|    ep_rew_mean          | -84.1    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 56       |\n",
      "|    iterations           | 221      |\n",
      "|    time_elapsed         | 8026     |\n",
      "|    total_timesteps      | 452608   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.038527 |\n",
      "|    clip_fraction        | 0.258    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | 0.313    |\n",
      "|    explained_variance   | 0.783    |\n",
      "|    learning_rate        | 0.0005   |\n",
      "|    loss                 | 178      |\n",
      "|    n_updates            | 2200     |\n",
      "|    policy_gradient_loss | 0.0105   |\n",
      "|    std                  | 0.21     |\n",
      "|    value_loss           | 381      |\n",
      "--------------------------------------\n",
      "Num timesteps: 453000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -79.81\n",
      "Num timesteps: 454000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -87.81\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.03        |\n",
      "|    ep_rew_mean          | -78.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 8059        |\n",
      "|    total_timesteps      | 454656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022575568 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.332       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 229         |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | 0.00797     |\n",
      "|    std                  | 0.207       |\n",
      "|    value_loss           | 355         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 455000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -76.97\n",
      "Num timesteps: 456000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -76.02\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.5         |\n",
      "|    ep_rew_mean          | -83.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 223         |\n",
      "|    time_elapsed         | 8091        |\n",
      "|    total_timesteps      | 456704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042395886 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.35        |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 181         |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    std                  | 0.205       |\n",
      "|    value_loss           | 330         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 457000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.07\n",
      "Num timesteps: 458000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -81.52\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.6         |\n",
      "|    ep_rew_mean          | -92.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 224         |\n",
      "|    time_elapsed         | 8122        |\n",
      "|    total_timesteps      | 458752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025518905 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.356       |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 168         |\n",
      "|    n_updates            | 2230        |\n",
      "|    policy_gradient_loss | 0.00924     |\n",
      "|    std                  | 0.206       |\n",
      "|    value_loss           | 305         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 459000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -92.99\n",
      "Num timesteps: 460000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -87.08\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.7        |\n",
      "|    ep_rew_mean          | -89.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 225        |\n",
      "|    time_elapsed         | 8154       |\n",
      "|    total_timesteps      | 460800     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02657925 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.354      |\n",
      "|    explained_variance   | 0.792      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 132        |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | 0.00567    |\n",
      "|    std                  | 0.205      |\n",
      "|    value_loss           | 368        |\n",
      "----------------------------------------\n",
      "Num timesteps: 461000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -86.14\n",
      "Num timesteps: 462000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -85.21\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.19        |\n",
      "|    ep_rew_mean          | -78.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 226         |\n",
      "|    time_elapsed         | 8187        |\n",
      "|    total_timesteps      | 462848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022880383 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.372       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | 0.00222     |\n",
      "|    std                  | 0.202       |\n",
      "|    value_loss           | 407         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 463000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -83.40\n",
      "Num timesteps: 464000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -84.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.3         |\n",
      "|    ep_rew_mean          | -74.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 227         |\n",
      "|    time_elapsed         | 8219        |\n",
      "|    total_timesteps      | 464896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018547416 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.383       |\n",
      "|    explained_variance   | 0.812       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 115         |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | 0.0129      |\n",
      "|    std                  | 0.204       |\n",
      "|    value_loss           | 371         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 465000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -76.25\n",
      "Num timesteps: 466000\n",
      "Best mean reward: -70.02 - Last mean reward per episode: -65.95\n",
      "Saving new best model to log/best_model\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.57       |\n",
      "|    ep_rew_mean          | -81.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 228        |\n",
      "|    time_elapsed         | 8251       |\n",
      "|    total_timesteps      | 466944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03254059 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.381      |\n",
      "|    explained_variance   | 0.777      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 178        |\n",
      "|    n_updates            | 2270       |\n",
      "|    policy_gradient_loss | 0.00838    |\n",
      "|    std                  | 0.203      |\n",
      "|    value_loss           | 395        |\n",
      "----------------------------------------\n",
      "Num timesteps: 467000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.90\n",
      "Num timesteps: 468000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -85.44\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.72        |\n",
      "|    ep_rew_mean          | -85         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 229         |\n",
      "|    time_elapsed         | 8283        |\n",
      "|    total_timesteps      | 468992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020711843 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.383       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 188         |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | 0.00273     |\n",
      "|    std                  | 0.202       |\n",
      "|    value_loss           | 380         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 469000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.85\n",
      "Num timesteps: 470000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.20\n",
      "Num timesteps: 471000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -88.88\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.68       |\n",
      "|    ep_rew_mean          | -89.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 230        |\n",
      "|    time_elapsed         | 8315       |\n",
      "|    total_timesteps      | 471040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02103945 |\n",
      "|    clip_fraction        | 0.258      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.383      |\n",
      "|    explained_variance   | 0.776      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 183        |\n",
      "|    n_updates            | 2290       |\n",
      "|    policy_gradient_loss | 0.00997    |\n",
      "|    std                  | 0.203      |\n",
      "|    value_loss           | 403        |\n",
      "----------------------------------------\n",
      "Num timesteps: 472000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.19\n",
      "Num timesteps: 473000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -73.50\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.23        |\n",
      "|    ep_rew_mean          | -77.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 231         |\n",
      "|    time_elapsed         | 8347        |\n",
      "|    total_timesteps      | 473088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024488367 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.385       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 146         |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | 0.00784     |\n",
      "|    std                  | 0.204       |\n",
      "|    value_loss           | 390         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 474000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -75.73\n",
      "Num timesteps: 475000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.85\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.61        |\n",
      "|    ep_rew_mean          | -81.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 232         |\n",
      "|    time_elapsed         | 8380        |\n",
      "|    total_timesteps      | 475136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020774333 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.376       |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 115         |\n",
      "|    n_updates            | 2310        |\n",
      "|    policy_gradient_loss | 0.00841     |\n",
      "|    std                  | 0.204       |\n",
      "|    value_loss           | 370         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 476000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.71\n",
      "Num timesteps: 477000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -69.77\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.62        |\n",
      "|    ep_rew_mean          | -76.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 233         |\n",
      "|    time_elapsed         | 8412        |\n",
      "|    total_timesteps      | 477184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030485198 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.371       |\n",
      "|    explained_variance   | 0.8         |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 148         |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | 0.00998     |\n",
      "|    std                  | 0.204       |\n",
      "|    value_loss           | 354         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 478000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.96\n",
      "Num timesteps: 479000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.44\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.43        |\n",
      "|    ep_rew_mean          | -84.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 234         |\n",
      "|    time_elapsed         | 8444        |\n",
      "|    total_timesteps      | 479232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022747999 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.369       |\n",
      "|    explained_variance   | 0.79        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 204         |\n",
      "|    n_updates            | 2330        |\n",
      "|    policy_gradient_loss | 0.00549     |\n",
      "|    std                  | 0.204       |\n",
      "|    value_loss           | 374         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 480000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.63\n",
      "Num timesteps: 481000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.90\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.19       |\n",
      "|    ep_rew_mean          | -76.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 235        |\n",
      "|    time_elapsed         | 8477       |\n",
      "|    total_timesteps      | 481280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02301997 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.381      |\n",
      "|    explained_variance   | 0.804      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 190        |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | 0.00457    |\n",
      "|    std                  | 0.203      |\n",
      "|    value_loss           | 388        |\n",
      "----------------------------------------\n",
      "Num timesteps: 482000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.76\n",
      "Num timesteps: 483000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -71.92\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.2         |\n",
      "|    ep_rew_mean          | -76.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 236         |\n",
      "|    time_elapsed         | 8509        |\n",
      "|    total_timesteps      | 483328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024698494 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.376       |\n",
      "|    explained_variance   | 0.791       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 204         |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | 0.0099      |\n",
      "|    std                  | 0.205       |\n",
      "|    value_loss           | 359         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 484000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.84\n",
      "Num timesteps: 485000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.66\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.88        |\n",
      "|    ep_rew_mean          | -72.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 237         |\n",
      "|    time_elapsed         | 8542        |\n",
      "|    total_timesteps      | 485376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020340972 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.359       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 230         |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | 0.00782     |\n",
      "|    std                  | 0.206       |\n",
      "|    value_loss           | 390         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 486000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.01\n",
      "Num timesteps: 487000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.36        |\n",
      "|    ep_rew_mean          | -80.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 52          |\n",
      "|    iterations           | 238         |\n",
      "|    time_elapsed         | 9203        |\n",
      "|    total_timesteps      | 487424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016986875 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.327       |\n",
      "|    explained_variance   | 0.779       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 205         |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | 0.00466     |\n",
      "|    std                  | 0.21        |\n",
      "|    value_loss           | 413         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 488000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -72.43\n",
      "Num timesteps: 489000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -92.68\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.71        |\n",
      "|    ep_rew_mean          | -88         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 239         |\n",
      "|    time_elapsed         | 9231        |\n",
      "|    total_timesteps      | 489472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032634117 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.325       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 227         |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | 0.00266     |\n",
      "|    std                  | 0.208       |\n",
      "|    value_loss           | 406         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 490000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -85.22\n",
      "Num timesteps: 491000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -75.91\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 7.27       |\n",
      "|    ep_rew_mean          | -89        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 240        |\n",
      "|    time_elapsed         | 9261       |\n",
      "|    total_timesteps      | 491520     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01700744 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.346      |\n",
      "|    explained_variance   | 0.791      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 166        |\n",
      "|    n_updates            | 2390       |\n",
      "|    policy_gradient_loss | 0.00454    |\n",
      "|    std                  | 0.206      |\n",
      "|    value_loss           | 399        |\n",
      "----------------------------------------\n",
      "Num timesteps: 492000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.65\n",
      "Num timesteps: 493000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.41\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.28        |\n",
      "|    ep_rew_mean          | -79.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 241         |\n",
      "|    time_elapsed         | 9293        |\n",
      "|    total_timesteps      | 493568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046262927 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.358       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 159         |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | 0.00779     |\n",
      "|    std                  | 0.205       |\n",
      "|    value_loss           | 410         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 494000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.33\n",
      "Num timesteps: 495000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.42\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.14        |\n",
      "|    ep_rew_mean          | -81.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 242         |\n",
      "|    time_elapsed         | 9334        |\n",
      "|    total_timesteps      | 495616      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017246146 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.376       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 217         |\n",
      "|    n_updates            | 2410        |\n",
      "|    policy_gradient_loss | 0.00484     |\n",
      "|    std                  | 0.203       |\n",
      "|    value_loss           | 425         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 496000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.05\n",
      "Num timesteps: 497000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.79\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.59       |\n",
      "|    ep_rew_mean          | -85.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 243        |\n",
      "|    time_elapsed         | 9370       |\n",
      "|    total_timesteps      | 497664     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.24775426 |\n",
      "|    clip_fraction        | 0.33       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.372      |\n",
      "|    explained_variance   | 0.761      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 288        |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | 0.0301     |\n",
      "|    std                  | 0.206      |\n",
      "|    value_loss           | 417        |\n",
      "----------------------------------------\n",
      "Num timesteps: 498000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.11\n",
      "Num timesteps: 499000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -88.27\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.89        |\n",
      "|    ep_rew_mean          | -81.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 244         |\n",
      "|    time_elapsed         | 9404        |\n",
      "|    total_timesteps      | 499712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020827767 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.365       |\n",
      "|    explained_variance   | 0.749       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 2430        |\n",
      "|    policy_gradient_loss | 0.0044      |\n",
      "|    std                  | 0.204       |\n",
      "|    value_loss           | 476         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 500000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.79\n",
      "Num timesteps: 501000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.30\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.31        |\n",
      "|    ep_rew_mean          | -83.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 245         |\n",
      "|    time_elapsed         | 9441        |\n",
      "|    total_timesteps      | 501760      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024309233 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.402       |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 260         |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | 0.00838     |\n",
      "|    std                  | 0.199       |\n",
      "|    value_loss           | 395         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 502000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.82\n",
      "Num timesteps: 503000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.67        |\n",
      "|    ep_rew_mean          | -84.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 246         |\n",
      "|    time_elapsed         | 9476        |\n",
      "|    total_timesteps      | 503808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027958732 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.433       |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 2450        |\n",
      "|    policy_gradient_loss | 0.00592     |\n",
      "|    std                  | 0.198       |\n",
      "|    value_loss           | 432         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 504000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -91.69\n",
      "Num timesteps: 505000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.29\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.74        |\n",
      "|    ep_rew_mean          | -93.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 247         |\n",
      "|    time_elapsed         | 9510        |\n",
      "|    total_timesteps      | 505856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032325067 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.432       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 224         |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | 0.00899     |\n",
      "|    std                  | 0.199       |\n",
      "|    value_loss           | 449         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 506000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -94.75\n",
      "Num timesteps: 507000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -85.41\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.26        |\n",
      "|    ep_rew_mean          | -80.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 248         |\n",
      "|    time_elapsed         | 9543        |\n",
      "|    total_timesteps      | 507904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017356457 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.422       |\n",
      "|    explained_variance   | 0.755       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 218         |\n",
      "|    n_updates            | 2470        |\n",
      "|    policy_gradient_loss | 0.00829     |\n",
      "|    std                  | 0.2         |\n",
      "|    value_loss           | 505         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 508000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.91\n",
      "Num timesteps: 509000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.83\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.61       |\n",
      "|    ep_rew_mean          | -78.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 249        |\n",
      "|    time_elapsed         | 9581       |\n",
      "|    total_timesteps      | 509952     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02424571 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.414      |\n",
      "|    explained_variance   | 0.753      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 244        |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | 0.00635    |\n",
      "|    std                  | 0.2        |\n",
      "|    value_loss           | 456        |\n",
      "----------------------------------------\n",
      "Num timesteps: 510000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.27\n",
      "Num timesteps: 511000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.64\n",
      "Num timesteps: 512000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -89.34\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.08        |\n",
      "|    ep_rew_mean          | -89.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 250         |\n",
      "|    time_elapsed         | 9614        |\n",
      "|    total_timesteps      | 512000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032832902 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.425       |\n",
      "|    explained_variance   | 0.781       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 181         |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | 0.0149      |\n",
      "|    std                  | 0.198       |\n",
      "|    value_loss           | 385         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 513000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.28\n",
      "Num timesteps: 514000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.80\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.31        |\n",
      "|    ep_rew_mean          | -80.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 251         |\n",
      "|    time_elapsed         | 9650        |\n",
      "|    total_timesteps      | 514048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030903373 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.431       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 161         |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    std                  | 0.198       |\n",
      "|    value_loss           | 423         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 515000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.88\n",
      "Num timesteps: 516000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.24\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 5.82      |\n",
      "|    ep_rew_mean          | -73.6     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 53        |\n",
      "|    iterations           | 252       |\n",
      "|    time_elapsed         | 9687      |\n",
      "|    total_timesteps      | 516096    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0213499 |\n",
      "|    clip_fraction        | 0.216     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 0.432     |\n",
      "|    explained_variance   | 0.78      |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 157       |\n",
      "|    n_updates            | 2510      |\n",
      "|    policy_gradient_loss | 0.00828   |\n",
      "|    std                  | 0.198     |\n",
      "|    value_loss           | 417       |\n",
      "---------------------------------------\n",
      "Num timesteps: 517000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.58\n",
      "Num timesteps: 518000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.97\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.08       |\n",
      "|    ep_rew_mean          | -78.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 253        |\n",
      "|    time_elapsed         | 9722       |\n",
      "|    total_timesteps      | 518144     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04730154 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.444      |\n",
      "|    explained_variance   | 0.781      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 319        |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | 0.016      |\n",
      "|    std                  | 0.196      |\n",
      "|    value_loss           | 364        |\n",
      "----------------------------------------\n",
      "Num timesteps: 519000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.93\n",
      "Num timesteps: 520000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -72.72\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.98        |\n",
      "|    ep_rew_mean          | -68.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 254         |\n",
      "|    time_elapsed         | 9757        |\n",
      "|    total_timesteps      | 520192      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023148227 |\n",
      "|    clip_fraction        | 0.274       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.471       |\n",
      "|    explained_variance   | 0.777       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 168         |\n",
      "|    n_updates            | 2530        |\n",
      "|    policy_gradient_loss | 0.00967     |\n",
      "|    std                  | 0.193       |\n",
      "|    value_loss           | 382         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 521000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.30\n",
      "Num timesteps: 522000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.07\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.14        |\n",
      "|    ep_rew_mean          | -79.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 255         |\n",
      "|    time_elapsed         | 9792        |\n",
      "|    total_timesteps      | 522240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055412926 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.471       |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 248         |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | 0.00412     |\n",
      "|    std                  | 0.194       |\n",
      "|    value_loss           | 378         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 523000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.95\n",
      "Num timesteps: 524000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.34\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.14        |\n",
      "|    ep_rew_mean          | -78.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 256         |\n",
      "|    time_elapsed         | 9826        |\n",
      "|    total_timesteps      | 524288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034466323 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.465       |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 213         |\n",
      "|    n_updates            | 2550        |\n",
      "|    policy_gradient_loss | 0.0142      |\n",
      "|    std                  | 0.195       |\n",
      "|    value_loss           | 361         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 525000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.23\n",
      "Num timesteps: 526000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.44\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.23      |\n",
      "|    ep_rew_mean          | -76.8     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 53        |\n",
      "|    iterations           | 257       |\n",
      "|    time_elapsed         | 9860      |\n",
      "|    total_timesteps      | 526336    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1992097 |\n",
      "|    clip_fraction        | 0.37      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 0.47      |\n",
      "|    explained_variance   | 0.803     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 145       |\n",
      "|    n_updates            | 2560      |\n",
      "|    policy_gradient_loss | 0.0407    |\n",
      "|    std                  | 0.193     |\n",
      "|    value_loss           | 397       |\n",
      "---------------------------------------\n",
      "Num timesteps: 527000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -89.20\n",
      "Num timesteps: 528000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -87.89\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.51       |\n",
      "|    ep_rew_mean          | -83.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 258        |\n",
      "|    time_elapsed         | 9893       |\n",
      "|    total_timesteps      | 528384     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02291375 |\n",
      "|    clip_fraction        | 0.291      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.476      |\n",
      "|    explained_variance   | 0.735      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 213        |\n",
      "|    n_updates            | 2570       |\n",
      "|    policy_gradient_loss | 0.0171     |\n",
      "|    std                  | 0.194      |\n",
      "|    value_loss           | 404        |\n",
      "----------------------------------------\n",
      "Num timesteps: 529000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.52\n",
      "Num timesteps: 530000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -71.81\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.65        |\n",
      "|    ep_rew_mean          | -66.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 259         |\n",
      "|    time_elapsed         | 9929        |\n",
      "|    total_timesteps      | 530432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021526227 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.457       |\n",
      "|    explained_variance   | 0.744       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 245         |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | 0.00526     |\n",
      "|    std                  | 0.196       |\n",
      "|    value_loss           | 434         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 531000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.77\n",
      "Num timesteps: 532000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.32\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.9         |\n",
      "|    ep_rew_mean          | -89.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 260         |\n",
      "|    time_elapsed         | 9963        |\n",
      "|    total_timesteps      | 532480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021919042 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.445       |\n",
      "|    explained_variance   | 0.784       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 225         |\n",
      "|    n_updates            | 2590        |\n",
      "|    policy_gradient_loss | 0.00211     |\n",
      "|    std                  | 0.196       |\n",
      "|    value_loss           | 413         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 533000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -89.23\n",
      "Num timesteps: 534000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.86\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.75       |\n",
      "|    ep_rew_mean          | -85.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 261        |\n",
      "|    time_elapsed         | 9996       |\n",
      "|    total_timesteps      | 534528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01992955 |\n",
      "|    clip_fraction        | 0.246      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.456      |\n",
      "|    explained_variance   | 0.797      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 144        |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | 0.00391    |\n",
      "|    std                  | 0.194      |\n",
      "|    value_loss           | 382        |\n",
      "----------------------------------------\n",
      "Num timesteps: 535000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.76\n",
      "Num timesteps: 536000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.54\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.61        |\n",
      "|    ep_rew_mean          | -85.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 262         |\n",
      "|    time_elapsed         | 10031       |\n",
      "|    total_timesteps      | 536576      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027606077 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.446       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 197         |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | 0.00917     |\n",
      "|    std                  | 0.197       |\n",
      "|    value_loss           | 400         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 537000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -92.08\n",
      "Num timesteps: 538000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -93.93\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.59        |\n",
      "|    ep_rew_mean          | -79.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 263         |\n",
      "|    time_elapsed         | 10064       |\n",
      "|    total_timesteps      | 538624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017728155 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.424       |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 247         |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | 0.00636     |\n",
      "|    std                  | 0.199       |\n",
      "|    value_loss           | 472         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 539000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.50\n",
      "Num timesteps: 540000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.07\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.35        |\n",
      "|    ep_rew_mean          | -82.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 264         |\n",
      "|    time_elapsed         | 10098       |\n",
      "|    total_timesteps      | 540672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024886101 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.432       |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 299         |\n",
      "|    n_updates            | 2630        |\n",
      "|    policy_gradient_loss | 0.00198     |\n",
      "|    std                  | 0.196       |\n",
      "|    value_loss           | 547         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 541000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.51\n",
      "Num timesteps: 542000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.44\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.77        |\n",
      "|    ep_rew_mean          | -88.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 265         |\n",
      "|    time_elapsed         | 10132       |\n",
      "|    total_timesteps      | 542720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019882603 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.461       |\n",
      "|    explained_variance   | 0.753       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 158         |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | 0.0075      |\n",
      "|    std                  | 0.194       |\n",
      "|    value_loss           | 474         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 543000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.78\n",
      "Num timesteps: 544000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.31\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.19       |\n",
      "|    ep_rew_mean          | -77.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 266        |\n",
      "|    time_elapsed         | 10167      |\n",
      "|    total_timesteps      | 544768     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03576605 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.468      |\n",
      "|    explained_variance   | 0.783      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 137        |\n",
      "|    n_updates            | 2650       |\n",
      "|    policy_gradient_loss | 0.009      |\n",
      "|    std                  | 0.195      |\n",
      "|    value_loss           | 418        |\n",
      "----------------------------------------\n",
      "Num timesteps: 545000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.38\n",
      "Num timesteps: 546000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.27\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.4         |\n",
      "|    ep_rew_mean          | -87.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 267         |\n",
      "|    time_elapsed         | 10201       |\n",
      "|    total_timesteps      | 546816      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016758692 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.462       |\n",
      "|    explained_variance   | 0.762       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 196         |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | 0.0105      |\n",
      "|    std                  | 0.194       |\n",
      "|    value_loss           | 462         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 547000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.09\n",
      "Num timesteps: 548000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.47\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.75       |\n",
      "|    ep_rew_mean          | -94.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 268        |\n",
      "|    time_elapsed         | 10235      |\n",
      "|    total_timesteps      | 548864     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04215832 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.476      |\n",
      "|    explained_variance   | 0.784      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 200        |\n",
      "|    n_updates            | 2670       |\n",
      "|    policy_gradient_loss | 0.0058     |\n",
      "|    std                  | 0.192      |\n",
      "|    value_loss           | 421        |\n",
      "----------------------------------------\n",
      "Num timesteps: 549000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.87\n",
      "Num timesteps: 550000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -72.31\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.22       |\n",
      "|    ep_rew_mean          | -82.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 269        |\n",
      "|    time_elapsed         | 10271      |\n",
      "|    total_timesteps      | 550912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01676701 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.472      |\n",
      "|    explained_variance   | 0.765      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 228        |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | 0.00568    |\n",
      "|    std                  | 0.194      |\n",
      "|    value_loss           | 474        |\n",
      "----------------------------------------\n",
      "Num timesteps: 551000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.76\n",
      "Num timesteps: 552000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.94        |\n",
      "|    ep_rew_mean          | -86.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 270         |\n",
      "|    time_elapsed         | 10305       |\n",
      "|    total_timesteps      | 552960      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022945793 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.469       |\n",
      "|    explained_variance   | 0.759       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 221         |\n",
      "|    n_updates            | 2690        |\n",
      "|    policy_gradient_loss | 0.00789     |\n",
      "|    std                  | 0.194       |\n",
      "|    value_loss           | 441         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 553000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.83\n",
      "Num timesteps: 554000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.84\n",
      "Num timesteps: 555000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -85.79\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.74       |\n",
      "|    ep_rew_mean          | -85.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 271        |\n",
      "|    time_elapsed         | 10339      |\n",
      "|    total_timesteps      | 555008     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02712607 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.463      |\n",
      "|    explained_variance   | 0.754      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 311        |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | 0.00273    |\n",
      "|    std                  | 0.195      |\n",
      "|    value_loss           | 491        |\n",
      "----------------------------------------\n",
      "Num timesteps: 556000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -70.95\n",
      "Num timesteps: 557000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.11\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.63        |\n",
      "|    ep_rew_mean          | -79.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 272         |\n",
      "|    time_elapsed         | 10374       |\n",
      "|    total_timesteps      | 557056      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010909409 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.461       |\n",
      "|    explained_variance   | 0.774       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 196         |\n",
      "|    n_updates            | 2710        |\n",
      "|    policy_gradient_loss | 0.00778     |\n",
      "|    std                  | 0.195       |\n",
      "|    value_loss           | 462         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 558000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.36\n",
      "Num timesteps: 559000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -75.23\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.07        |\n",
      "|    ep_rew_mean          | -73.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 273         |\n",
      "|    time_elapsed         | 10408       |\n",
      "|    total_timesteps      | 559104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018105192 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.488       |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 244         |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | 0.00372     |\n",
      "|    std                  | 0.191       |\n",
      "|    value_loss           | 446         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 560000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.58\n",
      "Num timesteps: 561000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.37\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.5        |\n",
      "|    ep_rew_mean          | -80.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 274        |\n",
      "|    time_elapsed         | 10442      |\n",
      "|    total_timesteps      | 561152     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04771117 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.506      |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 242        |\n",
      "|    n_updates            | 2730       |\n",
      "|    policy_gradient_loss | 0.00115    |\n",
      "|    std                  | 0.191      |\n",
      "|    value_loss           | 442        |\n",
      "----------------------------------------\n",
      "Num timesteps: 562000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.21\n",
      "Num timesteps: 563000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -75.96\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.56        |\n",
      "|    ep_rew_mean          | -78.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 275         |\n",
      "|    time_elapsed         | 10477       |\n",
      "|    total_timesteps      | 563200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028563632 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.539       |\n",
      "|    explained_variance   | 0.771       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 230         |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | 0.00996     |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 431         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 564000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -87.90\n",
      "Num timesteps: 565000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.05\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.3         |\n",
      "|    ep_rew_mean          | -77.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 276         |\n",
      "|    time_elapsed         | 10514       |\n",
      "|    total_timesteps      | 565248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016639765 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.535       |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 194         |\n",
      "|    n_updates            | 2750        |\n",
      "|    policy_gradient_loss | 0.00772     |\n",
      "|    std                  | 0.19        |\n",
      "|    value_loss           | 438         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 566000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.77\n",
      "Num timesteps: 567000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.42\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.42       |\n",
      "|    ep_rew_mean          | -81.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 277        |\n",
      "|    time_elapsed         | 10548      |\n",
      "|    total_timesteps      | 567296     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04272195 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.522      |\n",
      "|    explained_variance   | 0.756      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 222        |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | 0.0111     |\n",
      "|    std                  | 0.19       |\n",
      "|    value_loss           | 448        |\n",
      "----------------------------------------\n",
      "Num timesteps: 568000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -88.34\n",
      "Num timesteps: 569000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -87.69\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.83        |\n",
      "|    ep_rew_mean          | -90.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 278         |\n",
      "|    time_elapsed         | 10581       |\n",
      "|    total_timesteps      | 569344      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015900217 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.523       |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 139         |\n",
      "|    n_updates            | 2770        |\n",
      "|    policy_gradient_loss | 0.00607     |\n",
      "|    std                  | 0.19        |\n",
      "|    value_loss           | 381         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 570000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.07\n",
      "Num timesteps: 571000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -72.48\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 5.9       |\n",
      "|    ep_rew_mean          | -72.2     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 53        |\n",
      "|    iterations           | 279       |\n",
      "|    time_elapsed         | 10617     |\n",
      "|    total_timesteps      | 571392    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0237477 |\n",
      "|    clip_fraction        | 0.257     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 0.523     |\n",
      "|    explained_variance   | 0.769     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 153       |\n",
      "|    n_updates            | 2780      |\n",
      "|    policy_gradient_loss | 0.00572   |\n",
      "|    std                  | 0.19      |\n",
      "|    value_loss           | 421       |\n",
      "---------------------------------------\n",
      "Num timesteps: 572000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.75\n",
      "Num timesteps: 573000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.15\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.43       |\n",
      "|    ep_rew_mean          | -86.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 280        |\n",
      "|    time_elapsed         | 10651      |\n",
      "|    total_timesteps      | 573440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02046423 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.511      |\n",
      "|    explained_variance   | 0.783      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 221        |\n",
      "|    n_updates            | 2790       |\n",
      "|    policy_gradient_loss | 0.00663    |\n",
      "|    std                  | 0.192      |\n",
      "|    value_loss           | 405        |\n",
      "----------------------------------------\n",
      "Num timesteps: 574000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.10\n",
      "Num timesteps: 575000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -72.63\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.46        |\n",
      "|    ep_rew_mean          | -82.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 281         |\n",
      "|    time_elapsed         | 10686       |\n",
      "|    total_timesteps      | 575488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031256404 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.519       |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 305         |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | 0.0022      |\n",
      "|    std                  | 0.19        |\n",
      "|    value_loss           | 454         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 576000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.14\n",
      "Num timesteps: 577000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.63        |\n",
      "|    ep_rew_mean          | -82.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 282         |\n",
      "|    time_elapsed         | 10720       |\n",
      "|    total_timesteps      | 577536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032562148 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.551       |\n",
      "|    explained_variance   | 0.74        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 254         |\n",
      "|    n_updates            | 2810        |\n",
      "|    policy_gradient_loss | 0.00773     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 457         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 578000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -87.46\n",
      "Num timesteps: 579000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -91.34\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.23       |\n",
      "|    ep_rew_mean          | -78.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 53         |\n",
      "|    iterations           | 283        |\n",
      "|    time_elapsed         | 10756      |\n",
      "|    total_timesteps      | 579584     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01798853 |\n",
      "|    clip_fraction        | 0.24       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.56       |\n",
      "|    explained_variance   | 0.771      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 345        |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | 0.00536    |\n",
      "|    std                  | 0.187      |\n",
      "|    value_loss           | 460        |\n",
      "----------------------------------------\n",
      "Num timesteps: 580000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.20\n",
      "Num timesteps: 581000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.76\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.54        |\n",
      "|    ep_rew_mean          | -80.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 284         |\n",
      "|    time_elapsed         | 10792       |\n",
      "|    total_timesteps      | 581632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021971874 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.563       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 265         |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | 0.00535     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 423         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 582000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.21\n",
      "Num timesteps: 583000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.25\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.2       |\n",
      "|    ep_rew_mean          | -80.1     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 53        |\n",
      "|    iterations           | 285       |\n",
      "|    time_elapsed         | 10829     |\n",
      "|    total_timesteps      | 583680    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2312288 |\n",
      "|    clip_fraction        | 0.28      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 0.571     |\n",
      "|    explained_variance   | 0.781     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 208       |\n",
      "|    n_updates            | 2840      |\n",
      "|    policy_gradient_loss | 0.0196    |\n",
      "|    std                  | 0.186     |\n",
      "|    value_loss           | 420       |\n",
      "---------------------------------------\n",
      "Num timesteps: 584000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.72\n",
      "Num timesteps: 585000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -88.96\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.01        |\n",
      "|    ep_rew_mean          | -83.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 286         |\n",
      "|    time_elapsed         | 10866       |\n",
      "|    total_timesteps      | 585728      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025805343 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.581       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 330         |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | 0.00727     |\n",
      "|    std                  | 0.185       |\n",
      "|    value_loss           | 427         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 586000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.95\n",
      "Num timesteps: 587000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -89.28\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.25        |\n",
      "|    ep_rew_mean          | -79.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 287         |\n",
      "|    time_elapsed         | 10901       |\n",
      "|    total_timesteps      | 587776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023055896 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.569       |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 232         |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | 0.00606     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 383         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 588000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.94\n",
      "Num timesteps: 589000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.36\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.47        |\n",
      "|    ep_rew_mean          | -81.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 288         |\n",
      "|    time_elapsed         | 10938       |\n",
      "|    total_timesteps      | 589824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028742649 |\n",
      "|    clip_fraction        | 0.276       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.569       |\n",
      "|    explained_variance   | 0.785       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 210         |\n",
      "|    n_updates            | 2870        |\n",
      "|    policy_gradient_loss | 0.00606     |\n",
      "|    std                  | 0.185       |\n",
      "|    value_loss           | 385         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 590000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.59\n",
      "Num timesteps: 591000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -92.41\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.08        |\n",
      "|    ep_rew_mean          | -77.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 289         |\n",
      "|    time_elapsed         | 10974       |\n",
      "|    total_timesteps      | 591872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014718024 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.574       |\n",
      "|    explained_variance   | 0.797       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 160         |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | 0.00738     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 385         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 592000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -75.71\n",
      "Num timesteps: 593000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.13\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.6         |\n",
      "|    ep_rew_mean          | -82.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 290         |\n",
      "|    time_elapsed         | 11008       |\n",
      "|    total_timesteps      | 593920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021151975 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.559       |\n",
      "|    explained_variance   | 0.792       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 193         |\n",
      "|    n_updates            | 2890        |\n",
      "|    policy_gradient_loss | 0.00539     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 403         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 594000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.75\n",
      "Num timesteps: 595000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.48\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.73        |\n",
      "|    ep_rew_mean          | -74.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 291         |\n",
      "|    time_elapsed         | 11045       |\n",
      "|    total_timesteps      | 595968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027501298 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.564       |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 209         |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | 0.00927     |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 384         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 596000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.23\n",
      "Num timesteps: 597000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -73.17\n",
      "Num timesteps: 598000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.49\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.21        |\n",
      "|    ep_rew_mean          | -80.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 292         |\n",
      "|    time_elapsed         | 11083       |\n",
      "|    total_timesteps      | 598016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027633548 |\n",
      "|    clip_fraction        | 0.283       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.577       |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 177         |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | 0.00312     |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 410         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 599000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.54\n",
      "Num timesteps: 600000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -88.93\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.61        |\n",
      "|    ep_rew_mean          | -87.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 293         |\n",
      "|    time_elapsed         | 11122       |\n",
      "|    total_timesteps      | 600064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025764849 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.575       |\n",
      "|    explained_variance   | 0.795       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 160         |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | 0.0072      |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 396         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 601000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.08\n",
      "Num timesteps: 602000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.77\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.07        |\n",
      "|    ep_rew_mean          | -75.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 294         |\n",
      "|    time_elapsed         | 11156       |\n",
      "|    total_timesteps      | 602112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017228635 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.574       |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 152         |\n",
      "|    n_updates            | 2930        |\n",
      "|    policy_gradient_loss | 0.0047      |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 468         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 603000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.15\n",
      "Num timesteps: 604000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.17\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.87        |\n",
      "|    ep_rew_mean          | -89.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 53          |\n",
      "|    iterations           | 295         |\n",
      "|    time_elapsed         | 11190       |\n",
      "|    total_timesteps      | 604160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030716345 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.564       |\n",
      "|    explained_variance   | 0.763       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 228         |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | 0.00592     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 403         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 605000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -67.25\n",
      "Num timesteps: 606000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.63\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.5         |\n",
      "|    ep_rew_mean          | -80.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 296         |\n",
      "|    time_elapsed         | 11225       |\n",
      "|    total_timesteps      | 606208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031483613 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.546       |\n",
      "|    explained_variance   | 0.787       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 177         |\n",
      "|    n_updates            | 2950        |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    std                  | 0.189       |\n",
      "|    value_loss           | 389         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 607000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.48\n",
      "Num timesteps: 608000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.75\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.69       |\n",
      "|    ep_rew_mean          | -88.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 297        |\n",
      "|    time_elapsed         | 11260      |\n",
      "|    total_timesteps      | 608256     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01818176 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.534      |\n",
      "|    explained_variance   | 0.767      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 182        |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | 0.00318    |\n",
      "|    std                  | 0.189      |\n",
      "|    value_loss           | 443        |\n",
      "----------------------------------------\n",
      "Num timesteps: 609000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.40\n",
      "Num timesteps: 610000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.85\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.46        |\n",
      "|    ep_rew_mean          | -80.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 298         |\n",
      "|    time_elapsed         | 11297       |\n",
      "|    total_timesteps      | 610304      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021268973 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.53        |\n",
      "|    explained_variance   | 0.78        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 301         |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | 0.00747     |\n",
      "|    std                  | 0.19        |\n",
      "|    value_loss           | 397         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 611000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.90\n",
      "Num timesteps: 612000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.94\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.31        |\n",
      "|    ep_rew_mean          | -75.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 299         |\n",
      "|    time_elapsed         | 11332       |\n",
      "|    total_timesteps      | 612352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038700156 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.514       |\n",
      "|    explained_variance   | 0.773       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 233         |\n",
      "|    n_updates            | 2980        |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    std                  | 0.192       |\n",
      "|    value_loss           | 416         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 613000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.49\n",
      "Num timesteps: 614000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.26\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.64        |\n",
      "|    ep_rew_mean          | -67.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 300         |\n",
      "|    time_elapsed         | 11369       |\n",
      "|    total_timesteps      | 614400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017196152 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.517       |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 290         |\n",
      "|    n_updates            | 2990        |\n",
      "|    policy_gradient_loss | 0.00365     |\n",
      "|    std                  | 0.19        |\n",
      "|    value_loss           | 469         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 615000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.82\n",
      "Num timesteps: 616000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.01\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.72       |\n",
      "|    ep_rew_mean          | -73        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 301        |\n",
      "|    time_elapsed         | 11407      |\n",
      "|    total_timesteps      | 616448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03579212 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.506      |\n",
      "|    explained_variance   | 0.748      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 267        |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | 0.0069     |\n",
      "|    std                  | 0.193      |\n",
      "|    value_loss           | 473        |\n",
      "----------------------------------------\n",
      "Num timesteps: 617000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.64\n",
      "Num timesteps: 618000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -85.68\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.29        |\n",
      "|    ep_rew_mean          | -77.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 302         |\n",
      "|    time_elapsed         | 11441       |\n",
      "|    total_timesteps      | 618496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024941819 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.496       |\n",
      "|    explained_variance   | 0.761       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 192         |\n",
      "|    n_updates            | 3010        |\n",
      "|    policy_gradient_loss | 0.00662     |\n",
      "|    std                  | 0.192       |\n",
      "|    value_loss           | 453         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 619000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.94\n",
      "Num timesteps: 620000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -73.34\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.75        |\n",
      "|    ep_rew_mean          | -74.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 303         |\n",
      "|    time_elapsed         | 11477       |\n",
      "|    total_timesteps      | 620544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017639242 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.494       |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 163         |\n",
      "|    n_updates            | 3020        |\n",
      "|    policy_gradient_loss | 0.0102      |\n",
      "|    std                  | 0.193       |\n",
      "|    value_loss           | 466         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 621000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.96\n",
      "Num timesteps: 622000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -90.52\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.74        |\n",
      "|    ep_rew_mean          | -86.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 304         |\n",
      "|    time_elapsed         | 11515       |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021495398 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.502       |\n",
      "|    explained_variance   | 0.741       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 272         |\n",
      "|    n_updates            | 3030        |\n",
      "|    policy_gradient_loss | 0.00496     |\n",
      "|    std                  | 0.19        |\n",
      "|    value_loss           | 454         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 623000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.93\n",
      "Num timesteps: 624000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.74\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.8         |\n",
      "|    ep_rew_mean          | -84.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 305         |\n",
      "|    time_elapsed         | 11550       |\n",
      "|    total_timesteps      | 624640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017607234 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.508       |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 264         |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | 0.0045      |\n",
      "|    std                  | 0.191       |\n",
      "|    value_loss           | 423         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 625000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.42\n",
      "Num timesteps: 626000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.04\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.13        |\n",
      "|    ep_rew_mean          | -70.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 306         |\n",
      "|    time_elapsed         | 11585       |\n",
      "|    total_timesteps      | 626688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025343683 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.524       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 166         |\n",
      "|    n_updates            | 3050        |\n",
      "|    policy_gradient_loss | 0.00261     |\n",
      "|    std                  | 0.188       |\n",
      "|    value_loss           | 427         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 627000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.70\n",
      "Num timesteps: 628000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.83\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.35        |\n",
      "|    ep_rew_mean          | -79.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 307         |\n",
      "|    time_elapsed         | 11623       |\n",
      "|    total_timesteps      | 628736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026580125 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.525       |\n",
      "|    explained_variance   | 0.73        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 262         |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | 0.00633     |\n",
      "|    std                  | 0.19        |\n",
      "|    value_loss           | 472         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 629000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.84\n",
      "Num timesteps: 630000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.62\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.26        |\n",
      "|    ep_rew_mean          | -85.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 308         |\n",
      "|    time_elapsed         | 11658       |\n",
      "|    total_timesteps      | 630784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039085425 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.504       |\n",
      "|    explained_variance   | 0.767       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 227         |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | 0.00296     |\n",
      "|    std                  | 0.192       |\n",
      "|    value_loss           | 453         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 631000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -85.30\n",
      "Num timesteps: 632000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.78\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.39       |\n",
      "|    ep_rew_mean          | -70.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 309        |\n",
      "|    time_elapsed         | 11697      |\n",
      "|    total_timesteps      | 632832     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03289476 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.495      |\n",
      "|    explained_variance   | 0.771      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 183        |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | 0.00458    |\n",
      "|    std                  | 0.192      |\n",
      "|    value_loss           | 473        |\n",
      "----------------------------------------\n",
      "Num timesteps: 633000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -71.87\n",
      "Num timesteps: 634000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.76\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.37       |\n",
      "|    ep_rew_mean          | -77.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 310        |\n",
      "|    time_elapsed         | 11735      |\n",
      "|    total_timesteps      | 634880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02689825 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.499      |\n",
      "|    explained_variance   | 0.723      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 225        |\n",
      "|    n_updates            | 3090       |\n",
      "|    policy_gradient_loss | 0.00469    |\n",
      "|    std                  | 0.192      |\n",
      "|    value_loss           | 448        |\n",
      "----------------------------------------\n",
      "Num timesteps: 635000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.05\n",
      "Num timesteps: 636000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.68\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.41        |\n",
      "|    ep_rew_mean          | -76.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 311         |\n",
      "|    time_elapsed         | 11772       |\n",
      "|    total_timesteps      | 636928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015055902 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.521       |\n",
      "|    explained_variance   | 0.754       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 236         |\n",
      "|    n_updates            | 3100        |\n",
      "|    policy_gradient_loss | 0.00461     |\n",
      "|    std                  | 0.188       |\n",
      "|    value_loss           | 425         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 637000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.84\n",
      "Num timesteps: 638000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.43\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.11        |\n",
      "|    ep_rew_mean          | -78.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 312         |\n",
      "|    time_elapsed         | 11808       |\n",
      "|    total_timesteps      | 638976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019507196 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.534       |\n",
      "|    explained_variance   | 0.718       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 278         |\n",
      "|    n_updates            | 3110        |\n",
      "|    policy_gradient_loss | 0.00713     |\n",
      "|    std                  | 0.189       |\n",
      "|    value_loss           | 447         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 639000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.95\n",
      "Num timesteps: 640000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.18\n",
      "Num timesteps: 641000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -71.46\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.08       |\n",
      "|    ep_rew_mean          | -72.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 313        |\n",
      "|    time_elapsed         | 11845      |\n",
      "|    total_timesteps      | 641024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03245178 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.531      |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 288        |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | 0.00546    |\n",
      "|    std                  | 0.189      |\n",
      "|    value_loss           | 501        |\n",
      "----------------------------------------\n",
      "Num timesteps: 642000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -89.27\n",
      "Num timesteps: 643000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -88.19\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.58       |\n",
      "|    ep_rew_mean          | -87.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 314        |\n",
      "|    time_elapsed         | 11882      |\n",
      "|    total_timesteps      | 643072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09665413 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.529      |\n",
      "|    explained_variance   | 0.708      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 222        |\n",
      "|    n_updates            | 3130       |\n",
      "|    policy_gradient_loss | 0.00658    |\n",
      "|    std                  | 0.189      |\n",
      "|    value_loss           | 514        |\n",
      "----------------------------------------\n",
      "Num timesteps: 644000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.81\n",
      "Num timesteps: 645000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -87.05\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.87        |\n",
      "|    ep_rew_mean          | -87.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 315         |\n",
      "|    time_elapsed         | 11920       |\n",
      "|    total_timesteps      | 645120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028049042 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.552       |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 316         |\n",
      "|    n_updates            | 3140        |\n",
      "|    policy_gradient_loss | 0.00396     |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 551         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 646000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.75\n",
      "Num timesteps: 647000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.54\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1         |\n",
      "|    ep_rew_mean          | -79.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 316         |\n",
      "|    time_elapsed         | 11957       |\n",
      "|    total_timesteps      | 647168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019966234 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.564       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 317         |\n",
      "|    n_updates            | 3150        |\n",
      "|    policy_gradient_loss | 0.00361     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 651         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 648000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -88.14\n",
      "Num timesteps: 649000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -103.01\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.37        |\n",
      "|    ep_rew_mean          | -100        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 317         |\n",
      "|    time_elapsed         | 11990       |\n",
      "|    total_timesteps      | 649216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027690565 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.566       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 351         |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | 0.00514     |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 635         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 650000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -90.90\n",
      "Num timesteps: 651000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -72.62\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.19       |\n",
      "|    ep_rew_mean          | -79.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 318        |\n",
      "|    time_elapsed         | 12025      |\n",
      "|    total_timesteps      | 651264     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01686907 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.565      |\n",
      "|    explained_variance   | 0.699      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 371        |\n",
      "|    n_updates            | 3170       |\n",
      "|    policy_gradient_loss | 0.00739    |\n",
      "|    std                  | 0.187      |\n",
      "|    value_loss           | 627        |\n",
      "----------------------------------------\n",
      "Num timesteps: 652000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.00\n",
      "Num timesteps: 653000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.61\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.99       |\n",
      "|    ep_rew_mean          | -73.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 319        |\n",
      "|    time_elapsed         | 12061      |\n",
      "|    total_timesteps      | 653312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02846183 |\n",
      "|    clip_fraction        | 0.318      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.546      |\n",
      "|    explained_variance   | 0.7        |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 368        |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | 0.0172     |\n",
      "|    std                  | 0.189      |\n",
      "|    value_loss           | 611        |\n",
      "----------------------------------------\n",
      "Num timesteps: 654000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -88.67\n",
      "Num timesteps: 655000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -75.26\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.09        |\n",
      "|    ep_rew_mean          | -76.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 320         |\n",
      "|    time_elapsed         | 12098       |\n",
      "|    total_timesteps      | 655360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020869156 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.543       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 262         |\n",
      "|    n_updates            | 3190        |\n",
      "|    policy_gradient_loss | 0.00272     |\n",
      "|    std                  | 0.189       |\n",
      "|    value_loss           | 599         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 656000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.28\n",
      "Num timesteps: 657000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -90.69\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.74        |\n",
      "|    ep_rew_mean          | -87.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 321         |\n",
      "|    time_elapsed         | 12132       |\n",
      "|    total_timesteps      | 657408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030682728 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.551       |\n",
      "|    explained_variance   | 0.704       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 288         |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | 0.00164     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 565         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 658000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.09\n",
      "Num timesteps: 659000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.68\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.34        |\n",
      "|    ep_rew_mean          | -76.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 322         |\n",
      "|    time_elapsed         | 12167       |\n",
      "|    total_timesteps      | 659456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019861363 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.555       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 500         |\n",
      "|    n_updates            | 3210        |\n",
      "|    policy_gradient_loss | -0.000172   |\n",
      "|    std                  | 0.188       |\n",
      "|    value_loss           | 666         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 660000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.20\n",
      "Num timesteps: 661000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -85.87\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.29       |\n",
      "|    ep_rew_mean          | -77.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 323        |\n",
      "|    time_elapsed         | 12204      |\n",
      "|    total_timesteps      | 661504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03893508 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.562      |\n",
      "|    explained_variance   | 0.623      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 363        |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | 0.0103     |\n",
      "|    std                  | 0.187      |\n",
      "|    value_loss           | 617        |\n",
      "----------------------------------------\n",
      "Num timesteps: 662000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -70.43\n",
      "Num timesteps: 663000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.89\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 7.16        |\n",
      "|    ep_rew_mean          | -94         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 324         |\n",
      "|    time_elapsed         | 12240       |\n",
      "|    total_timesteps      | 663552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014831604 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.558       |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 300         |\n",
      "|    n_updates            | 3230        |\n",
      "|    policy_gradient_loss | 0.00306     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 611         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 664000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -94.64\n",
      "Num timesteps: 665000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -67.64\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.19        |\n",
      "|    ep_rew_mean          | -83.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 325         |\n",
      "|    time_elapsed         | 12274       |\n",
      "|    total_timesteps      | 665600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019867333 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.556       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 239         |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | 0.00434     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 669         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 666000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -87.26\n",
      "Num timesteps: 667000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.10\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.46       |\n",
      "|    ep_rew_mean          | -74.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 326        |\n",
      "|    time_elapsed         | 12311      |\n",
      "|    total_timesteps      | 667648     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02493531 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.554      |\n",
      "|    explained_variance   | 0.679      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 330        |\n",
      "|    n_updates            | 3250       |\n",
      "|    policy_gradient_loss | 0.00428    |\n",
      "|    std                  | 0.188      |\n",
      "|    value_loss           | 609        |\n",
      "----------------------------------------\n",
      "Num timesteps: 668000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.66\n",
      "Num timesteps: 669000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.01\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.58        |\n",
      "|    ep_rew_mean          | -79.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 327         |\n",
      "|    time_elapsed         | 12345       |\n",
      "|    total_timesteps      | 669696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032410778 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.541       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 308         |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | 0.00636     |\n",
      "|    std                  | 0.189       |\n",
      "|    value_loss           | 681         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 670000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -85.78\n",
      "Num timesteps: 671000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.60\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.31        |\n",
      "|    ep_rew_mean          | -76.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 328         |\n",
      "|    time_elapsed         | 12382       |\n",
      "|    total_timesteps      | 671744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019686963 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.55        |\n",
      "|    explained_variance   | 0.634       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 216         |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | 0.00131     |\n",
      "|    std                  | 0.187       |\n",
      "|    value_loss           | 626         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 672000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.41\n",
      "Num timesteps: 673000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.48\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.39       |\n",
      "|    ep_rew_mean          | -87.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 329        |\n",
      "|    time_elapsed         | 12419      |\n",
      "|    total_timesteps      | 673792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01896919 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.559      |\n",
      "|    explained_variance   | 0.653      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 222        |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | 0.00786    |\n",
      "|    std                  | 0.188      |\n",
      "|    value_loss           | 630        |\n",
      "----------------------------------------\n",
      "Num timesteps: 674000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -89.96\n",
      "Num timesteps: 675000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.80\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.5         |\n",
      "|    ep_rew_mean          | -78.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 330         |\n",
      "|    time_elapsed         | 12456       |\n",
      "|    total_timesteps      | 675840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024947908 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.547       |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 361         |\n",
      "|    n_updates            | 3290        |\n",
      "|    policy_gradient_loss | 0.00366     |\n",
      "|    std                  | 0.188       |\n",
      "|    value_loss           | 677         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 676000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.32\n",
      "Num timesteps: 677000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.22\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.69       |\n",
      "|    ep_rew_mean          | -88.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 331        |\n",
      "|    time_elapsed         | 12492      |\n",
      "|    total_timesteps      | 677888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02215875 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.544      |\n",
      "|    explained_variance   | 0.665      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 325        |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | 0.00345    |\n",
      "|    std                  | 0.189      |\n",
      "|    value_loss           | 650        |\n",
      "----------------------------------------\n",
      "Num timesteps: 678000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.83\n",
      "Num timesteps: 679000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.09\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.41        |\n",
      "|    ep_rew_mean          | -80.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 332         |\n",
      "|    time_elapsed         | 12530       |\n",
      "|    total_timesteps      | 679936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044132084 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.541       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 3310        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.189       |\n",
      "|    value_loss           | 576         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 680000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.36\n",
      "Num timesteps: 681000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.23\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.83        |\n",
      "|    ep_rew_mean          | -77.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 333         |\n",
      "|    time_elapsed         | 12568       |\n",
      "|    total_timesteps      | 681984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021441393 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.546       |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 229         |\n",
      "|    n_updates            | 3320        |\n",
      "|    policy_gradient_loss | 0.00756     |\n",
      "|    std                  | 0.189       |\n",
      "|    value_loss           | 657         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 682000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.77\n",
      "Num timesteps: 683000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.04\n",
      "Num timesteps: 684000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -75.14\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.81        |\n",
      "|    ep_rew_mean          | -74.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 334         |\n",
      "|    time_elapsed         | 12605       |\n",
      "|    total_timesteps      | 684032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018854083 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.56        |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 239         |\n",
      "|    n_updates            | 3330        |\n",
      "|    policy_gradient_loss | 0.00422     |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 669         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 685000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.76\n",
      "Num timesteps: 686000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.15\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.01        |\n",
      "|    ep_rew_mean          | -81.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 335         |\n",
      "|    time_elapsed         | 12643       |\n",
      "|    total_timesteps      | 686080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020850342 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.58        |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 451         |\n",
      "|    n_updates            | 3340        |\n",
      "|    policy_gradient_loss | 0.00776     |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 607         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 687000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -73.95\n",
      "Num timesteps: 688000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.02\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.09        |\n",
      "|    ep_rew_mean          | -68.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 336         |\n",
      "|    time_elapsed         | 12681       |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013851421 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.577       |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 250         |\n",
      "|    n_updates            | 3350        |\n",
      "|    policy_gradient_loss | 0.00428     |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 610         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 689000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.67\n",
      "Num timesteps: 690000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.36\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.26       |\n",
      "|    ep_rew_mean          | -79.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 337        |\n",
      "|    time_elapsed         | 12718      |\n",
      "|    total_timesteps      | 690176     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02687322 |\n",
      "|    clip_fraction        | 0.267      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.587      |\n",
      "|    explained_variance   | 0.649      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 252        |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | 0.00965    |\n",
      "|    std                  | 0.185      |\n",
      "|    value_loss           | 596        |\n",
      "----------------------------------------\n",
      "Num timesteps: 691000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.92\n",
      "Num timesteps: 692000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -77.87\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.37        |\n",
      "|    ep_rew_mean          | -78.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 338         |\n",
      "|    time_elapsed         | 12756       |\n",
      "|    total_timesteps      | 692224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031073492 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.578       |\n",
      "|    explained_variance   | 0.642       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 310         |\n",
      "|    n_updates            | 3370        |\n",
      "|    policy_gradient_loss | 0.004       |\n",
      "|    std                  | 0.188       |\n",
      "|    value_loss           | 655         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 693000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.82\n",
      "Num timesteps: 694000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -73.05\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.78        |\n",
      "|    ep_rew_mean          | -64.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 339         |\n",
      "|    time_elapsed         | 12792       |\n",
      "|    total_timesteps      | 694272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017750952 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.576       |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 488         |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | 0.00698     |\n",
      "|    std                  | 0.186       |\n",
      "|    value_loss           | 696         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 695000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -73.85\n",
      "Num timesteps: 696000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -85.06\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.64        |\n",
      "|    ep_rew_mean          | -80.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 340         |\n",
      "|    time_elapsed         | 12829       |\n",
      "|    total_timesteps      | 696320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016304972 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.591       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 269         |\n",
      "|    n_updates            | 3390        |\n",
      "|    policy_gradient_loss | 0.00187     |\n",
      "|    std                  | 0.184       |\n",
      "|    value_loss           | 629         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 697000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.54\n",
      "Num timesteps: 698000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.63\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.54        |\n",
      "|    ep_rew_mean          | -86.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 341         |\n",
      "|    time_elapsed         | 12867       |\n",
      "|    total_timesteps      | 698368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023017924 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.604       |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 390         |\n",
      "|    n_updates            | 3400        |\n",
      "|    policy_gradient_loss | 0.00487     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 699         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 699000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.81\n",
      "Num timesteps: 700000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.19\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6          |\n",
      "|    ep_rew_mean          | -80.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 342        |\n",
      "|    time_elapsed         | 12902      |\n",
      "|    total_timesteps      | 700416     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02119131 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.622      |\n",
      "|    explained_variance   | 0.682      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 391        |\n",
      "|    n_updates            | 3410       |\n",
      "|    policy_gradient_loss | 0.00522    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 617        |\n",
      "----------------------------------------\n",
      "Num timesteps: 701000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.38\n",
      "Num timesteps: 702000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.07\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.45        |\n",
      "|    ep_rew_mean          | -79         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 343         |\n",
      "|    time_elapsed         | 12940       |\n",
      "|    total_timesteps      | 702464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012673103 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.635       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 397         |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | 0.00785     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 694         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 703000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -85.14\n",
      "Num timesteps: 704000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.32\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.06        |\n",
      "|    ep_rew_mean          | -75.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 344         |\n",
      "|    time_elapsed         | 12979       |\n",
      "|    total_timesteps      | 704512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027906012 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.64        |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 328         |\n",
      "|    n_updates            | 3430        |\n",
      "|    policy_gradient_loss | 0.00771     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 665         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 705000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.67\n",
      "Num timesteps: 706000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.05\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.2         |\n",
      "|    ep_rew_mean          | -75.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 345         |\n",
      "|    time_elapsed         | 13022       |\n",
      "|    total_timesteps      | 706560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020338522 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.634       |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 474         |\n",
      "|    n_updates            | 3440        |\n",
      "|    policy_gradient_loss | 0.00409     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 642         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 707000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.15\n",
      "Num timesteps: 708000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.97\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.24        |\n",
      "|    ep_rew_mean          | -85.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 346         |\n",
      "|    time_elapsed         | 13062       |\n",
      "|    total_timesteps      | 708608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038280405 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.635       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 266         |\n",
      "|    n_updates            | 3450        |\n",
      "|    policy_gradient_loss | 0.00564     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 599         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 709000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.52\n",
      "Num timesteps: 710000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -75.33\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.46        |\n",
      "|    ep_rew_mean          | -75.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 347         |\n",
      "|    time_elapsed         | 13102       |\n",
      "|    total_timesteps      | 710656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014287086 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.643       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 401         |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | 0.00893     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 704         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 711000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.76\n",
      "Num timesteps: 712000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -89.55\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.52        |\n",
      "|    ep_rew_mean          | -75.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 348         |\n",
      "|    time_elapsed         | 13141       |\n",
      "|    total_timesteps      | 712704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036890186 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.641       |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 415         |\n",
      "|    n_updates            | 3470        |\n",
      "|    policy_gradient_loss | 0.00431     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 694         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 713000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.01\n",
      "Num timesteps: 714000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -68.38\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.22        |\n",
      "|    ep_rew_mean          | -75.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 349         |\n",
      "|    time_elapsed         | 13180       |\n",
      "|    total_timesteps      | 714752      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022410275 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.614       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 219         |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | 0.00924     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 604         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 715000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.93\n",
      "Num timesteps: 716000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.85\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.59        |\n",
      "|    ep_rew_mean          | -72.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 350         |\n",
      "|    time_elapsed         | 13217       |\n",
      "|    total_timesteps      | 716800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036701784 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.624       |\n",
      "|    explained_variance   | 0.693       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 257         |\n",
      "|    n_updates            | 3490        |\n",
      "|    policy_gradient_loss | 0.0087      |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 544         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 717000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.77\n",
      "Num timesteps: 718000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -71.11\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.45        |\n",
      "|    ep_rew_mean          | -84.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 351         |\n",
      "|    time_elapsed         | 13257       |\n",
      "|    total_timesteps      | 718848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021686696 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.638       |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 383         |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | 0.00625     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 630         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 719000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -89.01\n",
      "Num timesteps: 720000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.82\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.34        |\n",
      "|    ep_rew_mean          | -81.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 352         |\n",
      "|    time_elapsed         | 13292       |\n",
      "|    total_timesteps      | 720896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019132975 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.643       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 358         |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | 0.000478    |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 677         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 721000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.28\n",
      "Num timesteps: 722000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -69.96\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.42        |\n",
      "|    ep_rew_mean          | -84.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 353         |\n",
      "|    time_elapsed         | 13329       |\n",
      "|    total_timesteps      | 722944      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016108181 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.642       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 344         |\n",
      "|    n_updates            | 3520        |\n",
      "|    policy_gradient_loss | 0.0029      |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 637         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 723000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.27\n",
      "Num timesteps: 724000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.16\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.19        |\n",
      "|    ep_rew_mean          | -80         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 354         |\n",
      "|    time_elapsed         | 13367       |\n",
      "|    total_timesteps      | 724992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029693576 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.638       |\n",
      "|    explained_variance   | 0.676       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 320         |\n",
      "|    n_updates            | 3530        |\n",
      "|    policy_gradient_loss | 0.00541     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 643         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 725000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.82\n",
      "Num timesteps: 726000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -80.80\n",
      "Num timesteps: 727000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -71.48\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.46       |\n",
      "|    ep_rew_mean          | -74.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 355        |\n",
      "|    time_elapsed         | 13407      |\n",
      "|    total_timesteps      | 727040     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02731905 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.647      |\n",
      "|    explained_variance   | 0.688      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 293        |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | 0.0111     |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 566        |\n",
      "----------------------------------------\n",
      "Num timesteps: 728000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -73.86\n",
      "Num timesteps: 729000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.16\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.25        |\n",
      "|    ep_rew_mean          | -73.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 356         |\n",
      "|    time_elapsed         | 13445       |\n",
      "|    total_timesteps      | 729088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027348626 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.66        |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 350         |\n",
      "|    n_updates            | 3550        |\n",
      "|    policy_gradient_loss | 0.00424     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 620         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 730000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.85\n",
      "Num timesteps: 731000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.77        |\n",
      "|    ep_rew_mean          | -75.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 357         |\n",
      "|    time_elapsed         | 13482       |\n",
      "|    total_timesteps      | 731136      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041668594 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.668       |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 366         |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | 0.00215     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 591         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 732000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -88.06\n",
      "Num timesteps: 733000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -75.19\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.28       |\n",
      "|    ep_rew_mean          | -76.5      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 358        |\n",
      "|    time_elapsed         | 13520      |\n",
      "|    total_timesteps      | 733184     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01867614 |\n",
      "|    clip_fraction        | 0.276      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.658      |\n",
      "|    explained_variance   | 0.635      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 261        |\n",
      "|    n_updates            | 3570       |\n",
      "|    policy_gradient_loss | 0.00488    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 626        |\n",
      "----------------------------------------\n",
      "Num timesteps: 734000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.44\n",
      "Num timesteps: 735000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -76.52\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.36        |\n",
      "|    ep_rew_mean          | -79.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 359         |\n",
      "|    time_elapsed         | 13558       |\n",
      "|    total_timesteps      | 735232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014822016 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.647       |\n",
      "|    explained_variance   | 0.675       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 352         |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | 0.0113      |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 642         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 736000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -74.49\n",
      "Num timesteps: 737000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -70.28\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.39        |\n",
      "|    ep_rew_mean          | -83.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 360         |\n",
      "|    time_elapsed         | 13595       |\n",
      "|    total_timesteps      | 737280      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017668162 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.665       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 268         |\n",
      "|    n_updates            | 3590        |\n",
      "|    policy_gradient_loss | 0.0104      |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 581         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 738000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -69.87\n",
      "Num timesteps: 739000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.63\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.36        |\n",
      "|    ep_rew_mean          | -86.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 361         |\n",
      "|    time_elapsed         | 13631       |\n",
      "|    total_timesteps      | 739328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020470427 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.681       |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 540         |\n",
      "|    n_updates            | 3600        |\n",
      "|    policy_gradient_loss | 0.00647     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 597         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 740000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -86.42\n",
      "Num timesteps: 741000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -72.21\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.21        |\n",
      "|    ep_rew_mean          | -76.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 362         |\n",
      "|    time_elapsed         | 13667       |\n",
      "|    total_timesteps      | 741376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021898571 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.669       |\n",
      "|    explained_variance   | 0.643       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 283         |\n",
      "|    n_updates            | 3610        |\n",
      "|    policy_gradient_loss | 0.00379     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 663         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 742000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -84.99\n",
      "Num timesteps: 743000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -83.61\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.96        |\n",
      "|    ep_rew_mean          | -74.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 363         |\n",
      "|    time_elapsed         | 13703       |\n",
      "|    total_timesteps      | 743424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026866779 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.661       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 186         |\n",
      "|    n_updates            | 3620        |\n",
      "|    policy_gradient_loss | 0.00186     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 530         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 744000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -73.72\n",
      "Num timesteps: 745000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -70.71\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.4         |\n",
      "|    ep_rew_mean          | -77.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 364         |\n",
      "|    time_elapsed         | 13742       |\n",
      "|    total_timesteps      | 745472      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034652233 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.667       |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 408         |\n",
      "|    n_updates            | 3630        |\n",
      "|    policy_gradient_loss | 0.0106      |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 683         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 746000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -82.36\n",
      "Num timesteps: 747000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -66.13\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.6         |\n",
      "|    ep_rew_mean          | -81.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 365         |\n",
      "|    time_elapsed         | 13780       |\n",
      "|    total_timesteps      | 747520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025951184 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.673       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 248         |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | 0.00665     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 541         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 748000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -81.86\n",
      "Num timesteps: 749000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -95.96\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.59        |\n",
      "|    ep_rew_mean          | -88.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 366         |\n",
      "|    time_elapsed         | 13815       |\n",
      "|    total_timesteps      | 749568      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025676295 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.689       |\n",
      "|    explained_variance   | 0.648       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 396         |\n",
      "|    n_updates            | 3650        |\n",
      "|    policy_gradient_loss | 0.00714     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 658         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 750000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -78.47\n",
      "Num timesteps: 751000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -79.10\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.67       |\n",
      "|    ep_rew_mean          | -70.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 367        |\n",
      "|    time_elapsed         | 13853      |\n",
      "|    total_timesteps      | 751616     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01919825 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.704      |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 283        |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | 0.00586    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 552        |\n",
      "----------------------------------------\n",
      "Num timesteps: 752000\n",
      "Best mean reward: -65.95 - Last mean reward per episode: -65.32\n",
      "Saving new best model to log/best_model\n",
      "Num timesteps: 753000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -79.29\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.12        |\n",
      "|    ep_rew_mean          | -78.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 368         |\n",
      "|    time_elapsed         | 13889       |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030463997 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.719       |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 198         |\n",
      "|    n_updates            | 3670        |\n",
      "|    policy_gradient_loss | 0.00531     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 580         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 754000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -78.25\n",
      "Num timesteps: 755000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -73.14\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.02        |\n",
      "|    ep_rew_mean          | -70         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 369         |\n",
      "|    time_elapsed         | 13926       |\n",
      "|    total_timesteps      | 755712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019598328 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.724       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 205         |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | 0.00799     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 536         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 756000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -73.97\n",
      "Num timesteps: 757000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -81.69\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.68       |\n",
      "|    ep_rew_mean          | -81.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 370        |\n",
      "|    time_elapsed         | 13962      |\n",
      "|    total_timesteps      | 757760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03966961 |\n",
      "|    clip_fraction        | 0.331      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.728      |\n",
      "|    explained_variance   | 0.655      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 255        |\n",
      "|    n_updates            | 3690       |\n",
      "|    policy_gradient_loss | 0.0168     |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 509        |\n",
      "----------------------------------------\n",
      "Num timesteps: 758000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -81.05\n",
      "Num timesteps: 759000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -72.80\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.14        |\n",
      "|    ep_rew_mean          | -70.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 371         |\n",
      "|    time_elapsed         | 13998       |\n",
      "|    total_timesteps      | 759808      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024237074 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.723       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 476         |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | 0.00517     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 589         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 760000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -76.44\n",
      "Num timesteps: 761000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -66.70\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.64        |\n",
      "|    ep_rew_mean          | -76.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 372         |\n",
      "|    time_elapsed         | 14036       |\n",
      "|    total_timesteps      | 761856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047594972 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.715       |\n",
      "|    explained_variance   | 0.65        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 212         |\n",
      "|    n_updates            | 3710        |\n",
      "|    policy_gradient_loss | 0.0062      |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 512         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 762000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -73.55\n",
      "Num timesteps: 763000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -69.88\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6           |\n",
      "|    ep_rew_mean          | -71.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 373         |\n",
      "|    time_elapsed         | 14073       |\n",
      "|    total_timesteps      | 763904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029255167 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.7         |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 294         |\n",
      "|    n_updates            | 3720        |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 557         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 764000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -73.33\n",
      "Num timesteps: 765000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -84.13\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.75        |\n",
      "|    ep_rew_mean          | -68.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 374         |\n",
      "|    time_elapsed         | 14111       |\n",
      "|    total_timesteps      | 765952      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021537349 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.726       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 350         |\n",
      "|    n_updates            | 3730        |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 577         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 766000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -67.75\n",
      "Num timesteps: 767000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -79.81\n",
      "Num timesteps: 768000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -82.04\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.6         |\n",
      "|    ep_rew_mean          | -82         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 375         |\n",
      "|    time_elapsed         | 14147       |\n",
      "|    total_timesteps      | 768000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014881479 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.768       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 201         |\n",
      "|    n_updates            | 3740        |\n",
      "|    policy_gradient_loss | 0.00374     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 598         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 769000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -74.34\n",
      "Num timesteps: 770000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -74.72\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.05        |\n",
      "|    ep_rew_mean          | -75.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 376         |\n",
      "|    time_elapsed         | 14184       |\n",
      "|    total_timesteps      | 770048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029318772 |\n",
      "|    clip_fraction        | 0.234       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.774       |\n",
      "|    explained_variance   | 0.679       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 290         |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | 0.00553     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 573         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 771000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -70.88\n",
      "Num timesteps: 772000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -74.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.7         |\n",
      "|    ep_rew_mean          | -80.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 377         |\n",
      "|    time_elapsed         | 14222       |\n",
      "|    total_timesteps      | 772096      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029747393 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.76        |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 286         |\n",
      "|    n_updates            | 3760        |\n",
      "|    policy_gradient_loss | 0.00719     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 504         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 773000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -73.19\n",
      "Num timesteps: 774000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -80.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.25        |\n",
      "|    ep_rew_mean          | -80.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 378         |\n",
      "|    time_elapsed         | 14259       |\n",
      "|    total_timesteps      | 774144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022244554 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.748       |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 276         |\n",
      "|    n_updates            | 3770        |\n",
      "|    policy_gradient_loss | 0.00793     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 529         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 775000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -85.38\n",
      "Num timesteps: 776000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -80.98\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.65        |\n",
      "|    ep_rew_mean          | -82.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 379         |\n",
      "|    time_elapsed         | 14298       |\n",
      "|    total_timesteps      | 776192      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014483702 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.731       |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 321         |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | 0.00287     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 531         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 777000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -74.40\n",
      "Num timesteps: 778000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -72.07\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.49        |\n",
      "|    ep_rew_mean          | -73.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 380         |\n",
      "|    time_elapsed         | 14335       |\n",
      "|    total_timesteps      | 778240      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012265461 |\n",
      "|    clip_fraction        | 0.21        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.726       |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 340         |\n",
      "|    n_updates            | 3790        |\n",
      "|    policy_gradient_loss | 0.00579     |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 574         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 779000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -85.84\n",
      "Num timesteps: 780000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -81.38\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.04        |\n",
      "|    ep_rew_mean          | -78.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 381         |\n",
      "|    time_elapsed         | 14372       |\n",
      "|    total_timesteps      | 780288      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024917625 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.725       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 283         |\n",
      "|    n_updates            | 3800        |\n",
      "|    policy_gradient_loss | 0.0232      |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 462         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 781000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -92.66\n",
      "Num timesteps: 782000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -87.60\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.5         |\n",
      "|    ep_rew_mean          | -81.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 382         |\n",
      "|    time_elapsed         | 14410       |\n",
      "|    total_timesteps      | 782336      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020287089 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.714       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 181         |\n",
      "|    n_updates            | 3810        |\n",
      "|    policy_gradient_loss | 0.00775     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 511         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 783000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -72.45\n",
      "Num timesteps: 784000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -68.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.13        |\n",
      "|    ep_rew_mean          | -70.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 383         |\n",
      "|    time_elapsed         | 14448       |\n",
      "|    total_timesteps      | 784384      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023738733 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.693       |\n",
      "|    explained_variance   | 0.711       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 263         |\n",
      "|    n_updates            | 3820        |\n",
      "|    policy_gradient_loss | 0.00591     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 551         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 785000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -84.30\n",
      "Num timesteps: 786000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -74.36\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.6         |\n",
      "|    ep_rew_mean          | -80.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 384         |\n",
      "|    time_elapsed         | 14486       |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022430848 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.686       |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 290         |\n",
      "|    n_updates            | 3830        |\n",
      "|    policy_gradient_loss | 0.00493     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 517         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 787000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -87.43\n",
      "Num timesteps: 788000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -79.96\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.63        |\n",
      "|    ep_rew_mean          | -71.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 385         |\n",
      "|    time_elapsed         | 14522       |\n",
      "|    total_timesteps      | 788480      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018858664 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.688       |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 349         |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | 0.00389     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 543         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 789000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -66.93\n",
      "Num timesteps: 790000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -93.09\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.36       |\n",
      "|    ep_rew_mean          | -75        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 386        |\n",
      "|    time_elapsed         | 14557      |\n",
      "|    total_timesteps      | 790528     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02458099 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.682      |\n",
      "|    explained_variance   | 0.706      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 304        |\n",
      "|    n_updates            | 3850       |\n",
      "|    policy_gradient_loss | 0.00453    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 603        |\n",
      "----------------------------------------\n",
      "Num timesteps: 791000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -89.63\n",
      "Num timesteps: 792000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -73.99\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.48        |\n",
      "|    ep_rew_mean          | -76.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 387         |\n",
      "|    time_elapsed         | 14595       |\n",
      "|    total_timesteps      | 792576      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018984772 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.677       |\n",
      "|    explained_variance   | 0.647       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 286         |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | 0.00593     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 671         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 793000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -80.62\n",
      "Num timesteps: 794000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -69.88\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.77        |\n",
      "|    ep_rew_mean          | -90.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 388         |\n",
      "|    time_elapsed         | 14634       |\n",
      "|    total_timesteps      | 794624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023886878 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.677       |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 241         |\n",
      "|    n_updates            | 3870        |\n",
      "|    policy_gradient_loss | 0.0067      |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 566         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 795000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -90.40\n",
      "Num timesteps: 796000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -81.60\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.26        |\n",
      "|    ep_rew_mean          | -71.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 389         |\n",
      "|    time_elapsed         | 14672       |\n",
      "|    total_timesteps      | 796672      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022627097 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.685       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 279         |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | 0.0088      |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 632         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 797000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -87.88\n",
      "Num timesteps: 798000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -79.92\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.72        |\n",
      "|    ep_rew_mean          | -68.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 390         |\n",
      "|    time_elapsed         | 14710       |\n",
      "|    total_timesteps      | 798720      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019259036 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.697       |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 224         |\n",
      "|    n_updates            | 3890        |\n",
      "|    policy_gradient_loss | 0.0063      |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 557         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 799000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -76.79\n",
      "Num timesteps: 800000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -85.17\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.32        |\n",
      "|    ep_rew_mean          | -80.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 391         |\n",
      "|    time_elapsed         | 14749       |\n",
      "|    total_timesteps      | 800768      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050949097 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.715       |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 271         |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | 0.00437     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 589         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 801000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -77.06\n",
      "Num timesteps: 802000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -81.09\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.28       |\n",
      "|    ep_rew_mean          | -74.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 392        |\n",
      "|    time_elapsed         | 14786      |\n",
      "|    total_timesteps      | 802816     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09059298 |\n",
      "|    clip_fraction        | 0.217      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.72       |\n",
      "|    explained_variance   | 0.678      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 235        |\n",
      "|    n_updates            | 3910       |\n",
      "|    policy_gradient_loss | 0.00842    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 575        |\n",
      "----------------------------------------\n",
      "Num timesteps: 803000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -70.71\n",
      "Num timesteps: 804000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -79.11\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.38        |\n",
      "|    ep_rew_mean          | -73.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 393         |\n",
      "|    time_elapsed         | 14823       |\n",
      "|    total_timesteps      | 804864      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028329894 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.716       |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 254         |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | 0.00951     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 565         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 805000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -72.58\n",
      "Num timesteps: 806000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -87.77\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.67       |\n",
      "|    ep_rew_mean          | -71.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 394        |\n",
      "|    time_elapsed         | 14860      |\n",
      "|    total_timesteps      | 806912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21739945 |\n",
      "|    clip_fraction        | 0.352      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.738      |\n",
      "|    explained_variance   | 0.667      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 299        |\n",
      "|    n_updates            | 3930       |\n",
      "|    policy_gradient_loss | 0.0201     |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 600        |\n",
      "----------------------------------------\n",
      "Num timesteps: 807000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -74.24\n",
      "Num timesteps: 808000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -82.76\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.13       |\n",
      "|    ep_rew_mean          | -75.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 395        |\n",
      "|    time_elapsed         | 14899      |\n",
      "|    total_timesteps      | 808960     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02269347 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.74       |\n",
      "|    explained_variance   | 0.663      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 374        |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | 0.00325    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 628        |\n",
      "----------------------------------------\n",
      "Num timesteps: 809000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -72.28\n",
      "Num timesteps: 810000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -73.01\n",
      "Num timesteps: 811000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -75.90\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.85       |\n",
      "|    ep_rew_mean          | -76.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 396        |\n",
      "|    time_elapsed         | 14937      |\n",
      "|    total_timesteps      | 811008     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04097707 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.728      |\n",
      "|    explained_variance   | 0.678      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 275        |\n",
      "|    n_updates            | 3950       |\n",
      "|    policy_gradient_loss | 0.00415    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 554        |\n",
      "----------------------------------------\n",
      "Num timesteps: 812000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -86.82\n",
      "Num timesteps: 813000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -77.66\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.97       |\n",
      "|    ep_rew_mean          | -79.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 397        |\n",
      "|    time_elapsed         | 14974      |\n",
      "|    total_timesteps      | 813056     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02035101 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.736      |\n",
      "|    explained_variance   | 0.666      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 375        |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | 0.00386    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 640        |\n",
      "----------------------------------------\n",
      "Num timesteps: 814000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -71.92\n",
      "Num timesteps: 815000\n",
      "Best mean reward: -65.32 - Last mean reward per episode: -63.75\n",
      "Saving new best model to log/best_model\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.89        |\n",
      "|    ep_rew_mean          | -66.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 398         |\n",
      "|    time_elapsed         | 15011       |\n",
      "|    total_timesteps      | 815104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041846327 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.736       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 144         |\n",
      "|    n_updates            | 3970        |\n",
      "|    policy_gradient_loss | 0.00896     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 504         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 816000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -65.79\n",
      "Num timesteps: 817000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -77.05\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.46       |\n",
      "|    ep_rew_mean          | -81.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 399        |\n",
      "|    time_elapsed         | 15047      |\n",
      "|    total_timesteps      | 817152     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86557233 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.742      |\n",
      "|    explained_variance   | 0.68       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 278        |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | 0.0167     |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 622        |\n",
      "----------------------------------------\n",
      "Num timesteps: 818000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -84.04\n",
      "Num timesteps: 819000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -75.01\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.57        |\n",
      "|    ep_rew_mean          | -80         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 400         |\n",
      "|    time_elapsed         | 15081       |\n",
      "|    total_timesteps      | 819200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025161691 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.751       |\n",
      "|    explained_variance   | 0.646       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 244         |\n",
      "|    n_updates            | 3990        |\n",
      "|    policy_gradient_loss | 0.00764     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 598         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 820000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -66.73\n",
      "Num timesteps: 821000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -75.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.82        |\n",
      "|    ep_rew_mean          | -72.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 401         |\n",
      "|    time_elapsed         | 15112       |\n",
      "|    total_timesteps      | 821248      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029890828 |\n",
      "|    clip_fraction        | 0.28        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.773       |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 161         |\n",
      "|    n_updates            | 4000        |\n",
      "|    policy_gradient_loss | 0.0137      |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 581         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 822000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -87.43\n",
      "Num timesteps: 823000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -77.41\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.42        |\n",
      "|    ep_rew_mean          | -84.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 402         |\n",
      "|    time_elapsed         | 15143       |\n",
      "|    total_timesteps      | 823296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024721436 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.773       |\n",
      "|    explained_variance   | 0.702       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 212         |\n",
      "|    n_updates            | 4010        |\n",
      "|    policy_gradient_loss | 0.00673     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 561         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 824000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -78.43\n",
      "Num timesteps: 825000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -77.64\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.56        |\n",
      "|    ep_rew_mean          | -82.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 403         |\n",
      "|    time_elapsed         | 15170       |\n",
      "|    total_timesteps      | 825344      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016571442 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.77        |\n",
      "|    explained_variance   | 0.704       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 406         |\n",
      "|    n_updates            | 4020        |\n",
      "|    policy_gradient_loss | 0.00649     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 551         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 826000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -79.36\n",
      "Num timesteps: 827000\n",
      "Best mean reward: -63.75 - Last mean reward per episode: -62.44\n",
      "Saving new best model to log/best_model\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 5.66      |\n",
      "|    ep_rew_mean          | -74.9     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 54        |\n",
      "|    iterations           | 404       |\n",
      "|    time_elapsed         | 15196     |\n",
      "|    total_timesteps      | 827392    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6122048 |\n",
      "|    clip_fraction        | 0.33      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 0.787     |\n",
      "|    explained_variance   | 0.666     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 334       |\n",
      "|    n_updates            | 4030      |\n",
      "|    policy_gradient_loss | 0.0165    |\n",
      "|    std                  | 0.166     |\n",
      "|    value_loss           | 551       |\n",
      "---------------------------------------\n",
      "Num timesteps: 828000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.49\n",
      "Num timesteps: 829000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.62\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.89        |\n",
      "|    ep_rew_mean          | -74.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 405         |\n",
      "|    time_elapsed         | 15222       |\n",
      "|    total_timesteps      | 829440      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024638275 |\n",
      "|    clip_fraction        | 0.291       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.794       |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 231         |\n",
      "|    n_updates            | 4040        |\n",
      "|    policy_gradient_loss | 0.014       |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 573         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 830000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.05\n",
      "Num timesteps: 831000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -73.95\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.03        |\n",
      "|    ep_rew_mean          | -69.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 406         |\n",
      "|    time_elapsed         | 15247       |\n",
      "|    total_timesteps      | 831488      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015395885 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.8         |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 224         |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | 0.00582     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 559         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 832000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -71.19\n",
      "Num timesteps: 833000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.97\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.89        |\n",
      "|    ep_rew_mean          | -74.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 407         |\n",
      "|    time_elapsed         | 15272       |\n",
      "|    total_timesteps      | 833536      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028004792 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.81        |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 223         |\n",
      "|    n_updates            | 4060        |\n",
      "|    policy_gradient_loss | 0.00281     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 609         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 834000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.29\n",
      "Num timesteps: 835000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.83\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.62        |\n",
      "|    ep_rew_mean          | -75.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 408         |\n",
      "|    time_elapsed         | 15298       |\n",
      "|    total_timesteps      | 835584      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030982248 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.814       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 220         |\n",
      "|    n_updates            | 4070        |\n",
      "|    policy_gradient_loss | 0.0136      |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 519         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 836000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.16\n",
      "Num timesteps: 837000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -84.05\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.08        |\n",
      "|    ep_rew_mean          | -78.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 409         |\n",
      "|    time_elapsed         | 15323       |\n",
      "|    total_timesteps      | 837632      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062514596 |\n",
      "|    clip_fraction        | 0.34        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.829       |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 263         |\n",
      "|    n_updates            | 4080        |\n",
      "|    policy_gradient_loss | 0.0193      |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 489         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 838000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.31\n",
      "Num timesteps: 839000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -73.05\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.3        |\n",
      "|    ep_rew_mean          | -75.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 54         |\n",
      "|    iterations           | 410        |\n",
      "|    time_elapsed         | 15349      |\n",
      "|    total_timesteps      | 839680     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29686058 |\n",
      "|    clip_fraction        | 0.322      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.843      |\n",
      "|    explained_variance   | 0.735      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 264        |\n",
      "|    n_updates            | 4090       |\n",
      "|    policy_gradient_loss | 0.0214     |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 525        |\n",
      "----------------------------------------\n",
      "Num timesteps: 840000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.65\n",
      "Num timesteps: 841000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -94.88\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.11        |\n",
      "|    ep_rew_mean          | -77.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 411         |\n",
      "|    time_elapsed         | 15374       |\n",
      "|    total_timesteps      | 841728      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020488739 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.843       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 453         |\n",
      "|    n_updates            | 4100        |\n",
      "|    policy_gradient_loss | 0.00796     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 580         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 842000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -85.96\n",
      "Num timesteps: 843000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -87.39\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.28        |\n",
      "|    ep_rew_mean          | -78.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 412         |\n",
      "|    time_elapsed         | 15399       |\n",
      "|    total_timesteps      | 843776      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031093966 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.852       |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 204         |\n",
      "|    n_updates            | 4110        |\n",
      "|    policy_gradient_loss | 0.00539     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 472         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 844000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.41\n",
      "Num timesteps: 845000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.55\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.48        |\n",
      "|    ep_rew_mean          | -85.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 413         |\n",
      "|    time_elapsed         | 15424       |\n",
      "|    total_timesteps      | 845824      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026640844 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.865       |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 4120        |\n",
      "|    policy_gradient_loss | 0.00963     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 434         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 846000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -86.59\n",
      "Num timesteps: 847000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.75\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.94        |\n",
      "|    ep_rew_mean          | -79.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 414         |\n",
      "|    time_elapsed         | 15449       |\n",
      "|    total_timesteps      | 847872      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025642853 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.875       |\n",
      "|    explained_variance   | 0.725       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 232         |\n",
      "|    n_updates            | 4130        |\n",
      "|    policy_gradient_loss | 0.00604     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 539         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 848000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -82.34\n",
      "Num timesteps: 849000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -71.88\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.45        |\n",
      "|    ep_rew_mean          | -81.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 415         |\n",
      "|    time_elapsed         | 15474       |\n",
      "|    total_timesteps      | 849920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028559644 |\n",
      "|    clip_fraction        | 0.292       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.891       |\n",
      "|    explained_variance   | 0.733       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 287         |\n",
      "|    n_updates            | 4140        |\n",
      "|    policy_gradient_loss | 0.0128      |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 508         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 850000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.24\n",
      "Num timesteps: 851000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.47\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.23        |\n",
      "|    ep_rew_mean          | -82.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 416         |\n",
      "|    time_elapsed         | 15500       |\n",
      "|    total_timesteps      | 851968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032506693 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.923       |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 274         |\n",
      "|    n_updates            | 4150        |\n",
      "|    policy_gradient_loss | 0.0115      |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 531         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 852000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -82.62\n",
      "Num timesteps: 853000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.59\n",
      "Num timesteps: 854000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -82.25\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.49        |\n",
      "|    ep_rew_mean          | -81.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 417         |\n",
      "|    time_elapsed         | 15525       |\n",
      "|    total_timesteps      | 854016      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024178114 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.934       |\n",
      "|    explained_variance   | 0.724       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 386         |\n",
      "|    n_updates            | 4160        |\n",
      "|    policy_gradient_loss | 0.00589     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 537         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 855000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -67.58\n",
      "Num timesteps: 856000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -84.95\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.22        |\n",
      "|    ep_rew_mean          | -88         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 418         |\n",
      "|    time_elapsed         | 15551       |\n",
      "|    total_timesteps      | 856064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022976294 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.924       |\n",
      "|    explained_variance   | 0.742       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 277         |\n",
      "|    n_updates            | 4170        |\n",
      "|    policy_gradient_loss | 0.0139      |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 459         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 857000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.08\n",
      "Num timesteps: 858000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.10\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.23        |\n",
      "|    ep_rew_mean          | -78.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 419         |\n",
      "|    time_elapsed         | 15576       |\n",
      "|    total_timesteps      | 858112      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026588306 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.933       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 270         |\n",
      "|    n_updates            | 4180        |\n",
      "|    policy_gradient_loss | 0.00712     |\n",
      "|    std                  | 0.155       |\n",
      "|    value_loss           | 574         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 859000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.90\n",
      "Num timesteps: 860000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.69\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.9         |\n",
      "|    ep_rew_mean          | -74.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 420         |\n",
      "|    time_elapsed         | 15601       |\n",
      "|    total_timesteps      | 860160      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063642174 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.953       |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 222         |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | 0.0107      |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 576         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 861000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -87.73\n",
      "Num timesteps: 862000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.92\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.24        |\n",
      "|    ep_rew_mean          | -76         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 421         |\n",
      "|    time_elapsed         | 15626       |\n",
      "|    total_timesteps      | 862208      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028925594 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.96        |\n",
      "|    explained_variance   | 0.697       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 262         |\n",
      "|    n_updates            | 4200        |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 516         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 863000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.62\n",
      "Num timesteps: 864000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.10\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.8         |\n",
      "|    ep_rew_mean          | -71.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 422         |\n",
      "|    time_elapsed         | 15651       |\n",
      "|    total_timesteps      | 864256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020437207 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.953       |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 355         |\n",
      "|    n_updates            | 4210        |\n",
      "|    policy_gradient_loss | 0.00875     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 614         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 865000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.77\n",
      "Num timesteps: 866000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.18\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.24        |\n",
      "|    ep_rew_mean          | -77.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 423         |\n",
      "|    time_elapsed         | 15677       |\n",
      "|    total_timesteps      | 866304      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025227603 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.958       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 225         |\n",
      "|    n_updates            | 4220        |\n",
      "|    policy_gradient_loss | 0.0149      |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 521         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 867000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -89.17\n",
      "Num timesteps: 868000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -74.94\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.02      |\n",
      "|    ep_rew_mean          | -73.2     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 55        |\n",
      "|    iterations           | 424       |\n",
      "|    time_elapsed         | 15702     |\n",
      "|    total_timesteps      | 868352    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.2352757 |\n",
      "|    clip_fraction        | 0.354     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 0.957     |\n",
      "|    explained_variance   | 0.707     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 149       |\n",
      "|    n_updates            | 4230      |\n",
      "|    policy_gradient_loss | 0.0285    |\n",
      "|    std                  | 0.154     |\n",
      "|    value_loss           | 520       |\n",
      "---------------------------------------\n",
      "Num timesteps: 869000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.99\n",
      "Num timesteps: 870000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.05\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.09        |\n",
      "|    ep_rew_mean          | -80         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 425         |\n",
      "|    time_elapsed         | 15729       |\n",
      "|    total_timesteps      | 870400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037083577 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.95        |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 405         |\n",
      "|    n_updates            | 4240        |\n",
      "|    policy_gradient_loss | 0.00813     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 656         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 871000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.57\n",
      "Num timesteps: 872000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.81\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.06       |\n",
      "|    ep_rew_mean          | -89.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 426        |\n",
      "|    time_elapsed         | 15754      |\n",
      "|    total_timesteps      | 872448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01605989 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.956      |\n",
      "|    explained_variance   | 0.691      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 232        |\n",
      "|    n_updates            | 4250       |\n",
      "|    policy_gradient_loss | 0.0036     |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 587        |\n",
      "----------------------------------------\n",
      "Num timesteps: 873000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -95.38\n",
      "Num timesteps: 874000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -90.43\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.23        |\n",
      "|    ep_rew_mean          | -82.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 427         |\n",
      "|    time_elapsed         | 15778       |\n",
      "|    total_timesteps      | 874496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026990224 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.97        |\n",
      "|    explained_variance   | 0.656       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 381         |\n",
      "|    n_updates            | 4260        |\n",
      "|    policy_gradient_loss | 0.011       |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 690         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 875000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.73\n",
      "Num timesteps: 876000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.74\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.84        |\n",
      "|    ep_rew_mean          | -77.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 428         |\n",
      "|    time_elapsed         | 15804       |\n",
      "|    total_timesteps      | 876544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019406289 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.96        |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 298         |\n",
      "|    n_updates            | 4270        |\n",
      "|    policy_gradient_loss | 0.00438     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 736         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 877000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.09\n",
      "Num timesteps: 878000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -85.96\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.3         |\n",
      "|    ep_rew_mean          | -86.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 429         |\n",
      "|    time_elapsed         | 15829       |\n",
      "|    total_timesteps      | 878592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031642087 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.96        |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 363         |\n",
      "|    n_updates            | 4280        |\n",
      "|    policy_gradient_loss | 0.0112      |\n",
      "|    std                  | 0.152       |\n",
      "|    value_loss           | 627         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 879000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.29\n",
      "Num timesteps: 880000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -68.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.47       |\n",
      "|    ep_rew_mean          | -80        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 430        |\n",
      "|    time_elapsed         | 15855      |\n",
      "|    total_timesteps      | 880640     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03498423 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.987      |\n",
      "|    explained_variance   | 0.691      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 213        |\n",
      "|    n_updates            | 4290       |\n",
      "|    policy_gradient_loss | 0.00715    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 618        |\n",
      "----------------------------------------\n",
      "Num timesteps: 881000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.45\n",
      "Num timesteps: 882000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -95.32\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.43       |\n",
      "|    ep_rew_mean          | -79.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 431        |\n",
      "|    time_elapsed         | 15880      |\n",
      "|    total_timesteps      | 882688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02155323 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.02       |\n",
      "|    explained_variance   | 0.692      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 385        |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | 0.0101     |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 595        |\n",
      "----------------------------------------\n",
      "Num timesteps: 883000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.47\n",
      "Num timesteps: 884000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.00\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.89       |\n",
      "|    ep_rew_mean          | -76.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 432        |\n",
      "|    time_elapsed         | 15906      |\n",
      "|    total_timesteps      | 884736     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02095638 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.02       |\n",
      "|    explained_variance   | 0.654      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 528        |\n",
      "|    n_updates            | 4310       |\n",
      "|    policy_gradient_loss | 0.00404    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 715        |\n",
      "----------------------------------------\n",
      "Num timesteps: 885000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.32\n",
      "Num timesteps: 886000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.03\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.42       |\n",
      "|    ep_rew_mean          | -77.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 433        |\n",
      "|    time_elapsed         | 15931      |\n",
      "|    total_timesteps      | 886784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02126093 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.01       |\n",
      "|    explained_variance   | 0.705      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 273        |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | 0.00474    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 612        |\n",
      "----------------------------------------\n",
      "Num timesteps: 887000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.79\n",
      "Num timesteps: 888000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.9         |\n",
      "|    ep_rew_mean          | -73.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 434         |\n",
      "|    time_elapsed         | 15957       |\n",
      "|    total_timesteps      | 888832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019893155 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.02        |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 347         |\n",
      "|    n_updates            | 4330        |\n",
      "|    policy_gradient_loss | 0.00505     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 695         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 889000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -67.80\n",
      "Num timesteps: 890000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.64\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.32        |\n",
      "|    ep_rew_mean          | -81.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 435         |\n",
      "|    time_elapsed         | 15983       |\n",
      "|    total_timesteps      | 890880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020866891 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.03        |\n",
      "|    explained_variance   | 0.705       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 338         |\n",
      "|    n_updates            | 4340        |\n",
      "|    policy_gradient_loss | 0.00661     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 643         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 891000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.84\n",
      "Num timesteps: 892000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.46\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.25       |\n",
      "|    ep_rew_mean          | -78.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 436        |\n",
      "|    time_elapsed         | 16009      |\n",
      "|    total_timesteps      | 892928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04607164 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.03       |\n",
      "|    explained_variance   | 0.698      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 286        |\n",
      "|    n_updates            | 4350       |\n",
      "|    policy_gradient_loss | 0.00982    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 656        |\n",
      "----------------------------------------\n",
      "Num timesteps: 893000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.92\n",
      "Num timesteps: 894000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.74\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.58       |\n",
      "|    ep_rew_mean          | -84.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 437        |\n",
      "|    time_elapsed         | 16034      |\n",
      "|    total_timesteps      | 894976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03060319 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.03       |\n",
      "|    explained_variance   | 0.675      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 348        |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | 0.00488    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 666        |\n",
      "----------------------------------------\n",
      "Num timesteps: 895000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.26\n",
      "Num timesteps: 896000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.65\n",
      "Num timesteps: 897000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -74.09\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.26        |\n",
      "|    ep_rew_mean          | -73.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 438         |\n",
      "|    time_elapsed         | 16059       |\n",
      "|    total_timesteps      | 897024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025131047 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.04        |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 300         |\n",
      "|    n_updates            | 4370        |\n",
      "|    policy_gradient_loss | 0.00587     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 655         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 898000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -85.76\n",
      "Num timesteps: 899000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -85.68\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.65        |\n",
      "|    ep_rew_mean          | -83.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 439         |\n",
      "|    time_elapsed         | 16084       |\n",
      "|    total_timesteps      | 899072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021983594 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.03        |\n",
      "|    explained_variance   | 0.696       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 217         |\n",
      "|    n_updates            | 4380        |\n",
      "|    policy_gradient_loss | 0.0121      |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 603         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 900000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.44\n",
      "Num timesteps: 901000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.49\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.06       |\n",
      "|    ep_rew_mean          | -77.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 55         |\n",
      "|    iterations           | 440        |\n",
      "|    time_elapsed         | 16109      |\n",
      "|    total_timesteps      | 901120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04150705 |\n",
      "|    clip_fraction        | 0.272      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.02       |\n",
      "|    explained_variance   | 0.672      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 280        |\n",
      "|    n_updates            | 4390       |\n",
      "|    policy_gradient_loss | 0.0113     |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 648        |\n",
      "----------------------------------------\n",
      "Num timesteps: 902000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -84.13\n",
      "Num timesteps: 903000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -88.59\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.08        |\n",
      "|    ep_rew_mean          | -83.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 55          |\n",
      "|    iterations           | 441         |\n",
      "|    time_elapsed         | 16135       |\n",
      "|    total_timesteps      | 903168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020335406 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.02        |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 279         |\n",
      "|    n_updates            | 4400        |\n",
      "|    policy_gradient_loss | 0.0117      |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 593         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 904000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -73.37\n",
      "Num timesteps: 905000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -85.14\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.4         |\n",
      "|    ep_rew_mean          | -77.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 442         |\n",
      "|    time_elapsed         | 16161       |\n",
      "|    total_timesteps      | 905216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033607878 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.02        |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 249         |\n",
      "|    n_updates            | 4410        |\n",
      "|    policy_gradient_loss | 0.0097      |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 522         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 906000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.56\n",
      "Num timesteps: 907000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.76\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.04       |\n",
      "|    ep_rew_mean          | -81.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 443        |\n",
      "|    time_elapsed         | 16187      |\n",
      "|    total_timesteps      | 907264     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03164425 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.998      |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 315        |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | 0.00683    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 567        |\n",
      "----------------------------------------\n",
      "Num timesteps: 908000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.03\n",
      "Num timesteps: 909000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.16\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.13       |\n",
      "|    ep_rew_mean          | -74.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 444        |\n",
      "|    time_elapsed         | 16212      |\n",
      "|    total_timesteps      | 909312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02303058 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.99       |\n",
      "|    explained_variance   | 0.725      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 269        |\n",
      "|    n_updates            | 4430       |\n",
      "|    policy_gradient_loss | 0.00682    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 583        |\n",
      "----------------------------------------\n",
      "Num timesteps: 910000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.31\n",
      "Num timesteps: 911000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -92.62\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.16      |\n",
      "|    ep_rew_mean          | -87.9     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 56        |\n",
      "|    iterations           | 445       |\n",
      "|    time_elapsed         | 16238     |\n",
      "|    total_timesteps      | 911360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1473276 |\n",
      "|    clip_fraction        | 0.308     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 1.01      |\n",
      "|    explained_variance   | 0.685     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 296       |\n",
      "|    n_updates            | 4440      |\n",
      "|    policy_gradient_loss | 0.0076    |\n",
      "|    std                  | 0.147     |\n",
      "|    value_loss           | 572       |\n",
      "---------------------------------------\n",
      "Num timesteps: 912000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.75\n",
      "Num timesteps: 913000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.60\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 6.45      |\n",
      "|    ep_rew_mean          | -88.5     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 56        |\n",
      "|    iterations           | 446       |\n",
      "|    time_elapsed         | 16263     |\n",
      "|    total_timesteps      | 913408    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1432805 |\n",
      "|    clip_fraction        | 0.295     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 1.03      |\n",
      "|    explained_variance   | 0.684     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 370       |\n",
      "|    n_updates            | 4450      |\n",
      "|    policy_gradient_loss | 0.0158    |\n",
      "|    std                  | 0.147     |\n",
      "|    value_loss           | 626       |\n",
      "---------------------------------------\n",
      "Num timesteps: 914000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.15\n",
      "Num timesteps: 915000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -87.81\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.76        |\n",
      "|    ep_rew_mean          | -78.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 447         |\n",
      "|    time_elapsed         | 16289       |\n",
      "|    total_timesteps      | 915456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031548228 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.06        |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 372         |\n",
      "|    n_updates            | 4460        |\n",
      "|    policy_gradient_loss | 0.00666     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 622         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 916000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.01\n",
      "Num timesteps: 917000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.71\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.74       |\n",
      "|    ep_rew_mean          | -85.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 448        |\n",
      "|    time_elapsed         | 16315      |\n",
      "|    total_timesteps      | 917504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05471255 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.07       |\n",
      "|    explained_variance   | 0.707      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 329        |\n",
      "|    n_updates            | 4470       |\n",
      "|    policy_gradient_loss | 0.00914    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 540        |\n",
      "----------------------------------------\n",
      "Num timesteps: 918000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.26\n",
      "Num timesteps: 919000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.76\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.1         |\n",
      "|    ep_rew_mean          | -78         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 449         |\n",
      "|    time_elapsed         | 16340       |\n",
      "|    total_timesteps      | 919552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038757093 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.09        |\n",
      "|    explained_variance   | 0.709       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 256         |\n",
      "|    n_updates            | 4480        |\n",
      "|    policy_gradient_loss | 0.000612    |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 555         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 920000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -85.91\n",
      "Num timesteps: 921000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.76\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.74        |\n",
      "|    ep_rew_mean          | -87.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 450         |\n",
      "|    time_elapsed         | 16366       |\n",
      "|    total_timesteps      | 921600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034201987 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.1         |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 337         |\n",
      "|    n_updates            | 4490        |\n",
      "|    policy_gradient_loss | 0.00643     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 627         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 922000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.33\n",
      "Num timesteps: 923000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -84.47\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 5.65      |\n",
      "|    ep_rew_mean          | -75.4     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 56        |\n",
      "|    iterations           | 451       |\n",
      "|    time_elapsed         | 16391     |\n",
      "|    total_timesteps      | 923648    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0415583 |\n",
      "|    clip_fraction        | 0.259     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | 1.1       |\n",
      "|    explained_variance   | 0.663     |\n",
      "|    learning_rate        | 0.0005    |\n",
      "|    loss                 | 322       |\n",
      "|    n_updates            | 4500      |\n",
      "|    policy_gradient_loss | 0.0056    |\n",
      "|    std                  | 0.142     |\n",
      "|    value_loss           | 676       |\n",
      "---------------------------------------\n",
      "Num timesteps: 924000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.44\n",
      "Num timesteps: 925000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.22\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.2         |\n",
      "|    ep_rew_mean          | -75         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 452         |\n",
      "|    time_elapsed         | 16417       |\n",
      "|    total_timesteps      | 925696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025009286 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.09        |\n",
      "|    explained_variance   | 0.681       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 289         |\n",
      "|    n_updates            | 4510        |\n",
      "|    policy_gradient_loss | 0.00454     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 639         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 926000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -74.42\n",
      "Num timesteps: 927000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.93\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.24        |\n",
      "|    ep_rew_mean          | -79         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 453         |\n",
      "|    time_elapsed         | 16442       |\n",
      "|    total_timesteps      | 927744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047686197 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.08        |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 384         |\n",
      "|    n_updates            | 4520        |\n",
      "|    policy_gradient_loss | 0.00781     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 615         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 928000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.39\n",
      "Num timesteps: 929000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.91\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.23       |\n",
      "|    ep_rew_mean          | -77.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 454        |\n",
      "|    time_elapsed         | 16468      |\n",
      "|    total_timesteps      | 929792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03242597 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.09       |\n",
      "|    explained_variance   | 0.662      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 202        |\n",
      "|    n_updates            | 4530       |\n",
      "|    policy_gradient_loss | 0.00817    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 646        |\n",
      "----------------------------------------\n",
      "Num timesteps: 930000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.51\n",
      "Num timesteps: 931000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.70\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.03       |\n",
      "|    ep_rew_mean          | -82.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 455        |\n",
      "|    time_elapsed         | 16494      |\n",
      "|    total_timesteps      | 931840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02857067 |\n",
      "|    clip_fraction        | 0.284      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.1        |\n",
      "|    explained_variance   | 0.682      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 401        |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | 0.0082     |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 647        |\n",
      "----------------------------------------\n",
      "Num timesteps: 932000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.97\n",
      "Num timesteps: 933000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.81\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.19        |\n",
      "|    ep_rew_mean          | -79.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 456         |\n",
      "|    time_elapsed         | 16520       |\n",
      "|    total_timesteps      | 933888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026807114 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.1         |\n",
      "|    explained_variance   | 0.673       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 266         |\n",
      "|    n_updates            | 4550        |\n",
      "|    policy_gradient_loss | 0.00777     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 556         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 934000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.29\n",
      "Num timesteps: 935000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.22\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.05        |\n",
      "|    ep_rew_mean          | -76.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 457         |\n",
      "|    time_elapsed         | 16546       |\n",
      "|    total_timesteps      | 935936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031767756 |\n",
      "|    clip_fraction        | 0.324       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.09        |\n",
      "|    explained_variance   | 0.727       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 257         |\n",
      "|    n_updates            | 4560        |\n",
      "|    policy_gradient_loss | 0.00703     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 541         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 936000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.50\n",
      "Num timesteps: 937000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.98\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.29        |\n",
      "|    ep_rew_mean          | -83.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 458         |\n",
      "|    time_elapsed         | 16572       |\n",
      "|    total_timesteps      | 937984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029008381 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.09        |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 338         |\n",
      "|    n_updates            | 4570        |\n",
      "|    policy_gradient_loss | 0.00347     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 647         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 938000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.76\n",
      "Num timesteps: 939000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.53\n",
      "Num timesteps: 940000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.91\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.15       |\n",
      "|    ep_rew_mean          | -78.6      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 459        |\n",
      "|    time_elapsed         | 16597      |\n",
      "|    total_timesteps      | 940032     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12069303 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.09       |\n",
      "|    explained_variance   | 0.687      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 288        |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | 0.016      |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 577        |\n",
      "----------------------------------------\n",
      "Num timesteps: 941000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -74.14\n",
      "Num timesteps: 942000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.53\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.29        |\n",
      "|    ep_rew_mean          | -79.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 460         |\n",
      "|    time_elapsed         | 16623       |\n",
      "|    total_timesteps      | 942080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030865349 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.1         |\n",
      "|    explained_variance   | 0.687       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 230         |\n",
      "|    n_updates            | 4590        |\n",
      "|    policy_gradient_loss | 0.0111      |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 571         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 943000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.65\n",
      "Num timesteps: 944000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.88        |\n",
      "|    ep_rew_mean          | -70.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 461         |\n",
      "|    time_elapsed         | 16647       |\n",
      "|    total_timesteps      | 944128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023537848 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.11        |\n",
      "|    explained_variance   | 0.686       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 311         |\n",
      "|    n_updates            | 4600        |\n",
      "|    policy_gradient_loss | -0.00223    |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 663         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 945000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -82.61\n",
      "Num timesteps: 946000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.15        |\n",
      "|    ep_rew_mean          | -72.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 462         |\n",
      "|    time_elapsed         | 16672       |\n",
      "|    total_timesteps      | 946176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020704433 |\n",
      "|    clip_fraction        | 0.289       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.12        |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 392         |\n",
      "|    n_updates            | 4610        |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 698         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 947000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -74.67\n",
      "Num timesteps: 948000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -74.19\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.23       |\n",
      "|    ep_rew_mean          | -82.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 56         |\n",
      "|    iterations           | 463        |\n",
      "|    time_elapsed         | 16697      |\n",
      "|    total_timesteps      | 948224     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03787439 |\n",
      "|    clip_fraction        | 0.268      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.12       |\n",
      "|    explained_variance   | 0.69       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 382        |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | 0.00641    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 624        |\n",
      "----------------------------------------\n",
      "Num timesteps: 949000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.05\n",
      "Num timesteps: 950000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.14\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.89        |\n",
      "|    ep_rew_mean          | -87.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 464         |\n",
      "|    time_elapsed         | 16722       |\n",
      "|    total_timesteps      | 950272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021452475 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.12        |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 378         |\n",
      "|    n_updates            | 4630        |\n",
      "|    policy_gradient_loss | 0.00771     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 689         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 951000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -74.03\n",
      "Num timesteps: 952000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -72.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.65        |\n",
      "|    ep_rew_mean          | -69.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 465         |\n",
      "|    time_elapsed         | 16748       |\n",
      "|    total_timesteps      | 952320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048249267 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.13        |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 336         |\n",
      "|    n_updates            | 4640        |\n",
      "|    policy_gradient_loss | 0.0108      |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 641         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 953000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -72.01\n",
      "Num timesteps: 954000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -74.78\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.67        |\n",
      "|    ep_rew_mean          | -85.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 466         |\n",
      "|    time_elapsed         | 16773       |\n",
      "|    total_timesteps      | 954368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036856968 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.13        |\n",
      "|    explained_variance   | 0.699       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 349         |\n",
      "|    n_updates            | 4650        |\n",
      "|    policy_gradient_loss | 0.011       |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 557         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 955000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.15\n",
      "Num timesteps: 956000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.30\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.68        |\n",
      "|    ep_rew_mean          | -82.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 467         |\n",
      "|    time_elapsed         | 16797       |\n",
      "|    total_timesteps      | 956416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025123347 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.13        |\n",
      "|    explained_variance   | 0.683       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 319         |\n",
      "|    n_updates            | 4660        |\n",
      "|    policy_gradient_loss | 0.00862     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 633         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 957000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.28\n",
      "Num timesteps: 958000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.35\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.57        |\n",
      "|    ep_rew_mean          | -76.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 56          |\n",
      "|    iterations           | 468         |\n",
      "|    time_elapsed         | 16822       |\n",
      "|    total_timesteps      | 958464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029624537 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.14        |\n",
      "|    explained_variance   | 0.636       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 349         |\n",
      "|    n_updates            | 4670        |\n",
      "|    policy_gradient_loss | 0.00216     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 698         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 959000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.50\n",
      "Num timesteps: 960000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.84\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.48        |\n",
      "|    ep_rew_mean          | -74.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 469         |\n",
      "|    time_elapsed         | 16847       |\n",
      "|    total_timesteps      | 960512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036441382 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.15        |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 346         |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | 0.0074      |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 728         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 961000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.72\n",
      "Num timesteps: 962000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -86.30\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.92        |\n",
      "|    ep_rew_mean          | -75.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 470         |\n",
      "|    time_elapsed         | 16872       |\n",
      "|    total_timesteps      | 962560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047747515 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.15        |\n",
      "|    explained_variance   | 0.641       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 396         |\n",
      "|    n_updates            | 4690        |\n",
      "|    policy_gradient_loss | 0.012       |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 695         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 963000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -72.63\n",
      "Num timesteps: 964000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -72.03\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.04        |\n",
      "|    ep_rew_mean          | -73         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 471         |\n",
      "|    time_elapsed         | 16898       |\n",
      "|    total_timesteps      | 964608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022744695 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.17        |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 263         |\n",
      "|    n_updates            | 4700        |\n",
      "|    policy_gradient_loss | 0.000856    |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 638         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 965000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -69.49\n",
      "Num timesteps: 966000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -86.31\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.4        |\n",
      "|    ep_rew_mean          | -77.8      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 472        |\n",
      "|    time_elapsed         | 16923      |\n",
      "|    total_timesteps      | 966656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03213149 |\n",
      "|    clip_fraction        | 0.294      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.17       |\n",
      "|    explained_variance   | 0.668      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 304        |\n",
      "|    n_updates            | 4710       |\n",
      "|    policy_gradient_loss | 0.0111     |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 606        |\n",
      "----------------------------------------\n",
      "Num timesteps: 967000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -71.52\n",
      "Num timesteps: 968000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.25\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.96       |\n",
      "|    ep_rew_mean          | -79.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 473        |\n",
      "|    time_elapsed         | 16948      |\n",
      "|    total_timesteps      | 968704     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07426749 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.17       |\n",
      "|    explained_variance   | 0.682      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 440        |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | 0.0182     |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 608        |\n",
      "----------------------------------------\n",
      "Num timesteps: 969000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -85.83\n",
      "Num timesteps: 970000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -75.79\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.36       |\n",
      "|    ep_rew_mean          | -81        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 474        |\n",
      "|    time_elapsed         | 16971      |\n",
      "|    total_timesteps      | 970752     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05089423 |\n",
      "|    clip_fraction        | 0.31       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.18       |\n",
      "|    explained_variance   | 0.678      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 270        |\n",
      "|    n_updates            | 4730       |\n",
      "|    policy_gradient_loss | 0.018      |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 625        |\n",
      "----------------------------------------\n",
      "Num timesteps: 971000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -72.06\n",
      "Num timesteps: 972000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -80.15\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.27       |\n",
      "|    ep_rew_mean          | -74.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 475        |\n",
      "|    time_elapsed         | 16995      |\n",
      "|    total_timesteps      | 972800     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03950484 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.2        |\n",
      "|    explained_variance   | 0.676      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 332        |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | 0.0117     |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 701        |\n",
      "----------------------------------------\n",
      "Num timesteps: 973000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.60\n",
      "Num timesteps: 974000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -85.19\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.17        |\n",
      "|    ep_rew_mean          | -81.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 476         |\n",
      "|    time_elapsed         | 17018       |\n",
      "|    total_timesteps      | 974848      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026983377 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.23        |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 330         |\n",
      "|    n_updates            | 4750        |\n",
      "|    policy_gradient_loss | 0.0128      |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 601         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 975000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -84.81\n",
      "Num timesteps: 976000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.70\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.03        |\n",
      "|    ep_rew_mean          | -79.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 477         |\n",
      "|    time_elapsed         | 17041       |\n",
      "|    total_timesteps      | 976896      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034957882 |\n",
      "|    clip_fraction        | 0.27        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.24        |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 354         |\n",
      "|    n_updates            | 4760        |\n",
      "|    policy_gradient_loss | 0.00554     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 698         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 977000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.20\n",
      "Num timesteps: 978000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.60\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.33       |\n",
      "|    ep_rew_mean          | -80.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 478        |\n",
      "|    time_elapsed         | 17065      |\n",
      "|    total_timesteps      | 978944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04607176 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.23       |\n",
      "|    explained_variance   | 0.66       |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 384        |\n",
      "|    n_updates            | 4770       |\n",
      "|    policy_gradient_loss | 0.00483    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 722        |\n",
      "----------------------------------------\n",
      "Num timesteps: 979000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.05\n",
      "Num timesteps: 980000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -85.72\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.16        |\n",
      "|    ep_rew_mean          | -80.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 479         |\n",
      "|    time_elapsed         | 17088       |\n",
      "|    total_timesteps      | 980992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035502285 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.23        |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 319         |\n",
      "|    n_updates            | 4780        |\n",
      "|    policy_gradient_loss | 0.01        |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 724         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 981000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.36\n",
      "Num timesteps: 982000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -79.40\n",
      "Num timesteps: 983000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -83.24\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.32        |\n",
      "|    ep_rew_mean          | -84         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 480         |\n",
      "|    time_elapsed         | 17111       |\n",
      "|    total_timesteps      | 983040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032564573 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.22        |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 389         |\n",
      "|    n_updates            | 4790        |\n",
      "|    policy_gradient_loss | 0.00787     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 644         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 984000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -85.62\n",
      "Num timesteps: 985000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -76.12\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.98        |\n",
      "|    ep_rew_mean          | -78.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 481         |\n",
      "|    time_elapsed         | 17135       |\n",
      "|    total_timesteps      | 985088      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025104284 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.23        |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 274         |\n",
      "|    n_updates            | 4800        |\n",
      "|    policy_gradient_loss | 0.00573     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 633         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 986000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -73.79\n",
      "Num timesteps: 987000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -82.82\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.37       |\n",
      "|    ep_rew_mean          | -79.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 482        |\n",
      "|    time_elapsed         | 17158      |\n",
      "|    total_timesteps      | 987136     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03688349 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.23       |\n",
      "|    explained_variance   | 0.703      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 302        |\n",
      "|    n_updates            | 4810       |\n",
      "|    policy_gradient_loss | 0.0151     |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 651        |\n",
      "----------------------------------------\n",
      "Num timesteps: 988000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -88.93\n",
      "Num timesteps: 989000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -77.95\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.23        |\n",
      "|    ep_rew_mean          | -75         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 483         |\n",
      "|    time_elapsed         | 17181       |\n",
      "|    total_timesteps      | 989184      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029023932 |\n",
      "|    clip_fraction        | 0.267       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.23        |\n",
      "|    explained_variance   | 0.664       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 219         |\n",
      "|    n_updates            | 4820        |\n",
      "|    policy_gradient_loss | 0.0106      |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 679         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 990000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -86.28\n",
      "Num timesteps: 991000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -90.08\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.36        |\n",
      "|    ep_rew_mean          | -87.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 484         |\n",
      "|    time_elapsed         | 17205       |\n",
      "|    total_timesteps      | 991232      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026130004 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.22        |\n",
      "|    explained_variance   | 0.68        |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 285         |\n",
      "|    n_updates            | 4830        |\n",
      "|    policy_gradient_loss | 0.00738     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 690         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 992000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -81.72\n",
      "Num timesteps: 993000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -71.95\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 6.01       |\n",
      "|    ep_rew_mean          | -75.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 485        |\n",
      "|    time_elapsed         | 17228      |\n",
      "|    total_timesteps      | 993280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04045126 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.24       |\n",
      "|    explained_variance   | 0.704      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 354        |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | 0.0101     |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 594        |\n",
      "----------------------------------------\n",
      "Num timesteps: 994000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -78.47\n",
      "Num timesteps: 995000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -82.21\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.74        |\n",
      "|    ep_rew_mean          | -76.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 486         |\n",
      "|    time_elapsed         | 17252       |\n",
      "|    total_timesteps      | 995328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025742546 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.24        |\n",
      "|    explained_variance   | 0.674       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 316         |\n",
      "|    n_updates            | 4850        |\n",
      "|    policy_gradient_loss | 0.00278     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 714         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 996000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -87.73\n",
      "Num timesteps: 997000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -70.02\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 6.06        |\n",
      "|    ep_rew_mean          | -77.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 487         |\n",
      "|    time_elapsed         | 17276       |\n",
      "|    total_timesteps      | 997376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055112146 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.23        |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 330         |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | 0.0157      |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 709         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 998000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -65.80\n",
      "Num timesteps: 999000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -73.23\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.91        |\n",
      "|    ep_rew_mean          | -71.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 57          |\n",
      "|    iterations           | 488         |\n",
      "|    time_elapsed         | 17300       |\n",
      "|    total_timesteps      | 999424      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021589598 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 1.21        |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.0005      |\n",
      "|    loss                 | 269         |\n",
      "|    n_updates            | 4870        |\n",
      "|    policy_gradient_loss | 0.00771     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 693         |\n",
      "-----------------------------------------\n",
      "Num timesteps: 1000000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -82.72\n",
      "Num timesteps: 1001000\n",
      "Best mean reward: -62.44 - Last mean reward per episode: -84.14\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.83       |\n",
      "|    ep_rew_mean          | -71.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 57         |\n",
      "|    iterations           | 489        |\n",
      "|    time_elapsed         | 17324      |\n",
      "|    total_timesteps      | 1001472    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03621562 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 1.2        |\n",
      "|    explained_variance   | 0.643      |\n",
      "|    learning_rate        | 0.0005     |\n",
      "|    loss                 | 271        |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | 0.0157     |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 605        |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode_length</td><td>████▇▆▆▆▆▆▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>episode_reward</td><td>▁▄▆▆▆▇▇▇▇▇▇▇████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode_length</td><td>6.66554</td></tr><tr><td>episode_reward</td><td>-85.93766</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stilted-smoke-34</strong> at: <a href='https://wandb.ai/lindsayspoor-rlg/grid-env-training/runs/eu6881zy' target=\"_blank\">https://wandb.ai/lindsayspoor-rlg/grid-env-training/runs/eu6881zy</a><br> View project at: <a href='https://wandb.ai/lindsayspoor-rlg/grid-env-training' target=\"_blank\">https://wandb.ai/lindsayspoor-rlg/grid-env-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250123_173028-eu6881zy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train an RL agent on the environment from above\n",
    "\n",
    "# Create log dir\n",
    "log_dir = \"log/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "wandb.init(project=\"grid-env-training\")\n",
    "\n",
    "env = GridEnv()\n",
    "env = Monitor(env, log_dir)\n",
    "lr = 5e-4\n",
    "total_timesteps = 1e6\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, learning_rate = lr)\n",
    "model.learn(total_timesteps=total_timesteps, callback = WandbCallback(check_freq=1000, log_dir=log_dir))\n",
    "model.save(f\"PPO_{lr=}_{total_timesteps=}\")\n",
    "\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, num_evaluations, env, plot, max_timesteps=10):\n",
    "    \n",
    "    evaluation_results = []\n",
    "\n",
    "    for i in range(num_evaluations):\n",
    "        state, info = env.reset()\n",
    "        V_start = state[0]\n",
    "        theta_start = state[1]\n",
    "        done = False\n",
    "        timesteps = 0\n",
    "        trace = []\n",
    "\n",
    "        for t in range(max_timesteps):\n",
    "            # while not done:\n",
    "            # print(f\"{trace=}\")\n",
    "            action, _ = model.predict(state, deterministic=True)\n",
    "\n",
    "            next_state, reward, done, terminated, info = env.step(action)\n",
    "\n",
    "            trace.append({\n",
    "                \"state\": state.copy(),\n",
    "                \"action\": action.copy(),\n",
    "                \"reward\": reward.copy(),\n",
    "                \"done\": done,\n",
    "                \"terminated\": terminated,\n",
    "                \"next_state\": next_state.copy()\n",
    "            })\n",
    "            state = next_state\n",
    "\n",
    "            timesteps += 1\n",
    "\n",
    "\n",
    "\n",
    "            if done:\n",
    "                V_end = state[0]  # Assuming V_end is the first element of the state\n",
    "                theta_end = state[1]  # Assuming theta_end is the second element of the state\n",
    "                break\n",
    "            if terminated:\n",
    "                V_end = state[0]  # Assuming V_end is the first element of the state\n",
    "                theta_end = state[1]  # Assuming theta_end is the second element of the state\n",
    "                break\n",
    "\n",
    "        evaluation_results.append({\n",
    "                \"V_start\": V_start,\n",
    "                \"theta_start\": theta_start,\n",
    "                \"V_end\": V_end,\n",
    "                \"theta_end\": theta_end,\n",
    "                \"timesteps\": timesteps,\n",
    "                \"residual_end\": reward,\n",
    "                \"trace\": trace\n",
    "            })\n",
    "\n",
    "\n",
    "    # Convert results to a numpy array for easier plotting\n",
    "    V_start_values = np.array([result[\"V_start\"] for result in evaluation_results])\n",
    "    theta_start_values = np.array([result[\"theta_start\"] for result in evaluation_results])\n",
    "    timesteps_values = np.array([result[\"timesteps\"] for result in evaluation_results])\n",
    "\n",
    "    if plot:\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter = plt.scatter(theta_start_values, V_start_values, c=timesteps_values, cmap='viridis')\n",
    "        plt.colorbar(scatter, label='Number of Timesteps')\n",
    "        plt.xlabel('Theta_init')\n",
    "        plt.ylabel('V_init')\n",
    "        # plt.title('Evaluation of Trained Model')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_a_trace(evaluation_results, index):\n",
    "    # Convert results to a numpy array for easier plotting\n",
    "    V_start_values = np.array([result[\"V_start\"] for result in evaluation_results])\n",
    "    theta_start_values = np.array([result[\"theta_start\"] for result in evaluation_results])\n",
    "    timesteps_values = np.array([result[\"timesteps\"] for result in evaluation_results])\n",
    "\n",
    "\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(theta_start_values, V_start_values, c=timesteps_values, cmap='viridis', alpha=0.4)\n",
    "\n",
    "    trace = evaluation_results[index][\"trace\"]\n",
    "    # print(f\"{evaluation_results=}\")\n",
    "    # print(f'{trace=}')\n",
    "\n",
    "    V_trace = [step[\"state\"][0] for step in trace]\n",
    "\n",
    "    theta_trace = [step[\"state\"][1] for step in trace]\n",
    "\n",
    "    k_trace = [step[\"state\"][2] for step in trace]\n",
    "    residual_trace = [step[\"reward\"] for step in trace]\n",
    "\n",
    "    plt.plot(theta_trace, V_trace, color=\"black\")\n",
    "    plt.scatter(theta_trace, V_trace, color=\"black\")\n",
    "\n",
    "    plt.annotate(f'res={residual_trace[0]:.2f},\\n k={k_trace[0]}', (theta_trace[0], V_trace[0]),\n",
    "                        textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=5)\n",
    "    plt.annotate(f'res={residual_trace[-1]:.2f},\\n k={k_trace[-1]}', (theta_trace[-1], V_trace[-1]),\n",
    "                        textcoords=\"offset points\", xytext=(0,5), ha='center', fontsize=5)\n",
    "      \n",
    "    if len(list(V_trace))==10:\n",
    "        plt.scatter(theta_trace[-1], V_trace[-1], color=\"red\")\n",
    "\n",
    "\n",
    "    plt.colorbar(scatter, label='Number of Timesteps')\n",
    "    plt.xlabel('Theta')\n",
    "    plt.ylabel('V')\n",
    "    # plt.title('Evaluation of Trained Model')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/opt/miniconda3/envs/ictwi_alliander_py312/lib/python3.12/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = GridEnv()\n",
    "num_evaluations = 10000\n",
    "plot = True\n",
    "lr = 5e-4\n",
    "total_timesteps = 1e6\n",
    "#best model results are with this learning rate!\n",
    "\n",
    "model = PPO.load(f\"PPO_lr={lr}_total_timesteps={total_timesteps}\")\n",
    "\n",
    "evaluation_results = evaluate_model(model, num_evaluations, env, plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_a_trace(evaluation_results, index=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ictwi_alliander_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
